# Training & Deployment Investigation (2025-12-10)

Scope: verify whether local/cloud training is actually running, whether datasets/tokenization provide a useful learning signal, and whether merged artifacts are viable for RunPod inference/prod pods.

- Local training is running end-to-end for some jobs. `lib/training/logs/job_9aad812f-995b-4cd1-aecd-fc1c1372b20a.log` (Mistral 7B, 2050 examples → 1640 train/410 eval) and `lib/training/logs/job_837ba8af-7952-4518-a48c-9176cc0d1154.log` (Qwen 1.7B, same dataset size) both finished 3 epochs, generated metrics, and saved merged artifacts. Another run `lib/training/logs/job_278f57be-1b2a-465c-9722-88ddc4f6d80a.log` died before training with a CUDA OOM while preparing k-bit training on a 24GB GPU; capacity is currently tight for the Mistral 7B QLoRA config.
- Masking/tokenization gap on Mistral runs. In `job_9aad812f-995b-4cd1-aecd-fc1c1372b20a.log` the pretokenizer repeatedly logs `[PreTokenize] Response template not found... Template searched: ' [/INST] '`, so labels cover the full prompt+response instead of masking the prompt. This can teach the model to parrot prompt text and dampen learning. The Qwen runs do mask correctly with the `<|im_start|>assistant` template (`job_837ba8af-7952-4518-a48c-9176cc0d1154.log`). Likely cause: chat template mismatch for Mistral.
- Dataset quality/fit. Sample rows from `lib/training/logs/datasets/job_9aad812f-995b-4cd1-aecd-fc1c1372b20a/f89f0a1d-0fe8-489e-8990-ea766c142747.jsonl` are generic “how to fine-tune” Q&A across unrelated tasks (IMDB sentiment, CoNLL NER, email tool use, CNN/DailyMail summarization, etc.) with `<think>` traces. With ~2k examples and no domain-specific coverage, the signal is likely too weak to produce observable behavioral shifts for the target product.
- Merge artifacts are created but remain quantized. Both successful runs saved `merged_model/model.safetensors` (`4.4G` for Mistral, `1.9G` for Qwen) and their `config.json` still includes `quantization_config` (bitsandbytes). Logs explicitly say “Skipping dtype cast for quantized model - merged model will be saved in its current format.” vLLM/RunPod inference typically expects fp16/bf16 weights; quantized merges plus retained `quantization_config` can cause load failures or degraded quality. The RunPod training script tries to strip quantization_config after merge, but the local standalone trainer does not.
- Merge process itself completes. The same logs show adapters saved then merged (`merge_and_unload`) and merged checkpoints written to `merged_model/`, so the merge call is not failing outright; the risk is the quantized output format, not the merge step.
- Training server health. `lib/training/training_server.log` shows the worker reconnecting to an active job but then failing to bind `0.0.0.0:8000` because the address is already in use. If another process holds that port, local job orchestration/monitoring may be impaired.
- Cloud pipeline sanity. `app/api/training/deploy/runpod/route.ts` now generates a signed dataset download URL and passes it as `DATASET_URL`; the training script in `lib/training/runpod-service.ts` consumes that URL, so the earlier “local dataset path” blocker is addressed. The script still keeps pods alive for 1 hour after success and 2 minutes after failure (`lib/training/runpod-service.ts:1824`), which wastes GPU time but does not block training itself.

Actionable risks:
- Fix response masking for Mistral chat formatting so prompts are masked before training.
- Re-export merged artifacts in fp16/bf16 and remove `quantization_config` before shipping to inference/pods (or ensure the RunPod pipeline path is used for deployments).
- Right-size configs for 24GB GPUs or pick smaller bases to avoid OOMs.
- Clear the port 8000 conflict so the training server can start cleanly.
- Consider refreshing the dataset with task-specific, production-aligned examples; current generic guidance is unlikely to show measurable gains.
