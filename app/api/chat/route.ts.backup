// API route for chat completions with streaming support
import { NextRequest } from 'next/server';
import { streamOpenAIResponse, runOpenAIWithToolCalls } from '@/lib/llm/openai';
import type { ChatMessage, ToolDefinition } from '@/lib/llm/openai';
import { streamAnthropicResponse, runAnthropicWithToolCalls } from '@/lib/llm/anthropic';
import { graphragService } from '@/lib/graphrag';
import { executeTool } from '@/lib/tools/toolManager';
import type { EnhancedPrompt, SearchSource, SearchMetadata } from '@/lib/graphrag';
import { supabase } from '@/lib/supabaseClient';
import { loadLLMConfig } from '@/lib/config/llmConfig';

// Use Node.js runtime instead of Edge for OpenAI SDK compatibility
export const runtime = 'nodejs';

// Helper function to determine if GraphRAG should be skipped
function shouldSkipGraphRAG(message: string): boolean {
  if (!message || typeof message !== 'string') return true;

  const msg = message.toLowerCase().trim();

  // Skip very short messages (likely greetings)
  if (msg.length < 15) return true;

  // Skip greetings
  if (/^(hi|hey|hello|sup|yo)\b/i.test(msg)) return true;
  if (/^how are you/i.test(msg)) return true;

  // Skip ONLY direct time/date queries (not questions that mention time)
  if (/^(what time|what's the time|current time|what.*the date)/i.test(msg)) return true;

  // Skip weather queries
  if (/(weather|temperature|forecast)/i.test(msg)) return true;

  // Skip math/calculations - improved detection
  // Matches: "calculate X", "what is 5+5", "50 * 2", "23% of 456", etc.
  if (/^(calculate|convert|what is \d|how much is \d)/i.test(msg)) return true;
  // Detect mathematical expressions: numbers with operators
  if (/\d+\s*[\+\-\*\/\%]\s*\d+/.test(msg)) return true;
  // Detect percentage calculations: "X% of Y"
  if (/\d+\s*%\s*of\s*\d+/i.test(msg)) return true;

  return false;
}

export async function POST(req: NextRequest) {
  try {
    const { messages, memory, tools, conversationId } = await req.json();

    // Extract user ID from request (from auth/memory)
    let userId: string | null = null;
    try {
      // Get user ID from memory if available
      userId = memory?.userId || null;
    } catch (error) {
      console.log('[API] Could not get user ID for GraphRAG', error);
    }

    if (!messages || !Array.isArray(messages)) {
      return new Response('Invalid messages format', { status: 400 });
    }
    
    console.log('[API] Request with', messages.length, 'messages,', tools?.length || 0, 'tools');
    
    // Inject memory context as system message if provided
    let enhancedMessages = messages;
    if (memory && (Object.keys(memory.userPreferences || {}).length > 0 || 
                   Object.keys(memory.conversationMemories || {}).length > 0)) {
      const memoryContext = `Memory Context:
User Preferences: ${JSON.stringify(memory.userPreferences, null, 2)}
Conversation Context: ${JSON.stringify(memory.conversationMemories, null, 2)}`;
      
      console.log('[API] Adding memory context to messages');
      enhancedMessages = [
        { role: 'system', content: memoryContext },
        ...messages
      ];
    }

    // Inject conversation history from Supabase (last 20 messages)
    if (conversationId && userId) {
      try {
        const { data: recentMessages } = await supabase
          .from('messages')
          .select('role, content')
          .eq('conversation_id', conversationId)
          .order('created_at', { ascending: false })
          .limit(20);

        if (recentMessages && recentMessages.length > 0) {
          const historyContext = recentMessages
            .reverse()
            .map(m => `${m.role}: ${m.content}`)
            .join('\n');

          console.log('[API] Adding conversation history:', recentMessages.length, 'messages');
          enhancedMessages = [
            { role: 'system', content: `Recent conversation history:\n${historyContext}` },
            ...enhancedMessages
          ];
        }
      } catch (error) {
        console.error('[API] Error loading conversation history:', error);
        // Continue without history on error
      }
    }

    // GraphRAG enhancement - inject document context
    let graphRAGMetadata: { sources?: SearchSource[]; metadata?: SearchMetadata } | null = null;
    if (userId) {
      try {
        const userMessage = messages[messages.length - 1]?.content;
        if (userMessage && typeof userMessage === 'string') {
          // Check if we should skip GraphRAG for this query
          if (shouldSkipGraphRAG(userMessage)) {
            console.log('[API] Skipping GraphRAG for query type:', userMessage.slice(0, 50));
          } else {
            const enhanced: EnhancedPrompt = await graphragService.enhancePrompt(
              userId,
              userMessage
            );

            if (enhanced.contextUsed) {
              console.log('[API] GraphRAG context added from', enhanced.sources?.length, 'sources');
              console.log('[API] Original message:', userMessage.slice(0, 100));
              console.log('[API] Enhanced message preview:', enhanced.prompt.slice(0, 200) + '...');
              // Replace the last message with enhanced version
              enhancedMessages[enhancedMessages.length - 1] = {
                role: 'user',
                content: enhanced.prompt
              };

              graphRAGMetadata = {
                sources: enhanced.sources,
                metadata: enhanced.metadata
              };
            }
          }
        }
      } catch (error) {
        console.error('[API] GraphRAG enhancement error:', error);
        // Continue without GraphRAG on error
      }
    }

    // Load model and provider from config
    const llmConfig = loadLLMConfig();
    const provider = llmConfig.provider || 'openai';
    let model: string;
    let temperature: number;
    let maxTokens: number;
    // Use shared types for function signatures
    let streamLLMResponse: (
      messages: ChatMessage[],
      model: string,
      temperature: number,
      maxTokens: number,
      tools?: ToolDefinition[]
    ) => AsyncGenerator<string, void, unknown>;
    let runLLMWithToolCalls: (
      messages: ChatMessage[],
      model: string,
      temperature: number,
      maxTokens: number,
      tools: ToolDefinition[],
      toolCallHandler?: (toolName: string, args: Record<string, unknown>) => Promise<unknown>
    ) => Promise<string>;
    if (provider === 'anthropic') {
      model = llmConfig.anthropic?.model || 'claude-3-5-sonnet-20241022';
      streamLLMResponse = streamAnthropicResponse;
      runLLMWithToolCalls = runAnthropicWithToolCalls;
      temperature = llmConfig.anthropic?.temperature ?? 0.7;
      maxTokens = llmConfig.anthropic?.max_tokens ?? 2000;
    } else {
      model = llmConfig.openai?.model || 'gpt-4o-mini';
      streamLLMResponse = streamOpenAIResponse;
      runLLMWithToolCalls = runOpenAIWithToolCalls;
      temperature = llmConfig.openai?.temperature ?? 0.7;
      maxTokens = llmConfig.openai?.max_tokens ?? 2000;
    }

    // Tool-call aware chat completion (provider-aware)
    const toolCallHandler = async (toolName: string, args: Record<string, unknown>) => {
      // Use conversationId if available, else empty string
      const convId = conversationId || '';
      const result = await executeTool(toolName, args, convId);
      if (result.error) return { error: result.error };
      return result.data;
    };

    const encoder = new TextEncoder();

    // Branch: Use non-streaming for tool-enabled requests, streaming for simple chats
    if (tools && tools.length > 0) {
      // NON-STREAMING PATH: Execute tools and get complete response
      console.log('[API] Using non-streaming tool-aware path');
      const finalResponse = await runLLMWithToolCalls(
        enhancedMessages,
        model,
        temperature,
        maxTokens,
        tools,
        toolCallHandler
      );

      // Create stream that "fake streams" the complete response
      const stream = new ReadableStream({
        async start(controller) {
          try {
            // Send GraphRAG metadata if available
            if (graphRAGMetadata) {
              const citations = graphRAGMetadata.sources
                ? graphragService.formatCitations(graphRAGMetadata.sources)
                : [];
              const metaData = `data: ${JSON.stringify({
                type: 'graphrag_metadata',
                citations,
                contextsUsed: graphRAGMetadata.sources?.length || 0
              })}\n\n`;
              controller.enqueue(encoder.encode(metaData));
            }

            // Fake stream the complete response character-by-character for smooth UX
            const chunkSize = 3; // Send 3 characters at a time
            for (let i = 0; i < finalResponse.length; i += chunkSize) {
              const chunk = finalResponse.slice(i, i + chunkSize);
              const data = `data: ${JSON.stringify({ content: chunk })}\n\n`;
              controller.enqueue(encoder.encode(data));
              // Small delay to simulate streaming
              await new Promise(resolve => setTimeout(resolve, 10));
            }

            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
            controller.close();
          } catch (error) {
            console.error('[API] Non-streaming error:', error);
            const errorData = `data: ${JSON.stringify({ error: 'Stream error' })}\n\n`;
            controller.enqueue(encoder.encode(errorData));
            controller.close();
          }
        },
      });

      return new Response(stream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
        },
      });
    } else {
      // STREAMING PATH: Regular chat without tools
      console.log('[API] Using streaming path (no tools)');
      const stream = new ReadableStream({
        async start(controller) {
          try {
            // Send GraphRAG metadata if available
            if (graphRAGMetadata) {
              const citations = graphRAGMetadata.sources
                ? graphragService.formatCitations(graphRAGMetadata.sources)
                : [];
              const metaData = `data: ${JSON.stringify({
                type: 'graphrag_metadata',
                citations,
                contextsUsed: graphRAGMetadata.sources?.length || 0
              })}\n\n`;
              controller.enqueue(encoder.encode(metaData));
            }

            // Stream LLM output (plain text chunks)
            for await (const chunk of streamLLMResponse(
              enhancedMessages,
              model,
              temperature,
              maxTokens
            )) {
              // Send plain text chunks directly
              const data = `data: ${JSON.stringify({ content: chunk })}\n\n`;
              controller.enqueue(encoder.encode(data));
            }

            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
            controller.close();
          } catch (error) {
            console.error('[API] Streaming error:', error);
            const errorData = `data: ${JSON.stringify({ error: 'Stream error' })}\n\n`;
            controller.enqueue(encoder.encode(errorData));
            controller.close();
          }
        },
      });

      return new Response(stream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
        },
      });
    }
  } catch (error) {
    console.error('Chat API error:', error);
    return new Response('Internal server error', { status: 500 });
  }
}
