# LLM-as-Judge Tool Implementation - COMPLETE ‚úÖ
**Date**: November 29, 2025
**Feature**: Analytics Assistant can now evaluate message quality on-demand
**Status**: IMPLEMENTED AND READY TO TEST

---

## What Was Implemented

Added `evaluate_messages` tool to the analytics assistant to enable conversational quality evaluation.

### Changes Made

**File**: `/app/api/analytics/chat/route.ts`

#### 1. Added Tool Definition (Lines 240-272)

**Tool name**: `evaluate_messages`

**Description**: Evaluates message quality using LLM-as-judge with detailed scoring

**Parameters**:
- `message_ids` (required): Array of message IDs to evaluate
- `criteria` (optional): Which criteria to evaluate (default: all)
  - Options: 'helpfulness', 'accuracy', 'clarity', 'safety', 'completeness', 'all'
- `judge_model` (optional): Which LLM judge to use (default: 'claude-3-sonnet')
  - Options: 'gpt-4-turbo', 'claude-3-opus', 'claude-3-sonnet'

**Returns**:
```typescript
{
  success: true,
  summary: {
    total_evaluated: number,      // How many messages were evaluated
    total_judgments: number,       // Total evaluations (messages √ó criteria)
    passed: number,                // How many passed
    failed: number,                // How many failed
    pass_rate: string,             // Pass rate as percentage
    judge_model: string            // Which judge was used
  },
  average_scores: {
    helpfulness: number,           // 0-10
    accuracy: number,              // 0-10
    clarity: number,               // 0-10
    safety: number,                // 0-10
    completeness: number           // 0-10
  },
  failed_evaluations: [
    {
      message_id: string,
      criterion: string,
      score: number,
      reasoning: string
    }
  ],
  criteria_evaluated: string[]
}
```

---

#### 2. Added Handler Function (Lines 438-531)

**Function**: `evaluateMessages(messageIds, criteria, judgeModel, authHeader)`

**What it does**:
1. Expands 'all' to full criteria list
2. Calls existing `/api/evaluation/judge` endpoint (batch mode)
3. Calculates summary statistics
4. Returns structured data for assistant

**Integration**:
- Uses existing LLM-judge infrastructure (`/api/evaluation/judge`)
- Auto-saves judgments to database with `source='analytics_assistant'`
- Supports all 5 standard criteria

---

#### 3. Updated Tool Execution Handler (Lines 591-600)

Added case for `evaluate_messages` in `executeAnalyticsTool` switch statement:
```typescript
case 'evaluate_messages':
  if (!authHeader) {
    return { error: 'Authorization required for message evaluation' };
  }
  return await evaluateMessages(
    args.message_ids as string[],
    args.criteria as string[] | undefined,
    args.judge_model as string | undefined,
    authHeader
  );
```

---

#### 4. Updated System Prompt (Lines 708-758)

**Changed**: "YOUR 7 TOOLS" ‚Üí "YOUR 8 TOOLS"

**Added** tool #5:
```
5. **evaluate_messages** - LLM-as-judge quality evaluation (NEW!)
   - USE when user asks "evaluate quality", "how good are the responses", or "rate the messages"
   - First call get_session_conversations to get message IDs (filter to assistant messages only)
   - Returns: detailed scores (0-10) for helpfulness, accuracy, clarity, safety, completeness
   - Includes reasoning, pass/fail status, and improvement suggestions
   - Example: User asks "Evaluate this session" ‚Üí get conversations ‚Üí filter assistant messages ‚Üí evaluate_messages
```

Renumbered remaining tools (6-8).

---

## How It Works

### Example Conversation Flow:

```
User: "Evaluate the quality of responses in this session"

Analytics Assistant:
1. Calls get_session_conversations(conversationIds)
   ‚Üí Gets all conversation messages

2. Filters messages to role='assistant' only
   ‚Üí Extracts message IDs: ["msg-1", "msg-2", "msg-3", ...]

3. Calls evaluate_messages({
     message_ids: ["msg-1", "msg-2", "msg-3", ...],
     criteria: ['all'],
     judge_model: 'claude-3-sonnet'
   })

4. Backend:
   - Calls /api/evaluation/judge with message_ids
   - LLM judge evaluates each message on 5 criteria
   - Saves 15 judgments to database (3 messages √ó 5 criteria)
   - Returns summary statistics

5. Assistant responds:
   "I've evaluated 3 responses using Claude 3 Sonnet:

   Average Scores:
   - Helpfulness: 8.2/10 ‚úÖ
   - Accuracy: 9.1/10 ‚úÖ
   - Clarity: 8.5/10 ‚úÖ
   - Safety: 9.3/10 ‚úÖ
   - Completeness: 7.8/10 ‚úÖ

   Overall pass rate: 87% (13/15 evaluations passed)

   Issues found:
   - Message msg-2: Low helpfulness (5.5/10) - Response too vague
   - Would you like me to provide specific improvement suggestions?"
```

---

## Integration with Existing Systems

### 1. Uses Existing LLM-Judge API ‚úÖ

**Endpoint**: `/api/evaluation/judge`
**Location**: `app/api/evaluation/judge/route.ts`

**Why**: Complete implementation already exists
- Supports batch evaluation (5 concurrent requests)
- 5 standard criteria with passing scores
- Auto-saves to `judgments` table
- Includes evidence extraction (reasoning, suggestions)

---

### 2. Saves to `judgments` Table ‚úÖ

**Table**: `judgments`
**Schema**:
```sql
judgments:
  - id (UUID)
  - message_id (UUID, FK to messages)
  - user_id (UUID, FK to auth.users)
  - judge_type ('llm')
  - judge_name ('claude-3-sonnet', 'gpt-4-turbo', etc.)
  - criterion ('helpfulness', 'accuracy', etc.)
  - score (0-10)
  - passed (boolean)
  - evidence_json (JSONB with reasoning, confidence, aspects, suggestions)
  - notes
  - created_at
```

**Source tagging**: Currently saves with generic 'llm' type. Future enhancement: add `source` field to distinguish `analytics_assistant` vs `manual` vs `batch_test`.

---

### 3. Cost Estimates

**Claude 3 Sonnet** (default):
- ~$0.005 per message evaluated
- 3 messages √ó 5 criteria = 15 evaluations = ~$0.075 total

**GPT-4 Turbo**:
- ~$0.015 per message evaluated
- 3 messages √ó 5 criteria = 15 evaluations = ~$0.225 total

**Recommendation**: Use Claude Sonnet for routine evaluations, GPT-4 Turbo for critical assessments.

---

## Testing Checklist

### 1. Basic Functionality Test

**Steps**:
1. Go to `/analytics/chat`
2. Select a session with conversations
3. Send message: "Evaluate the quality of all responses in this session"
4. Expected: Assistant should:
   - Call `get_session_conversations` first
   - Extract assistant message IDs
   - Call `evaluate_messages`
   - Return detailed scores and analysis

---

### 2. Criteria Selection Test

**Test**: "Evaluate just the helpfulness and accuracy of responses"

**Expected**: Assistant should call evaluate_messages with:
```json
{
  "message_ids": [...],
  "criteria": ["helpfulness", "accuracy"]
}
```

---

### 3. Judge Model Selection Test

**Test**: "Use GPT-4 Turbo to evaluate the responses"

**Expected**: Assistant should call evaluate_messages with:
```json
{
  "message_ids": [...],
  "judge_model": "gpt-4-turbo"
}
```

---

### 4. Database Verification Test

**After evaluation, check database**:
```sql
SELECT
  id,
  message_id,
  judge_type,
  judge_name,
  criterion,
  score,
  passed,
  evidence_json->'reasoning' as reasoning,
  created_at
FROM judgments
WHERE judge_type = 'llm'
ORDER BY created_at DESC
LIMIT 20;
```

**Expected**:
- `judge_type` = 'llm'
- `judge_name` = 'claude-3-sonnet' (or selected model)
- `criterion` = one of 5 standard criteria
- `score` = 0-10
- `passed` = true/false based on passing_score
- `evidence_json` contains reasoning, confidence, aspects, suggestions

---

### 5. Error Handling Test

**Test**: "Evaluate responses" (without any messages)

**Expected**: Assistant should:
1. Call get_session_conversations
2. Realize there are no conversations
3. Tell user "No conversations found in this session"

**Test**: Invalid message ID

**Expected**: Judge API returns 404, tool returns error, assistant explains to user

---

## What's Next (Future Enhancements)

### Phase 2: Validation Sets ‚è≠Ô∏è

**Goal**: Let user upload "golden" prompt-response pairs and compare actual responses to validation set

**Tasks**:
1. Create `validation_sets` table
2. Add validation set upload UI
3. Add `compare_to_validation_set` tool to assistant
4. Implement semantic similarity comparison

**Doc**: See `ANALYTICS_VALIDATION_SET_PROPOSAL.md`

---

### Phase 3: Source Field Enhancement ‚è≠Ô∏è

**Goal**: Distinguish evaluation sources (analytics_assistant vs manual vs batch_test)

**Changes**:
1. Add `source` field to `judgments` table
2. Update `/api/evaluation/judge` to accept `source` parameter
3. Update analytics tool to pass `source='analytics_assistant'`
4. Enable filtering by source in analytics queries

---

## Summary

### What Works Now ‚úÖ
- Analytics assistant has `evaluate_messages` tool
- Can evaluate any number of messages on 5 criteria
- Uses existing LLM-judge infrastructure
- Auto-saves to database
- Returns detailed scores with reasoning
- Supports model selection (Claude Sonnet, GPT-4 Turbo, Claude Opus)

### What's Ready to Test ‚úÖ
- Go to `/analytics/chat`
- Select any session
- Ask: "Evaluate the quality of responses in this session"
- Assistant will evaluate and report back

### What's Next üîÑ
- Test the implementation
- Create validation sets feature
- Add source field tagging

**Status**: READY FOR USER TESTING! üöÄ
