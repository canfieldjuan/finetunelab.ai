# RunPod Serverless & HuggingFace Upload - Implementation Plan

**Date Created:** 2025-11-28
**Last Updated:** 2025-11-28
**Status:** Phase 1 Complete - In Implementation
**Priority:** High - Blocking end-to-end model deployment pipeline

---

## Implementation Status

### Phase 1: HuggingFace Upload Fix ‚úÖ COMPLETE

**Date Completed:** 2025-11-28

**Files Modified:**
- `/lib/training/runpod-service.ts` (Python training script embedded in TypeScript)

**Changes Implemented:**

1. **File Copying from Base Model Cache** (lines 841-894)
   - Copies `tokenizer_config.json`, `special_tokens_map.json`, `generation_config.json`
   - Constructs cache path dynamically: `/root/.cache/huggingface/hub/models--{model_name}/snapshots`
   - Robust error handling with continue-on-failure
   - Debug logging for each file operation

2. **.gitattributes Creation** (lines 908-930)
   - Creates Git LFS tracking for 8 file types
   - Includes: bin, safetensors, h5, msgpack, onnx, arrow, pt, pth
   - Try-catch wrapper with graceful fallback

3. **File Verification Before Upload** (lines 994-1030)
   - Validates required files: config.json, README.md
   - Logs optional files with file sizes
   - Aborts upload if required files missing
   - Reports total file count before upload

4. **Enhanced Debug Logging** (lines 843, 851, 878, 886, 927, 1004-1045)
   - Cache path discovery
   - File copy operations (success/skip/missing)
   - File verification results with sizes
   - Upload progress and completion
   - HuggingFace model URL

**Breaking Changes:** NONE
**Rollback Complexity:** Simple - revert file changes

**Next Steps:**
- Run training job with HF upload enabled
- Verify all files uploaded to HuggingFace
- Check "Text Generation" badge appears

### Phase 2: RunPod Serverless Handler Fix ‚úÖ COMPLETE

**Date Completed:** 2025-11-28

**Files Modified:**
- `/lib/inference/runpod-serverless-service.ts`

**Changes Implemented:**

1. **Updated Environment Variables** (lines 508-524)
   - Changed `MAX_MODEL_LEN` to `MAX_MODEL_LENGTH` (correct worker-vllm variable)
   - Added `TRUST_REMOTE_CODE: true`
   - Added `MODEL_REVISION: main`
   - Added `BASE_PATH: /runpod-volume`
   - Extracted defaults to constants (no hardcoding)
   - Debug logging of configuration

2. **Changed Docker Image** (lines 554-577)
   - Changed from `runpod/worker-v1-vllm:stable-cuda12.1.0` (Pod image)
   - To `runpod/worker-vllm:stable` (Serverless worker image)
   - Increased container disk: 20GB ‚Üí 30GB
   - Increased volume size: 50GB ‚Üí 100GB
   - All sizes extracted to constants

3. **Updated Endpoint URL** (lines 672-678, 682)
   - Changed from `/run` (async) to `/runsync` (synchronous)
   - Logged both sync and async URLs
   - Worker-vllm supports OpenAI-compatible API

4. **Enhanced Debug Logging** (lines 506, 520-524, 529-531, 540-545, 594-606, 657-659, 675-677, 691)
   - Environment variable configuration
   - HF token presence/absence
   - Custom variables (truncated for security)
   - Template creation progress
   - Endpoint ID and name
   - Both URL formats logged
   - Error logging with context

**Breaking Changes:** NONE
**Rollback Complexity:** Simple - revert file changes

**Next Steps:**
- Deploy serverless endpoint via UI
- Check logs for worker-vllm image usage
- Test inference request to /runsync endpoint
- Verify request processes (doesn't queue)

### Phase 3: Auto-Register Models ‚úÖ COMPLETE

**Date Completed:** 2025-11-28

**Files Modified:**
- `/app/api/training/deploy/route.ts`

**Changes Implemented:**

1. **Model Entry Creation** (lines 518-540)
   - Extracts model name from deploymentName or generates from hfModelId
   - Defines default values as constants (no hardcoding)
   - Checks for existing model entry before insert
   - Debug logging for model name and base URL

2. **Model Data Object** (lines 542-578)
   - Comprehensive model metadata
   - Training information: method, dataset, date
   - Deployment information: endpoint ID, URL, type
   - Cost tracking: cost per request, budget limit
   - Context length from config or default
   - All values extracted from variables

3. **Insert or Update Logic** (lines 580-624)
   - Try-catch wrapper (non-fatal failure)
   - Updates existing model if found
   - Inserts new model if not found
   - Tracks isNewModel flag
   - Detailed success/error logging
   - Continues deployment even if model registration fails

4. **Response Enhancement** (lines 626-644)
   - Adds model_id to response for redirect
   - Dynamic success message based on registration
   - Debug logging of model_id
   - DeployModelButton already handles redirect (line 199)

5. **Debug Logging** (lines 519-520, 525-526, 537-540, 573-578, 586, 600, 602, 616, 619-623, 631-632)
   - Model creation start
   - Model name and URL
   - Existing vs new model
   - Data preparation summary
   - Insert/update operations
   - Success/failure messages
   - Response preparation

**Breaking Changes:** NONE
**Rollback Complexity:** Simple - revert file changes

**Integration Points:**
- DeployModelButton.tsx (line 199): Handles redirect to /models?modelId=xxx
- Models page: Highlights new model automatically
- Chat portal: Model appears in dropdown immediately

**Next Steps:**
- Deploy serverless model end-to-end
- Verify model entry created in database
- Check redirect to models page works
- Verify model appears in chat dropdown
- Test chat inference with serverless model

---

## Executive Summary

This document outlines the phased implementation plan to fix two critical issues blocking the RunPod Serverless deployment pipeline:

1. **Serverless Pod Handler Missing**: Requests queue indefinitely because no handler function processes them
2. **HuggingFace Upload Incomplete**: Missing config files prevent HF from recognizing models as text-generation models

Both issues are independently fixable and require targeted code additions without breaking changes.

---

## Current State Analysis

### Issue 1: Serverless Pod Requests Stuck in Queue

**File:** `/lib/inference/runpod-serverless-service.ts` (lines 496-566)

**Current Behavior:**
```typescript
// Line 543 - Using vLLM Pod image for serverless (INCORRECT)
imageName: request.docker_image || 'runpod/worker-v1-vllm:stable-cuda12.1.0'

// Line 547 - Empty docker args (NO HANDLER SPECIFIED)
dockerArgs: ''

// Lines 506-524 - Only environment variables set
envVars: [
  { key: 'MODEL_NAME', value: request.model_storage_url },
  { key: 'MAX_MODEL_LEN', value: '4096' },
  { key: 'GPU_MEMORY_UTILIZATION', value: '0.95' },
]
```

**Root Cause:**
- Using `runpod/worker-v1-vllm:stable-cuda12.1.0` which is for **Pod deployments**, not serverless
- Pod image runs HTTP server, serverless requires `runpod.serverless.start()` handler
- No `handler.py` file generated or bundled with template
- Workers spin up but have no code to execute when requests arrive

**Evidence:**
- Serverless endpoint creates successfully (endpoint_id returned)
- Requests accept but stay in queue indefinitely
- No 404/500 errors (would indicate handler exists but failing)

### Issue 2: HuggingFace Upload Missing Model Config Files

**File:** `/lib/training/runpod-service.ts` (lines 859-920)

**Current Upload Process:**
```python
# Lines 821-822, 838-839 - Model saving
merged_model.save_pretrained(save_path, safe_serialization=True)
tokenizer.save_pretrained(save_path)

# Lines 865-913 - Create README.md ONLY
readme_content = f"""---
pipeline_tag: text-generation
language: en
...
"""

# Lines 915-920 - Upload entire folder
api.upload_folder(
    folder_path=save_path,  # /workspace/fine_tuned_model
    repo_id=HF_REPO_NAME,
    token=HF_TOKEN,
    repo_type="model"
)
```

**Files Currently Uploaded:**
- ‚úÖ `pytorch_model.bin` or `model.safetensors` (model weights)
- ‚úÖ `tokenizer.json` (tokenizer vocab)
- ‚úÖ `config.json` (model architecture)
- ‚úÖ `README.md` (with pipeline_tag)

**Missing Files:**
- ‚ùå `tokenizer_config.json` - Tokenizer settings (chat template, padding, truncation)
- ‚ùå `special_tokens_map.json` - Special tokens (BOS, EOS, PAD, UNK)
- ‚ùå `generation_config.json` - Default generation parameters (temperature, top_p, max_length)
- ‚ùå `.gitattributes` - Git LFS tracking for large files

**Impact:**
- HF can't properly configure tokenizer for chat/instruction format
- Missing generation defaults (users must manually configure)
- Special tokens may be incorrect (breaks chat templates)
- Model appears "incomplete" in HuggingFace UI

---

## Verified Dependencies

### RunPod Worker-vLLM (Official Template)

**Source:** [GitHub - runpod-workers/worker-vllm](https://github.com/runpod-workers/worker-vllm)

**Required Handler Structure:**
```python
import os
import runpod
from utils import JobInput
from engine import vLLMEngine, OpenAIvLLMEngine

vllm_engine = vLLMEngine()
openai_engine = OpenAIvLLMEngine(vllm_engine)

async def handler(job):
    """Process each inference request"""
    job_input = JobInput(job["input"])
    engine = openai_engine if job_input.openai_route else vllm_engine
    results_generator = engine.generate(job_input)

    async for batch in results_generator:
        yield batch

# CRITICAL: This registers the handler with RunPod
runpod.serverless.start({
    "handler": handler,
    "concurrency_modifier": lambda x: vllm_engine.max_concurrency,
    "return_aggregate_stream": True,
})
```

**Dependencies (from requirements.txt):**
- `vllm==0.10.0`
- `flashinfer` (CUDA 12.1)
- `runpod` (serverless SDK)
- `transformers`
- `torch`
- `accelerate`

**Docker Image:**
- Base: `runpod/base:0.7.2-cuda12.1.0`
- Or use pre-built: `runpod/worker-v1-vllm:stable-cuda12.1.0` (for **PODS ONLY**, not serverless)

### HuggingFace Model File Requirements

**Source:** [HuggingFace Model Hub Documentation](https://huggingface.co/docs/hub/models-cards)

**Required Files for Text-Generation Recognition:**
1. `config.json` - Model architecture config
2. `tokenizer.json` or `tokenizer.model` - Tokenizer vocabulary
3. `tokenizer_config.json` - Tokenizer settings
4. `special_tokens_map.json` - Special token definitions
5. `generation_config.json` - Inference parameters
6. `README.md` - Model card with `pipeline_tag: text-generation`
7. `.gitattributes` - LFS tracking

---

## Phased Implementation Plan

### Phase 1: Fix HuggingFace Upload (Low Risk, Quick Win)

**Objective:** Add missing config files to HF upload

**Files to Modify:**
- `/lib/training/runpod-service.ts` (lines 859-920)

**Implementation Steps:**

1. **Before upload, copy missing files from base model** (insert after line 839):
   ```python
   # Copy missing tokenizer/generation config files from base model
   import shutil
   import os

   # Get base model cache path
   base_model_cache = f"/root/.cache/huggingface/hub/models--{model_name.replace('/', '--')}/snapshots"

   # Find latest snapshot
   if os.path.exists(base_model_cache):
       snapshots = os.listdir(base_model_cache)
       if snapshots:
           latest_snapshot = os.path.join(base_model_cache, snapshots[-1])

           # Files to copy from base model
           files_to_copy = [
               'tokenizer_config.json',
               'special_tokens_map.json',
               'generation_config.json',
           ]

           for file_name in files_to_copy:
               src = os.path.join(latest_snapshot, file_name)
               dest = os.path.join(save_path, file_name)

               if os.path.exists(src) and not os.path.exists(dest):
                   logger.info(f"[HF Upload] Copying {file_name} from base model")
                   shutil.copy(src, dest)
               elif not os.path.exists(src):
                   logger.warning(f"[HF Upload] {file_name} not found in base model")
   ```

2. **Create .gitattributes for LFS tracking** (insert after step 1):
   ```python
   # Create .gitattributes for Git LFS
   gitattributes_path = os.path.join(save_path, '.gitattributes')
   gitattributes_content = """*.bin filter=lfs diff=lfs merge=lfs -text
*.safetensors filter=lfs diff=lfs merge=lfs -text
*.h5 filter=lfs diff=lfs merge=lfs -text
*.msgpack filter=lfs diff=lfs merge=lfs -text
*.onnx filter=lfs diff=lfs merge=lfs -text
"""

   logger.info("[HF Upload] Creating .gitattributes for LFS tracking")
   with open(gitattributes_path, 'w') as f:
       f.write(gitattributes_content)
   ```

3. **Verify all files exist before upload** (insert before line 915):
   ```python
   # Verify required files exist
   required_files = [
       'config.json',
       'README.md',
   ]
   optional_files = [
       'tokenizer.json',
       'tokenizer_config.json',
       'special_tokens_map.json',
       'generation_config.json',
   ]

   missing_required = []
   for file_name in required_files:
       if not os.path.exists(os.path.join(save_path, file_name)):
           missing_required.append(file_name)

   if missing_required:
       raise FileNotFoundError(f"Required files missing: {missing_required}")

   # Log optional files status
   for file_name in optional_files:
       exists = os.path.exists(os.path.join(save_path, file_name))
       status = "‚úì" if exists else "‚úó"
       logger.info(f"[HF Upload] {status} {file_name}")
   ```

**Verification:**
- Run training with HF upload enabled
- Check uploaded model on HuggingFace Hub
- Verify "Text Generation" badge appears
- Verify "Use in Transformers" button works
- Test model inference from HF Hub

**Rollback:**
- Simply remove the added code blocks
- No breaking changes to existing functionality

---

### Phase 2: Implement RunPod Serverless Handler (Medium Risk)

**Objective:** Use official RunPod worker-vllm template instead of custom handler

**Decision:** Use RunPod's **pre-built worker-vllm template** instead of creating custom handler

**Why Use Pre-built Template:**
- ‚úÖ Maintained by RunPod (updates, bug fixes, security)
- ‚úÖ Production-tested and optimized
- ‚úÖ Includes all dependencies pre-installed
- ‚úÖ OpenAI-compatible API out of the box
- ‚úÖ No custom handler code to maintain
- ‚úÖ Supports streaming and batching
- ‚ùå Less control over inference parameters (acceptable trade-off)

**Files to Modify:**
- `/lib/inference/runpod-serverless-service.ts` (lines 496-566)

**Implementation Steps:**

1. **Change template creation to use official worker-vllm** (line 543):
   ```typescript
   // OLD (INCORRECT - Pod image):
   imageName: request.docker_image || 'runpod/worker-v1-vllm:stable-cuda12.1.0'

   // NEW (CORRECT - Serverless worker):
   imageName: request.docker_image || 'runpod/worker-vllm:stable'
   ```

2. **Update environment variables for worker-vllm** (lines 506-524):
   ```typescript
   const envVars: Array<{ key: string; value: string }> = [
       // Model configuration
       { key: 'MODEL_NAME', value: request.model_storage_url },

       // vLLM engine configuration
       { key: 'MAX_MODEL_LENGTH', value: '4096' },  // Changed from MAX_MODEL_LEN
       { key: 'GPU_MEMORY_UTILIZATION', value: '0.95' },
       { key: 'TRUST_REMOTE_CODE', value: 'true' },

       // Model revision (optional)
       { key: 'MODEL_REVISION', value: 'main' },

       // Base path for model storage
       { key: 'BASE_PATH', value: '/runpod-volume' },
   ];

   // Add HuggingFace token if provided
   if (request.environment_variables?.HUGGINGFACE_TOKEN) {
       envVars.push({ key: 'HF_TOKEN', value: request.environment_variables.HUGGINGFACE_TOKEN });
   }
   ```

3. **Update template configuration** (lines 540-550):
   ```typescript
   const variables = {
       input: {
           name: templateName,
           imageName: 'runpod/worker-vllm:stable',  // Official worker image
           isServerless: true,
           containerDiskInGb: 30,  // Increased for vLLM
           volumeInGb: 100,  // Increased for model weights + cache
           dockerArgs: '',  // Worker handles its own entrypoint
           env: envVars,
       },
   };
   ```

4. **Update endpoint URL format** (line 652):
   ```typescript
   // Worker-vLLM uses /runsync endpoint for synchronous requests
   endpoint_url: `https://api.runpod.ai/v2/${endpoint.id}/runsync`,

   // For async/streaming: https://api.runpod.ai/v2/${endpoint.id}/run
   ```

**Verification:**
- Deploy serverless endpoint with trained model
- Send test inference request to `/runsync` endpoint
- Verify request processes (doesn't queue indefinitely)
- Check response format matches OpenAI API structure
- Test with streaming endpoint `/run`

**Rollback:**
- Revert image name to original
- Revert environment variable changes
- No database schema changes needed

---

### Phase 3: Add Model Auto-Registration (Medium Risk)

**Objective:** Automatically create model entry in `llm_models` table after serverless deployment

**Files to Modify:**
- `/app/api/training/deploy/route.ts` (lines 462-528)

**Implementation Steps:**

1. **Create model entry after serverless deployment** (insert after line 516):
   ```typescript
   // Create model entry in llm_models table for chat portal access
   const modelName = deploymentName || `${hfModelId}-serverless`;

   // Check if model already exists
   const { data: existingModel } = await supabase
       .from('llm_models')
       .select('id')
       .eq('user_id', userId)
       .eq('name', modelName)
       .single();

   const modelData = {
       user_id: userId,
       provider: 'runpod',
       name: modelName,
       model_id: hfModelId,
       base_url: serverlessResponse.endpoint_url,
       served_model_name: hfModelId,
       is_global: false,
       enabled: true,
       auth_type: 'none',  // RunPod handles auth via API key
       supports_streaming: true,
       supports_functions: false,
       supports_vision: false,
       context_length: config?.max_model_len || 4096,
       max_output_tokens: 2000,
       training_method: job?.training_method || 'sft',
       base_model: hfModelId,
       training_dataset: job?.dataset_path || null,
       training_date: job?.completed_at || new Date().toISOString(),
       metadata: {
           training_job_id: job_id,
           runpod_endpoint_id: serverlessResponse.endpoint_id,
           runpod_endpoint_url: serverlessResponse.endpoint_url,
           deployment_type: 'runpod_serverless',
           deployed_at: new Date().toISOString(),
           cost_per_request: serverlessResponse.cost.cost_per_request,
           budget_limit: config?.budget_limit || 10,
       },
   };

   let modelEntry;
   if (existingModel) {
       // Update existing model
       const { data } = await supabase
           .from('llm_models')
           .update(modelData)
           .eq('id', existingModel.id)
           .select()
           .single();
       modelEntry = data;
       console.log('[DeployAPI] Model entry updated:', existingModel.id);
   } else {
       // Insert new model
       const { data } = await supabase
           .from('llm_models')
           .insert(modelData)
           .select()
           .single();
       modelEntry = data;
       console.log('[DeployAPI] Model entry created:', data?.id);
   }
   ```

2. **Return model_id for redirect** (update lines 519-528):
   ```typescript
   return NextResponse.json({
       success: true,
       endpoint_id: serverlessResponse.endpoint_id,
       deployment_id: serverlessResponse.deployment_id,
       status: serverlessResponse.status,
       endpoint_url: serverlessResponse.endpoint_url,
       model_id: modelEntry?.id,  // NEW: For redirect to /models?modelId=xxx
       inference_deployment_id: inferenceDeployment?.id,
       deployment_name: deploymentName!,
       message: 'Serverless endpoint created! Model registered and ready to use in chat.',
   });
   ```

3. **DeployModelButton.tsx already handles redirect** (lines 198-202 - NO CHANGES NEEDED):
   ```typescript
   // This code already exists and will work once we return model_id
   if (data.model_id) {
       router.push(`/models?modelId=${data.model_id}`);
   } else {
       router.push('/models');
   }
   ```

**Verification:**
- Deploy serverless model
- Check `llm_models` table for new entry
- Verify redirect to `/models?modelId=xxx`
- Check model appears in chat portal dropdown
- Send chat message using serverless model
- Verify inference works end-to-end

**Rollback:**
- Remove model creation code
- Model entries can be manually deleted from database
- Does not affect serverless endpoint (still functional)

---

## Dependency Map

### Files Modified (in order):

1. `/lib/training/runpod-service.ts` (Phase 1)
   - Lines 859-920: HF upload section
   - Impact: Training job completion
   - Dependencies: None
   - Breaking changes: None

2. `/lib/inference/runpod-serverless-service.ts` (Phase 2)
   - Lines 496-566: Template creation
   - Lines 649-666: Response formatting
   - Impact: Serverless endpoint creation
   - Dependencies: None
   - Breaking changes: None (only affects new deployments)

3. `/app/api/training/deploy/route.ts` (Phase 3)
   - Lines 462-528: Serverless deployment flow
   - Impact: Model registration
   - Dependencies: Phase 2 must work first
   - Breaking changes: None (adds functionality)

### External Dependencies:

- **RunPod API:** GraphQL endpoint for template/endpoint creation
- **HuggingFace Hub:** Model upload destination
- **Supabase Tables:**
  - `llm_models` (Phase 3)
  - `inference_deployments` (already used)
  - `local_training_jobs` (already used)

### Integration Points:

- `/components/training/DeployModelButton.tsx` - Already supports redirect
- `/app/models/page.tsx` - Already supports modelId highlighting
- `/app/api/chat/route.ts` - Already fetches from llm_models
- `/components/models/ModelSelector.tsx` - Already lists llm_models

---

## Risk Assessment

### Phase 1: HuggingFace Upload (LOW RISK)

**Risk Level:** üü¢ Low

**Potential Issues:**
- Base model cache path might not exist
- Files might be missing from base model

**Mitigations:**
- Check for file existence before copying
- Log warnings for missing optional files
- Don't fail upload if optional files missing
- Only fail if required files (config.json, README.md) missing

**Rollback Complexity:** Simple - remove code blocks

---

### Phase 2: RunPod Serverless Handler (MEDIUM RISK)

**Risk Level:** üü° Medium

**Potential Issues:**
- Official worker-vllm might have breaking API changes
- Model might fail to load in worker environment
- GPU memory issues with large models

**Mitigations:**
- Pin to stable tag: `runpod/worker-vllm:stable`
- Test with small model first (< 7B parameters)
- Monitor RunPod worker logs for errors
- Increase volume size to 100GB for model cache

**Rollback Complexity:** Simple - revert image name

---

### Phase 3: Model Auto-Registration (MEDIUM RISK)

**Risk Level:** üü° Medium

**Potential Issues:**
- Model table entry fails but serverless works
- Duplicate model entries
- Model redirect breaks chat portal

**Mitigations:**
- Check for existing model by name before insert
- Wrap in try-catch (don't fail deployment if model registration fails)
- Log all database operations
- Add model_id to response only if entry created successfully

**Rollback Complexity:** Medium - delete model entries from database

---

## Testing Plan

### Phase 1 Testing: HuggingFace Upload

**Pre-conditions:**
- Training job completes successfully
- HF_TOKEN and HF_REPO_NAME environment variables set

**Test Cases:**

1. **Test missing files are copied:**
   - Run training job
   - Check `/workspace/fine_tuned_model` for all files
   - Verify `tokenizer_config.json`, `special_tokens_map.json`, `generation_config.json` exist

2. **Test HF model recognition:**
   - Open HuggingFace model page
   - Verify "Text Generation" badge appears
   - Verify "Use in Transformers" button works
   - Click "Use in Transformers" and verify code snippet is correct

3. **Test model inference from HF:**
   ```python
   from transformers import AutoModelForCausalLM, AutoTokenizer

   model = AutoModelForCausalLM.from_pretrained("your-username/model-name")
   tokenizer = AutoTokenizer.from_pretrained("your-username/model-name")

   messages = [{"role": "user", "content": "Hello!"}]
   inputs = tokenizer.apply_chat_template(messages, return_tensors="pt")
   outputs = model.generate(inputs, max_new_tokens=100)
   print(tokenizer.decode(outputs[0]))
   ```

4. **Test .gitattributes created:**
   - Clone model repo locally
   - Verify large files tracked by Git LFS
   - Check `.git/lfs/objects` directory for weights

**Success Criteria:**
- ‚úÖ All files present in uploaded model
- ‚úÖ "Text Generation" badge visible on HF
- ‚úÖ Model loads successfully with AutoModel
- ‚úÖ Chat template applies correctly
- ‚úÖ Git LFS tracks large files

---

### Phase 2 Testing: RunPod Serverless Handler

**Pre-conditions:**
- RunPod API key configured in secrets vault
- Trained model uploaded to HuggingFace

**Test Cases:**

1. **Test endpoint creation:**
   - Deploy serverless endpoint via UI
   - Verify endpoint_id returned
   - Check RunPod dashboard for endpoint status

2. **Test inference request (synchronous):**
   ```bash
   curl -X POST https://api.runpod.ai/v2/{endpoint_id}/runsync \
     -H "Authorization: Bearer $RUNPOD_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{
       "input": {
         "prompt": "Hello, how are you?",
         "max_tokens": 100,
         "temperature": 0.7
       }
     }'
   ```

3. **Test inference request (OpenAI format):**
   ```python
   import openai

   client = openai.OpenAI(
       base_url=f"https://api.runpod.ai/v2/{endpoint_id}/openai/v1",
       api_key=runpod_api_key
   )

   response = client.chat.completions.create(
       model="model-name",
       messages=[{"role": "user", "content": "Hello!"}]
   )

   print(response.choices[0].message.content)
   ```

4. **Test streaming:**
   ```python
   response = client.chat.completions.create(
       model="model-name",
       messages=[{"role": "user", "content": "Write a story"}],
       stream=True
   )

   for chunk in response:
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

5. **Test worker logs:**
   - Check RunPod logs for errors
   - Verify model loads successfully
   - Check inference time and GPU utilization

**Success Criteria:**
- ‚úÖ Endpoint creates successfully
- ‚úÖ Request processes (doesn't queue forever)
- ‚úÖ Response format matches OpenAI API
- ‚úÖ Streaming works
- ‚úÖ No errors in worker logs

---

### Phase 3 Testing: Model Auto-Registration

**Pre-conditions:**
- Phase 2 completed and working
- Database access configured

**Test Cases:**

1. **Test model entry creation:**
   - Deploy serverless model
   - Check `llm_models` table for new entry
   - Verify all fields populated correctly

2. **Test redirect:**
   - Complete deployment
   - Verify redirect to `/models?modelId=xxx`
   - Check model highlighted in UI

3. **Test chat portal integration:**
   - Open chat portal
   - Verify model appears in dropdown
   - Select serverless model
   - Send test message
   - Verify response received

4. **Test duplicate handling:**
   - Deploy same model twice
   - Verify only one entry exists
   - Check updated entry has latest deployment info

5. **Test model deletion:**
   - Delete serverless endpoint in RunPod
   - Verify model entry remains (intentional)
   - User can manually disable/delete model

**Success Criteria:**
- ‚úÖ Model entry created in database
- ‚úÖ Redirect works to models page
- ‚úÖ Model appears in chat dropdown
- ‚úÖ Chat inference works end-to-end
- ‚úÖ Duplicates handled gracefully

---

## Implementation Order

### Recommended Sequence:

1. **Start with Phase 1 (HF Upload)** - Lowest risk, independent
   - Immediate value: Models work in HF Hub
   - No dependencies on other phases
   - Can test immediately after training

2. **Then Phase 2 (Serverless Handler)** - Enables serverless deployments
   - Unblocks serverless pipeline
   - Required for Phase 3
   - Can test with existing HF models

3. **Finally Phase 3 (Auto-Registration)** - Completes end-to-end flow
   - Requires Phase 2 working
   - Ties everything together
   - Enables seamless user experience

### Alternative Sequence (if urgent):

- **Phase 2 + Phase 3 together** - If serverless is critical
  - Implement both in one session
  - Test end-to-end flow immediately
  - Higher risk but faster completion

---

## Validation Checklist

### Before Implementation:

- [ ] All files read and understood
- [ ] Dependencies mapped completely
- [ ] No breaking changes identified
- [ ] Rollback plan documented
- [ ] Test environment prepared

### Phase 1 Validation:

- [ ] Training completes successfully
- [ ] All files copied from base model
- [ ] .gitattributes created
- [ ] HF upload succeeds
- [ ] Model shows "Text Generation" badge
- [ ] Model loads with AutoModel

### Phase 2 Validation:

- [ ] Template created with worker-vllm image
- [ ] Endpoint created successfully
- [ ] Test request processes (doesn't queue)
- [ ] Response format correct
- [ ] Streaming works
- [ ] Worker logs show no errors

### Phase 3 Validation:

- [ ] Model entry created in database
- [ ] Redirect to models page works
- [ ] Model appears in chat dropdown
- [ ] Chat inference works
- [ ] Duplicate handling works
- [ ] Cost tracking works

---

## Breaking Changes Analysis

### Phase 1: HuggingFace Upload
**Breaking Changes:** ‚ùå NONE

- Only adds files to upload
- Doesn't modify existing file structure
- Doesn't change API contracts
- Backward compatible with existing models

### Phase 2: RunPod Serverless Handler
**Breaking Changes:** ‚ùå NONE

- Only affects **new** serverless deployments
- Existing endpoints continue working
- Doesn't modify database schema
- Doesn't change API response format

### Phase 3: Model Auto-Registration
**Breaking Changes:** ‚ùå NONE

- Only **adds** model entries (doesn't modify existing)
- Doesn't change llm_models schema
- Doesn't affect existing models
- Optional feature (deployment works without it)

---

## Session Context for Continuity

### Current Session Accomplishments:

1. ‚úÖ Investigated serverless queue issue
2. ‚úÖ Identified missing handler as root cause
3. ‚úÖ Investigated HF upload missing files
4. ‚úÖ Verified RunPod worker-vllm template
5. ‚úÖ Created comprehensive implementation plan

### Previous Session Issues (from RUNPOD_TRAINING_EVALUATION_FIX.md):

1. ‚úÖ HuggingFace authentication for gated models
2. ‚úÖ Hardcoded training parameters
3. ‚úÖ Evaluation strategy parameter errors
4. ‚úÖ CUDA OOM during evaluation
5. ‚úÖ vLLM max context length crash
6. ‚úÖ UI control for max_model_len configuration

### Still Pending After This Implementation:

- Metrics overwhelming training monitor UI (downsampling needed)
- Checkpoint upload to HuggingFace Hub (already works per user)
- Serverless cost tracking and budget alerts
- Multi-GPU tensor parallelism for serverless

---

## References

### RunPod Documentation:
- [Deploy a vLLM worker](https://docs.runpod.io/serverless/vllm/get-started)
- [RunPod Serverless Overview](https://docs.runpod.io/serverless/workers/vllm/overview)
- [Build a load balancing vLLM endpoint](https://docs.runpod.io/serverless/load-balancing/vllm-worker)

### GitHub Repositories:
- [runpod-workers/worker-vllm](https://github.com/runpod-workers/worker-vllm)
- [worker-vllm handler.py](https://github.com/runpod-workers/worker-vllm/blob/main/src/handler.py)
- [worker-vllm Dockerfile](https://github.com/runpod-workers/worker-vllm/blob/main/Dockerfile)

### HuggingFace Documentation:
- [Model Hub Documentation](https://huggingface.co/docs/hub/models-cards)
- [Text Generation Models](https://huggingface.co/docs/hub/models-the-hub#text-generation)
- [Git LFS Documentation](https://git-lfs.github.com/)

---

**Author:** Claude (Sonnet 4.5)
**Last Updated:** 2025-11-28
**Review Status:** All Phases Complete - Ready for Testing
**Implementation Date:** 2025-11-28

---

## Implementation Complete Summary

### All Three Phases Implemented Successfully

**Phase 1: HuggingFace Upload Fix** ‚úÖ
- File: `/lib/training/runpod-service.ts`
- Lines modified: 841-1046
- Copies missing config files from base model cache
- Creates .gitattributes for Git LFS
- Verifies all files before upload
- Comprehensive debug logging

**Phase 2: RunPod Serverless Handler Fix** ‚úÖ
- File: `/lib/inference/runpod-serverless-service.ts`
- Lines modified: 506-606, 656-692
- Changed Docker image to official worker-vllm
- Updated environment variables for serverless
- Fixed endpoint URL format (/runsync)
- Enhanced error handling and logging

**Phase 3: Auto-Register Models** ‚úÖ
- File: `/app/api/training/deploy/route.ts`
- Lines modified: 518-644
- Creates model entry in llm_models table
- Returns model_id for automatic redirect
- Enables immediate chat portal integration
- Non-fatal failure handling

### Code Quality Metrics

‚úÖ **No Hardcoded Values** - All defaults extracted to constants
‚úÖ **No Unicode in Python** - Only ASCII in Python code blocks
‚úÖ **Robust Error Handling** - Try-catch at all critical points
‚úÖ **Debug Logging** - Comprehensive logging at breaking points
‚úÖ **Logical Blocks** - All changes in 15-54 line blocks
‚úÖ **No Breaking Changes** - All phases backward compatible
‚úÖ **TypeScript Valid** - No compilation errors

### Testing Checklist

- [ ] Phase 1: Run training with HF upload, verify files on HuggingFace
- [ ] Phase 2: Deploy serverless endpoint, check logs for worker-vllm
- [ ] Phase 2: Send test request to /runsync, verify response
- [ ] Phase 3: Verify model entry in llm_models table
- [ ] Phase 3: Check redirect to /models?modelId=xxx
- [ ] Phase 3: Verify model in chat dropdown
- [ ] End-to-End: Train ‚Üí Upload ‚Üí Deploy ‚Üí Chat

### Rollback Instructions

If issues arise, revert in reverse order:

```bash
# Revert Phase 3
git checkout app/api/training/deploy/route.ts

# Revert Phase 2
git checkout lib/inference/runpod-serverless-service.ts

# Revert Phase 1
git checkout lib/training/runpod-service.ts
```

Or revert all at once:
```bash
git checkout app/api/training/deploy/route.ts lib/inference/runpod-serverless-service.ts lib/training/runpod-service.ts
```
