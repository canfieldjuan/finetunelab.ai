import axios from 'axios';

class RunPodServerlessService {
  private async graphql<T>(query: string, variables: Record<string, any>, apiKey: string): Promise<T> {
    const response = await axios.post(
      'https://api.runpod.io/graphql',
      { query, variables },
      {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`,
        },
      }
    );
    return response.data.data;
  }

  private getGPUCostPerSecond(gpuType?: RunPodServerlessGPU): number {
    const costMap: Record<RunPodServerlessGPU, number> = {
      'NVIDIA RTX A4000': 0.00012,
      'NVIDIA RTX A5000': 0.00015,
      'NVIDIA RTX A6000': 0.00026,
      'NVIDIA A40': 0.00030,
      'NVIDIA A100 40GB': 0.00044,
      'NVIDIA A100 80GB': 0.00069,
      'NVIDIA H100': 0.00145,
    };

    return costMap[gpuType || 'NVIDIA RTX A4000'] || 0.00012;
  }

  estimateCostPerRequest(gpuType?: RunPodServerlessGPU, requestDurationSeconds: number = 2): number {
    const costPerSecond = this.getGPUCostPerSecond(gpuType);
    const baseRequestFee = 0.0001;

    const computeCost = requestDurationSeconds * costPerSecond;
    const totalCost = computeCost + baseRequestFee;

    return totalCost;
  }

  estimateTotalCost(
    expectedRequests: number,
    gpuType?: RunPodServerlessGPU,
    avgRequestDurationSeconds: number = 2
  ): {
    cost_per_request: number;
    total_requests: number;
    estimated_total_cost: number;
    avg_request_duration_seconds: number;
  } {
    const costPerRequest = this.estimateCostPerRequest(gpuType, avgRequestDurationSeconds);
    const totalCost = expectedRequests * costPerRequest;

    console.log(
      `[RunPodServerlessService] Cost estimate: ${expectedRequests} requests Ã— $${costPerRequest.toFixed(4)} (${avgRequestDurationSeconds}s) = $${totalCost.toFixed(2)}`
    );

    return {
      cost_per_request: costPerRequest,
      total_requests: expectedRequests,
      estimated_total_cost: totalCost,
      avg_request_duration_seconds: avgRequestDurationSeconds,
    };
  }

  private mapGPUType(gpuType?: RunPodServerlessGPU): string {
    const gpuMap: Record<RunPodServerlessGPU, string> = {
      'NVIDIA RTX A4000': 'AMPERE_16',
      'NVIDIA RTX A5000': 'AMPERE_24',
      'NVIDIA RTX A6000': 'AMPERE_48',
      'NVIDIA A40': 'AMPERE_48',
      'NVIDIA A100 40GB': 'AMPERE_40',
      'NVIDIA A100 80GB': 'AMPERE_80',
      'NVIDIA H100': 'ADA_48',
    };

    return gpuMap[gpuType || 'NVIDIA RTX A4000'] || 'AMPERE_16';
  }

  private mapEndpointStatus(endpointStatus?: string): InferenceStatus {
    if (!endpointStatus) {
      return 'deploying';
    }

    const statusMap: Record<string, InferenceStatus> = {
      'RUNNING': 'active',
      'ACTIVE': 'active',
      'INITIALIZING': 'deploying',
      'DEPLOYING': 'deploying',
      'SCALING': 'scaling',
      'STOPPED': 'stopped',
      'FAILED': 'failed',
      'ERROR': 'error',
    };

    return statusMap[endpointStatus.toUpperCase()] || 'deploying';
  }

  isOverBudget(currentSpend: number, budgetLimit: number): boolean {
    return currentSpend >= budgetLimit;
  }

  getBudgetUtilization(currentSpend: number, budgetLimit: number): number {
    if (budgetLimit === 0) return 0;
    return (currentSpend / budgetLimit) * 100;
  }

  getBudgetAlertStatus(currentSpend: number, budgetLimit: number): {
    threshold_50: boolean;
    threshold_80: boolean;
    budget_exceeded: boolean;
  } {
    const utilization = this.getBudgetUtilization(currentSpend, budgetLimit);

    return {
      threshold_50: utilization >= 50,
      threshold_80: utilization >= 80,
      budget_exceeded: utilization >= 100,
    };
  }

  async createDeployment(deploymentConfig: RunPodServerlessDeploymentConfig): Promise<RunPodServerlessDeployment> {
    console.log('[RunPodServerlessService] Creating deployment:', deploymentConfig);

    try {
      const mutation = `
        mutation CreateEndpoint($input: CreateEndpointInput!) {
          createEndpoint(input: $input) {
            id
            name
            gpuIds
            idleTimeout
            templateId
            workersMax
            workersMin
          }
        }
      `;

      const variables = {
        input: {
          name: deploymentConfig.name,
          gpuType: this.mapGPUType(deploymentConfig.gpuType),
          idleTimeout: deploymentConfig.idleTimeout || 300,
          workersMin: deploymentConfig.workersMin || 1,
          workersMax: deploymentConfig.workersMax || 1,
        },
      };

      const data = await this.graphql<CreateEndpointResponse>(mutation, variables, deploymentConfig.apiKey);

      return {
        deployment_id: data.createEndpoint.id,
        endpoint_id: data.createEndpoint.id,
        endpoint_url: `https://api.runpod.ai/v2/${data.createEndpoint.id}/run`,
        status: 'deploying',
        cost: {
          cost_per_request: this.estimateCostPerRequest(deploymentConfig.gpuType),
          budget_limit: deploymentConfig.budgetLimit || 0,
          current_spend: 0,
          request_count: 0,
        },
        last_checked_at: new Date().toISOString(),
      };
    } catch (error) {
      console.error('[RunPodServerlessService] Create deployment failed:', error);
      throw error;
    }
  }

  async getDeploymentStatus(deploymentId: string, apiKey: string): Promise<RunPodServerlessDeploymentStatus> {
    console.log('[RunPodServerlessService] Getting deployment status:', deploymentId);

    try {
      const query = `
        query GetMyEndpoints {
          myself {
            endpoints {
              id
              name
              gpuIds
              idleTimeout
              templateId
              workersMax
              workersMin
              workersStandby
            }
          }
        }
      `;

      const data = await this.graphql<GetEndpointResponse>(query, {}, apiKey);

      const endpoint = data.myself.endpoints.find(e => e.id === deploymentId);

      if (!endpoint) {
        throw new Error(`Deployment ${deploymentId} not found`);
      }

      const status: InferenceStatus = 'active';

      const costPerRequest = this.estimateCostPerRequest('NVIDIA RTX A4000', 2);

      const cost: InferenceCost = {
        cost_per_request: costPerRequest,
        budget_limit: 0,
        current_spend: 0,
        request_count: 0,
      };

      return {
        deployment_id: endpointId,
        endpoint_id: endpointId,
        endpoint_url: `https://api.runpod.ai/v2/${endpointId}/run`,
        status,
        metrics: undefined,
        cost,
        last_checked_at: new Date().toISOString(),
      };
    } catch (error) {
      console.error('[RunPodServerlessService] Get status failed:', error);
      throw error;
    }
  }

  async stopDeployment(deploymentId: string, apiKey: string): Promise<void> {
    console.log('[RunPodServerlessService] Stopping deployment:', deploymentId);

    try {
      const mutation = `
        mutation DeleteEndpoint($id: String!) {
          deleteEndpoint(id: $id)
        }
      `;

      await this.graphql<DeleteEndpointResponse>(mutation, { id: deploymentId }, apiKey);

      console.log('[RunPodServerlessService] Deployment deleted');
    } catch (error) {
      console.error('[RunPodServerlessService] Stop deployment failed:', error);
      throw error;
    }
  }
}

export const runpodServerlessService = new RunPodServerlessService();