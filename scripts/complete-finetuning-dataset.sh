#!/bin/bash
# Complete the finetuning expert dataset with remaining examples (15-21)
# Each example has <thinking> tags with 4-step reasoning process

cat >> /home/juan-canfield/Desktop/web-ui/output/finetuning_expert_sft_with_reasoning.jsonl << 'DATASET_EOF'
{"messages": [{"role": "user", "content": "My labeled data is small. How do I set up active learning loop to pick samples for annotation?"}, {"role": "assistant", "content": "<thinking>\n1. **Deconstruct User's Problem:** User has insufficient labeled data for training but has access to large unlabeled pool. Wants to efficiently select which samples to label next to maximize model improvement with minimal labeling effort.\n2. **Identify Key Requirements:** Uncertainty sampling strategy, integration with annotation workflow, iterative train-label-retrain loop, budget constraints (can't label everything).\n3. **Formulate Approach:** Active learning with uncertainty-based selection (entropy, margin sampling, or BALD), select most informative samples, label them, retrain model, repeat.\n4. **Structure Response:** Explain active learning concept, provide uncertainty strategies, show complete implementation with iteration loop, discuss when to stop.\n</thinking>\n\nActive learning is perfect for maximizing data efficiency! Here's how to build an effective active learning loop:\n\n**Active Learning Concept:**\n```\nStart with small labeled set (100-500 examples)\n  ↓\nTrain initial model\n  ↓\nUse model to score unlabeled examples by uncertainty\n  ↓\nSelect top K most uncertain examples\n  ↓\nHuman annotates selected examples\n  ↓\nAdd to training set\n  ↓\nRetrain model\n  ↓\nRepeat until performance plateaus or budget exhausted\n```\n\n**Step 1: Uncertainty Sampling Strategies**\n\n**Strategy A: Entropy (Most Common)**\n```python\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef entropy_sampling(model, unlabeled_data, k=100):\n    \"\"\"\n    Select samples where model is most uncertain (high entropy)\n    \"\"\"\n    uncertainties = []\n    \n    for example in unlabeled_data:\n        # Get prediction probabilities\n        probs = model.predict_proba(example)\n        \n        # Calculate entropy: H(p) = -Σ p_i * log(p_i)\n        ent = entropy(probs)\n        \n        uncertainties.append((example, ent))\n    \n    # Sort by entropy (descending) and take top K\n    sorted_by_uncertainty = sorted(uncertainties, key=lambda x: x[1], reverse=True)\n    selected = [ex for ex, _ in sorted_by_uncertainty[:k]]\n    \n    return selected\n```\n\n**Example:**\n```\nPredictions for 3 samples:\nSample 1: [0.99, 0.01] → Entropy: 0.06 (very confident, don't label)\nSample 2: [0.51, 0.49] → Entropy: 0.69 (uncertain, LABEL THIS)\nSample 3: [0.98, 0.02] → Entropy: 0.11 (confident, don't label)\n\n→ Select Sample 2 for labeling\n```\n\n**Strategy B: Margin Sampling**\n```python\ndef margin_sampling(model, unlabeled_data, k=100):\n    \"\"\"\n    Select samples with smallest margin between top 2 predictions\n    \"\"\"\n    margins = []\n    \n    for example in unlabeled_data:\n        probs = model.predict_proba(example)\n        \n        # Sort probabilities\n        sorted_probs = np.sort(probs)[::-1]\n        \n        # Margin = difference between top 2\n        margin = sorted_probs[0] - sorted_probs[1]\n        \n        margins.append((example, margin))\n    \n    # Sort by margin (ascending) - small margin = uncertain\n    sorted_by_margin = sorted(margins, key=lambda x: x[1])\n    selected = [ex for ex, _ in sorted_by_margin[:k]]\n    \n    return selected\n```\n\n**Strategy C: BALD (Bayesian Active Learning by Disagreement)**\n```python\ndef bald_sampling(model, unlabeled_data, k=100, n_forward_passes=10):\n    \"\"\"\n    Use dropout at test time to get model uncertainty\n    Best for deep learning models\n    \"\"\"\n    uncertainties = []\n    \n    for example in unlabeled_data:\n        # Multiple forward passes with dropout enabled\n        predictions = []\n        for _ in range(n_forward_passes):\n            pred = model.predict_with_dropout(example)\n            predictions.append(pred)\n        \n        # Calculate variance across predictions\n        pred_variance = np.var(predictions, axis=0).mean()\n        \n        uncertainties.append((example, pred_variance))\n    \n    # Select high variance samples\n    sorted_by_variance = sorted(uncertainties, key=lambda x: x[1], reverse=True)\n    selected = [ex for ex, _ in sorted_by_variance[:k]]\n    \n    return selected\n```\n\n**Step 2: Complete Active Learning Pipeline**\n\n```python\nclass ActiveLearningLoop:\n    def __init__(self, initial_labeled, unlabeled_pool, model, annotator):\n        self.labeled_data = initial_labeled\n        self.unlabeled_pool = unlabeled_pool\n        self.model = model\n        self.annotator = annotator  # Human or annotation API\n        \n        self.iteration_history = []\n    \n    def run(self, n_iterations=10, batch_size=100, strategy='entropy'):\n        \"\"\"\n        Run active learning loop\n        \"\"\"\n        for iteration in range(n_iterations):\n            print(f\"\\n=== Iteration {iteration + 1} ===\")\n            \n            # 1. Train model on current labeled data\n            print(f\"Training on {len(self.labeled_data)} examples...\")\n            self.model.train(self.labeled_data)\n            \n            # 2. Evaluate current model\n            val_acc = self.model.evaluate(validation_set)\n            print(f\"Validation accuracy: {val_acc:.2%}\")\n            \n            # 3. Check stopping criteria\n            if self._should_stop(val_acc, iteration):\n                print(\"Stopping criteria met!\")\n                break\n            \n            # 4. Select uncertain samples\n            print(f\"Selecting {batch_size} samples for labeling...\")\n            if strategy == 'entropy':\n                selected = self.entropy_sampling(batch_size)\n            elif strategy == 'margin':\n                selected = self.margin_sampling(batch_size)\n            elif strategy == 'bald':\n                selected = self.bald_sampling(batch_size)\n            \n            # 5. Get labels from human annotator\n            print(\"Sending to annotator...\")\n            newly_labeled = self.annotator.label(selected)\n            \n            # 6. Update datasets\n            self.labeled_data.extend(newly_labeled)\n            self.unlabeled_pool = [x for x in self.unlabeled_pool if x not in selected]\n            \n            # 7. Track progress\n            self.iteration_history.append({\n                'iteration': iteration,\n                'labeled_count': len(self.labeled_data),\n                'validation_accuracy': val_acc,\n                'samples_added': len(newly_labeled)\n            })\n            \n            print(f\"Total labeled: {len(self.labeled_data)}\")\n            print(f\"Remaining unlabeled: {len(self.unlabeled_pool)}\")\n        \n        return self.model, self.iteration_history\n    \n    def _should_stop(self, current_acc, iteration):\n        \"\"\"\n        Stopping criteria:\n        - Performance plateau (no improvement in 3 iterations)\n        - Budget exhausted\n        - Unlabeled pool empty\n        \"\"\"\n        # No more unlabeled data\n        if len(self.unlabeled_pool) < 10:\n            return True\n        \n        # Performance plateau check\n        if len(self.iteration_history) >= 3:\n            recent_accs = [h['validation_accuracy'] for h in self.iteration_history[-3:]]\n            improvement = current_acc - min(recent_accs)\n            \n            if improvement < 0.005:  # Less than 0.5% improvement\n                print(f\"Performance plateau detected (improvement: {improvement:.3%})\")\n                return True\n        \n        return False\n```\n\n**Step 3: Integration with Annotation Platform**\n\n**Option A: Manual Annotation (Local)**\n```python\nclass ManualAnnotator:\n    def label(self, samples):\n        \"\"\"Human labels samples via command line\"\"\"\n        labeled = []\n        \n        for i, sample in enumerate(samples):\n            print(f\"\\n[{i+1}/{len(samples)}]\")\n            print(f\"Text: {sample['text']}\")\n            label = input(\"Label (0/1/2): \")\n            \n            sample['label'] = int(label)\n            labeled.append(sample)\n        \n        return labeled\n```\n\n**Option B: API-Based Annotation (Label Studio, Prodigy)**\n```python\nimport requests\n\nclass LabelStudioAnnotator:\n    def __init__(self, api_url, api_key, project_id):\n        self.api_url = api_url\n        self.headers = {'Authorization': f'Token {api_key}'}\n        self.project_id = project_id\n    \n    def label(self, samples):\n        \"\"\"Send samples to Label Studio for annotation\"\"\"\n        # Create tasks\n        tasks = [\n            {\n                'data': {'text': sample['text']},\n                'project': self.project_id\n            }\n            for sample in samples\n        ]\n        \n        # Submit to Label Studio\n        response = requests.post(\n            f\"{self.api_url}/api/tasks/bulk\",\n            headers=self.headers,\n            json=tasks\n        )\n        \n        task_ids = [t['id'] for t in response.json()]\n        \n        # Wait for annotations (polling)\n        print(\"Waiting for human annotations...\")\n        labeled_samples = self._wait_for_annotations(task_ids)\n        \n        return labeled_samples\n    \n    def _wait_for_annotations(self, task_ids):\n        \"\"\"Poll Label Studio until all tasks are annotated\"\"\"\n        import time\n        \n        while True:\n            # Check annotation status\n            response = requests.get(\n                f\"{self.api_url}/api/tasks\",\n                headers=self.headers,\n                params={'ids': ','.join(map(str, task_ids))}\n            )\n            \n            tasks = response.json()\n            completed = [t for t in tasks if t['is_labeled']]\n            \n            if len(completed) == len(task_ids):\n                break\n            \n            print(f\"Annotated: {len(completed)}/{len(task_ids)}\")\n            time.sleep(30)  # Check every 30 seconds\n        \n        # Extract labels\n        labeled = []\n        for task in completed:\n            labeled.append({\n                'text': task['data']['text'],\n                'label': task['annotations'][0]['result'][0]['value']['choices'][0]\n            })\n        \n        return labeled\n```\n\n**Step 4: Cost-Benefit Analysis**\n\n```python\ndef estimate_labeling_budget(initial_labeled, target_accuracy, cost_per_label=0.10):\n    \"\"\"\n    Estimate how many labels needed to reach target accuracy\n    \"\"\"\n    # Rule of thumb: active learning needs 30-50% of random sampling\n    # For 90% accuracy, might need 10k random samples or 3-5k active learning samples\n    \n    simulated_results = {\n        'random_sampling': {\n            'labels_needed': 10000,\n            'cost': 10000 * cost_per_label,\n            'estimated_accuracy': target_accuracy\n        },\n        'active_learning': {\n            'labels_needed': 4000,\n            'cost': 4000 * cost_per_label,\n            'estimated_accuracy': target_accuracy,\n            'savings': (10000 - 4000) * cost_per_label\n        }\n    }\n    \n    print(f\"Random Sampling: {simulated_results['random_sampling']['labels_needed']} labels, ${simulated_results['random_sampling']['cost']}\")\n    print(f\"Active Learning: {simulated_results['active_learning']['labels_needed']} labels, ${simulated_results['active_learning']['cost']}\")\n    print(f\"Savings: ${simulated_results['active_learning']['savings']} (60% reduction)\")\n    \n    return simulated_results\n```\n\n**Step 5: Example Usage**\n\n```python\n# Initialize\ninitial_labeled = load_dataset('labeled_seed.jsonl')  # 500 examples\nunlabeled_pool = load_dataset('unlabeled_pool.jsonl')  # 50,000 examples\n\nmodel = LogisticRegression()  # Or any sklearn/transformers model\nannotator = LabelStudioAnnotator(\n    api_url='http://localhost:8080',\n    api_key='your-api-key',\n    project_id=42\n)\n\n# Run active learning\nal_loop = ActiveLearningLoop(\n    initial_labeled=initial_labeled,\n    unlabeled_pool=unlabeled_pool,\n    model=model,\n    annotator=annotator\n)\n\nfinal_model, history = al_loop.run(\n    n_iterations=10,\n    batch_size=100,\n    strategy='entropy'\n)\n\n# Plot learning curve\nimport matplotlib.pyplot as plt\n\niterations = [h['iteration'] for h in history]\naccuracies = [h['validation_accuracy'] for h in history]\nlabeled_counts = [h['labeled_count'] for h in history]\n\nplt.plot(labeled_counts, accuracies)\nplt.xlabel('Number of Labeled Examples')\nplt.ylabel('Validation Accuracy')\nplt.title('Active Learning Progress')\nplt.show()\n```\n\n**Expected Results:**\n- **Iteration 1:** 500 labels → 75% accuracy\n- **Iteration 3:** 800 labels → 83% accuracy\n- **Iteration 6:** 1,100 labels → 88% accuracy\n- **Iteration 10:** 1,500 labels → 91% accuracy (plateau)\n\nVs Random Sampling:\n- **Random:** 5,000+ labels needed for 91% accuracy\n- **Active Learning:** 1,500 labels for same accuracy\n- **Savings:** 70% fewer labels required\n\n**When to Use Active Learning:**\n✅ Labeling is expensive ($0.10-$1.00 per example)\n✅ Large unlabeled pool available\n✅ Initial labeled set is small (<1,000)\n✅ Budget constrained\n\n**When NOT to Use:**\n❌ Labeling is cheap/free\n❌ Already have 10k+ labeled examples\n❌ Need production model urgently (AL takes more iterations)\n❌ Unlabeled pool is small (<10k)"}]}
{"messages": [{"role": "user", "content": "I want to finetune a conversational assistant with RAG using my product docs. How should I prepare training pairs?"}, {"role": "assistant", "content": "<thinking>\n1. **Deconstruct User's Problem:** User wants to build a RAG (Retrieval-Augmented Generation) system where a conversational assistant retrieves relevant documentation chunks and generates answers. Need to create training data that teaches model to use retrieved context effectively.\n2. **Identify Key Requirements:** Document chunking strategy, question-answer pair generation, retrieval index creation, teaching model to cite sources, handling multi-hop reasoning across chunks.\n3. **Formulate Approach:** Chunk documents optimally (512 tokens with overlap), generate QA pairs from chunks, create retrieval index (FAISS/Weaviate), format training data with [retrieved context] → [answer with citations].\n4. **Structure Response:** Cover document preprocessing, chunking strategy, QA pair generation, retrieval setup, training data format for RAG.\n</thinking>\n\nBuilding a RAG-based assistant requires careful data preparation! Here's the complete workflow:\n\n**Step 1: Document Preprocessing & Chunking**\n\n**Why Chunking Matters:**\n- Full documents too long for context window\n- Need granular retrieval (specific sections, not whole docs)\n- Balance: chunks too small lose context, too large reduce precision\n\n**Optimal Chunking Strategy:**\n```python\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef chunk_documents(docs_path, chunk_size=512, overlap=64):\n    \"\"\"\n    Chunk documents with overlap to preserve context\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=overlap,\n        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Prefer semantic boundaries\n        length_function=len\n    )\n    \n    chunks = []\n    for doc_file in os.listdir(docs_path):\n        with open(os.path.join(docs_path, doc_file), 'r') as f:\n            doc_text = f.read()\n        \n        # Split into chunks\n        doc_chunks = text_splitter.split_text(doc_text)\n        \n        # Add metadata to each chunk\n        for i, chunk in enumerate(doc_chunks):\n            chunks.append({\n                'id': f\"{doc_file}_chunk_{i}\",\n                'text': chunk,\n                'source': doc_file,\n                'chunk_index': i,\n                'total_chunks': len(doc_chunks)\n            })\n    \n    return chunks\n\n# Example usage\nchunks = chunk_documents('product_docs/', chunk_size=512, overlap=64)\nprint(f\"Created {len(chunks)} chunks from documentation\")\n```\n\n**Chunk Size Guidelines:**\n| Doc Type | Recommended Chunk Size | Overlap |\n|----------|------------------------|----------|\n| API Docs | 256-512 tokens | 32-64 |\n| Tutorials | 512-1024 tokens | 64-128 |\n| FAQs | 128-256 tokens | 16-32 |\n| Policies | 512 tokens | 64 |\n\n**Step 2: Generate Question-Answer Pairs**\n\n**Method A: LLM-Generated QA Pairs**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ndef generate_qa_from_chunk(chunk_text, n_questions=3):\n    \"\"\"\n    Use LLM to generate realistic questions from chunk\n    \"\"\"\n    prompt = f\"\"\"Given this documentation chunk, generate {n_questions} realistic user questions that can be answered using ONLY this chunk.\n\nDocumentation:\n{chunk_text}\n\nGenerate questions in JSON format:\n{{\n  \"questions\": [\n    {{\"question\": \"...\", \"answer\": \"...\"}}\n  ]\n}}\n\"\"\"\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.7\n    )\n    \n    qa_pairs = json.loads(response.choices[0].message.content)\n    return qa_pairs['questions']\n\n# Generate QA pairs for all chunks\nrag_training_data = []\nfor chunk in chunks:\n    qa_pairs = generate_qa_from_chunk(chunk['text'], n_questions=2)\n    \n    for qa in qa_pairs:\n        rag_training_data.append({\n            'question': qa['question'],\n            'answer': qa['answer'],\n            'source_chunk': chunk['text'],\n            'source_doc': chunk['source'],\n            'chunk_id': chunk['id']\n        })\n\nprint(f\"Generated {len(rag_training_data)} QA pairs\")\n```\n\n**Method B: Human-Curated Questions**\n```python\n# Collect real user questions from:\n# - Support tickets\n# - Chat logs\n# - FAQ submissions\n\nreal_user_questions = load_support_questions('support_tickets.csv')\n\n# Match questions to relevant chunks\nfrom sentence_transformers import SentenceTransformer\nimport faiss\n\nembedder = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Embed chunks\nchunk_embeddings = embedder.encode([c['text'] for c in chunks])\nindex = faiss.IndexFlatL2(chunk_embeddings.shape[1])\nindex.add(chunk_embeddings)\n\n# For each question, find relevant chunk\nfor question in real_user_questions:\n    q_embedding = embedder.encode([question['text']])\n    distances, indices = index.search(q_embedding, k=1)\n    \n    matched_chunk = chunks[indices[0][0]]\n    \n    # Human writes answer using the chunk\n    answer = get_human_answer(question['text'], matched_chunk['text'])\n    \n    rag_training_data.append({\n        'question': question['text'],\n        'answer': answer,\n        'source_chunk': matched_chunk['text'],\n        'source_doc': matched_chunk['source']\n    })\n```\n\n**Step 3: Create Retrieval Index**\n\n**FAISS Index (Fast, Local)**\n```python\nimport faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize embedder\nembedder = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, good quality\n\n# Embed all chunks\nchunk_texts = [c['text'] for c in chunks]\nembeddings = embedder.encode(chunk_texts, show_progress_bar=True)\n\n# Create FAISS index\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)  # L2 distance\nindex.add(embeddings.astype('float32'))\n\n# Save index\nfaiss.write_index(index, 'product_docs.faiss')\n\n# Save chunk metadata\nwith open('chunks_metadata.json', 'w') as f:\n    json.dump(chunks, f)\n\nprint(f\"Created FAISS index with {index.ntotal} chunks\")\n```\n\n**Retrieval Function:**\n```python\ndef retrieve_relevant_chunks(question, index, chunks, embedder, top_k=3):\n    \"\"\"\n    Retrieve top-k most relevant chunks for question\n    \"\"\"\n    # Embed question\n    q_embedding = embedder.encode([question])\n    \n    # Search index\n    distances, indices = index.search(q_embedding.astype('float32'), k=top_k)\n    \n    # Get chunks\n    retrieved = [\n        {\n            'chunk': chunks[idx],\n            'score': float(distances[0][i])\n        }\n        for i, idx in enumerate(indices[0])\n    ]\n    \n    return retrieved\n```\n\n**Step 4: Format Training Data for RAG**\n\n**Training Format (Teach Model to Use Retrieved Context):**\n```python\ndef format_rag_training_pair(qa_pair, retrieved_chunks):\n    \"\"\"\n    Format: [CONTEXT] {chunks} [QUESTION] {question} [ANSWER] {answer with citation}\n    \"\"\"\n    # Combine retrieved chunks\n    context_parts = []\n    for i, chunk in enumerate(retrieved_chunks):\n        context_parts.append(f\"[Source {i+1}: {chunk['chunk']['source']}]\\n{chunk['chunk']['text']}\")\n    \n    context = \"\\n\\n\".join(context_parts)\n    \n    # Format training example\n    training_example = {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Answer questions using ONLY the provided context. Always cite your sources.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Context:\n{context}\n\nQuestion: {qa_pair['question']}\"\"\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": f\"{qa_pair['answer']}\\n\\nSource: {qa_pair['source_doc']}\"\n            }\n        ]\n    }\n    \n    return training_example\n\n# Create full training dataset\nrag_dataset = []\nfor qa in rag_training_data:\n    # Simulate retrieval (in training, we know the right chunk)\n    retrieved = [{\n        'chunk': {\n            'text': qa['source_chunk'],\n            'source': qa['source_doc']\n        }\n    }]\n    \n    example = format_rag_training_pair(qa, retrieved)\n    rag_dataset.append(example)\n\n# Save\nwith open('rag_training_dataset.jsonl', 'w') as f:\n    for example in rag_dataset:\n        f.write(json.dumps(example) + '\\n')\n\nprint(f\"Created {len(rag_dataset)} RAG training examples\")\n```\n\n**Step 5: Advanced: Multi-Hop Reasoning**\n\n**For questions requiring multiple chunks:**\n```python\ndef create_multihop_example(question, relevant_chunks):\n    \"\"\"\n    Teach model to synthesize info from multiple chunks\n    \"\"\"\n    context = \"\\n\\n\".join([\n        f\"[Source {i+1}]\\n{chunk['text']}\"\n        for i, chunk in enumerate(relevant_chunks)\n    ])\n    \n    # Answer should reference multiple sources\n    answer = generate_multihop_answer(question, relevant_chunks)\n    \n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"Answer using multiple sources when needed.\"},\n            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"},\n            {\"role\": \"assistant\", \"content\": f\"{answer}\\n\\nSources: [1, 2]\"}\n        ]\n    }\n```\n\n**Step 6: Quality Validation**\n\n```python\ndef validate_rag_dataset(dataset):\n    \"\"\"\n    Check training data quality\n    \"\"\"\n    issues = []\n    \n    for i, example in enumerate(dataset):\n        user_msg = example['messages'][1]['content']\n        assistant_msg = example['messages'][2]['content']\n        \n        # Check 1: Answer contains info from context?\n        context = user_msg.split('Question:')[0]\n        if not has_overlap(context, assistant_msg):\n            issues.append(f\"Example {i}: Answer not grounded in context\")\n        \n        # Check 2: Source cited?\n        if 'Source:' not in assistant_msg:\n            issues.append(f\"Example {i}: Missing source citation\")\n        \n        # Check 3: Answer is not too short/long?\n        if len(assistant_msg.split()) < 10:\n            issues.append(f\"Example {i}: Answer too short\")\n        if len(assistant_msg.split()) > 200:\n            issues.append(f\"Example {i}: Answer too long\")\n    \n    return issues\n\nvalidation_issues = validate_rag_dataset(rag_dataset)\nif validation_issues:\n    print(f\"Found {len(validation_issues)} issues\")\n    for issue in validation_issues[:10]:\n        print(f\"  - {issue}\")\n```\n\n**Complete Pipeline:**\n\n```python\ndef build_rag_training_dataset(docs_path, output_file):\n    # 1. Chunk documents\n    chunks = chunk_documents(docs_path, chunk_size=512, overlap=64)\n    \n    # 2. Create retrieval index\n    embedder, index = create_faiss_index(chunks)\n    \n    # 3. Generate QA pairs\n    qa_pairs = []\n    for chunk in chunks:\n        pairs = generate_qa_from_chunk(chunk['text'], n_questions=2)\n        qa_pairs.extend(pairs)\n    \n    # 4. Format for training\n    training_data = []\n    for qa in qa_pairs:\n        retrieved = retrieve_relevant_chunks(qa['question'], index, chunks, embedder)\n        example = format_rag_training_pair(qa, retrieved)\n        training_data.append(example)\n    \n    # 5. Validate\n    issues = validate_rag_dataset(training_data)\n    print(f\"Validation issues: {len(issues)}\")\n    \n    # 6. Save\n    with open(output_file, 'w') as f:\n        for ex in training_data:\n            f.write(json.dumps(ex) + '\\n')\n    \n    print(f\"Saved {len(training_data)} RAG training examples to {output_file}\")\n    \n    return training_data\n\n# Run pipeline\nrag_data = build_rag_training_dataset(\n    docs_path='product_docs/',\n    output_file='rag_training.jsonl'\n)\n```\n\n**Expected Dataset Size:**\n- 100 docs → ~500 chunks → ~1,000 QA pairs\n- 500 docs → ~2,500 chunks → ~5,000 QA pairs\n\n**Training Recommendations:**\n- Use QLoRA for efficient fine-tuning\n- Train for 2-3 epochs\n- Learning rate: 2e-5\n- Evaluate on held-out questions\n- Measure: answer accuracy, source citation rate"}]}
DATASET_EOF

echo "✅ Completed! Added examples 15-16"
