# LLM Judge Score Normalization Fix
**Date:** December 3, 2025
**Issue:** Score scale mismatch between LLM judges (1-10) and rule validators (0-1)
**Status:** âœ… FIXED

---

## ðŸŽ¯ Problem Summary

**Before Fix:**
- Rule validators save scores: **0.0 to 1.0** (e.g., 0.85 = 85%)
- Human judgments save scores: **0.0 to 1.0**
- LLM judge saves scores: **1 to 10** (e.g., 8 = 8/10)

**Impact:**
```sql
SELECT judge_type, score FROM judgments WHERE message_id = 'msg-123';

-- Mixed scales in same column:
| judge_type | score |
|------------|-------|
| rule       | 1.0   |  â† 0-1 scale
| rule       | 0.85  |  â† 0-1 scale
| llm        | 8.0   |  â† 1-10 scale âš ï¸
| llm        | 7.5   |  â† 1-10 scale âš ï¸

-- Analytics broken:
AVG(score) = (1.0 + 0.85 + 8.0 + 7.5) / 4 = 4.34 âŒ WRONG!
```

**After Fix:**
```sql
-- All scores normalized to 0-1:
| judge_type | score |
|------------|-------|
| rule       | 1.0   |  â† 0-1 scale
| rule       | 0.85  |  â† 0-1 scale
| llm        | 0.8   |  â† 0-1 scale âœ…
| llm        | 0.75  |  â† 0-1 scale âœ…

-- Analytics correct:
AVG(score) = (1.0 + 0.85 + 0.8 + 0.75) / 4 = 0.85 âœ… CORRECT!
```

---

## âœ… Changes Made

### File 1: `app/api/evaluation/judge/route.ts`

#### Change 1: Normalize score when saving to database (Line 258)

**Before:**
```typescript
async function saveJudgmentsToDatabase(supabase: SupabaseClient, judgments: LLMJudgmentResult[]) {
  const records = judgments.map((judgment) => ({
    message_id: judgment.message_id,
    judge_type: 'llm',
    judge_name: judgment.judge_model,
    criterion: judgment.criterion,
    score: judgment.score,  // âŒ Saves 1-10 directly
    passed: judgment.passed,
    evidence_json: {
      reasoning: judgment.reasoning,
      confidence: judgment.confidence,
      positive_aspects: judgment.evidence.positive_aspects,
      negative_aspects: judgment.evidence.negative_aspects,
      improvement_suggestions: judgment.evidence.improvement_suggestions,
    },
    notes: `AI evaluation with ${(judgment.confidence * 100).toFixed(0)}% confidence`,
  }));
```

**After:**
```typescript
async function saveJudgmentsToDatabase(supabase: SupabaseClient, judgments: LLMJudgmentResult[]) {
  const records = judgments.map((judgment) => ({
    message_id: judgment.message_id,
    judge_type: 'llm',
    judge_name: judgment.judge_model,
    criterion: judgment.criterion,
    score: judgment.score / 10, // âœ… Normalize 1-10 scale to 0-1 for consistency with rule validators
    passed: judgment.passed,
    evidence_json: {
      reasoning: judgment.reasoning,
      confidence: judgment.confidence,
      positive_aspects: judgment.evidence.positive_aspects,
      negative_aspects: judgment.evidence.negative_aspects,
      improvement_suggestions: judgment.evidence.improvement_suggestions,
      original_score: judgment.score, // âœ… Store original 1-10 score for reference
    },
    notes: `AI evaluation: ${judgment.score}/10 (${(judgment.confidence * 100).toFixed(0)}% confidence)`, // âœ… Show original score in notes
  }));
```

**Changes:**
- âœ… Line 258: Divide score by 10 to normalize
- âœ… Line 266: Store original 1-10 score in evidence_json
- âœ… Line 268: Update notes to show original score

---

#### Change 2: Normalize average_score in single evaluation response (Line 148)

**Before:**
```typescript
return NextResponse.json({
  success: true,
  message_id: request.message_id,
  evaluations: results,
  summary: {
    total_criteria: results.length,
    passed: results.filter((r) => r.passed).length,
    failed: results.filter((r) => !r.passed).length,
    average_score: results.reduce((sum, r) => sum + r.score, 0) / results.length,  // âŒ Returns 1-10 scale
    average_confidence: results.reduce((sum, r) => sum + r.confidence, 0) / results.length,
  },
});
```

**After:**
```typescript
return NextResponse.json({
  success: true,
  message_id: request.message_id,
  evaluations: results,
  summary: {
    total_criteria: results.length,
    passed: results.filter((r) => r.passed).length,
    failed: results.filter((r) => !r.passed).length,
    average_score: results.reduce((sum, r) => sum + r.score, 0) / results.length / 10, // âœ… Normalize to 0-1
    average_confidence: results.reduce((sum, r) => sum + r.confidence, 0) / results.length,
  },
});
```

**Changes:**
- âœ… Line 148: Divide average_score by 10 to normalize

---

#### Change 3: Normalize average_score in batch evaluation response (Line 214)

**Before:**
```typescript
// Calculate summary statistics
const allJudgments = Array.from(results.values()).flat();
const summary = {
  total_messages: messages.length,
  total_evaluations: allJudgments.length,
  passed: allJudgments.filter((r) => r.passed).length,
  failed: allJudgments.filter((r) => !r.passed).length,
  average_score: allJudgments.reduce((sum, r) => sum + r.score, 0) / allJudgments.length,  // âŒ Returns 1-10 scale
  average_confidence:
    allJudgments.reduce((sum, r) => sum + r.confidence, 0) / allJudgments.length,
};
```

**After:**
```typescript
// Calculate summary statistics
const allJudgments = Array.from(results.values()).flat();
const summary = {
  total_messages: messages.length,
  total_evaluations: allJudgments.length,
  passed: allJudgments.filter((r) => r.passed).length,
  failed: allJudgments.filter((r) => !r.passed).length,
  average_score: allJudgments.reduce((sum, r) => sum + r.score, 0) / allJudgments.length / 10, // âœ… Normalize to 0-1
  average_confidence:
    allJudgments.reduce((sum, r) => sum + r.confidence, 0) / allJudgments.length,
};
```

**Changes:**
- âœ… Line 214: Divide average_score by 10 to normalize

---

### File 2: `app/api/batch-testing/run/route.ts`

#### Change: Improve log message to show both scales (Line 668)

**Before:**
```typescript
if (judgeResponse.ok) {
  const judgeData = await judgeResponse.json();
  const avgScore = judgeData.evaluations?.reduce((sum: number, e: any) => sum + (e.score || 0), 0) / (judgeData.evaluations?.length || 1);
  console.log(`[Process Prompt] ${promptIndex + 1}: Judge evaluation complete, avg score: ${avgScore.toFixed(1)}/10`);
} else {
```

**After:**
```typescript
if (judgeResponse.ok) {
  const judgeData = await judgeResponse.json();
  const avgScore = judgeData.evaluations?.reduce((sum: number, e: any) => sum + (e.score || 0), 0) / (judgeData.evaluations?.length || 1);
  console.log(`[Process Prompt] ${promptIndex + 1}: Judge evaluation complete, avg score: ${avgScore.toFixed(1)}/10 (${(avgScore / 10 * 100).toFixed(0)}%)`);  // âœ… Show both scales
} else {
```

**Changes:**
- âœ… Line 668: Add percentage display to log message

---

## ðŸ“Š Impact Analysis

### What This Fix Changes

#### 1. Database Storage âœ…
**Before:** LLM scores stored as 1-10
**After:** LLM scores stored as 0.1-1.0 (normalized)
**Original score preserved:** In evidence_json.original_score

#### 2. API Responses âœ…
**Before:** summary.average_score returns 1-10 scale
**After:** summary.average_score returns 0-1 scale (consistent)

#### 3. Analytics Queries âœ…
**Before:** Mixed scale averages produce incorrect results
**After:** All scores on 0-1 scale, analytics work correctly

**Example:**
```typescript
// lib/tools/evaluation-metrics/operations/benchmarkAnalysis.ts:113
const avgScore = scoresAvailable.length > 0
  ? scoresAvailable.reduce((sum, j) => sum + (j.score || 0), 0) / scoresAvailable.length
  : 0;

// BEFORE: (0.85 + 1.0 + 8.0 + 7.5) / 4 = 4.34 âŒ Wrong!
// AFTER:  (0.85 + 1.0 + 0.8 + 0.75) / 4 = 0.85 âœ… Correct!
```

#### 4. UI Display âœ…
**Before:** MessageJudgments multiplies score by 100, showing "800%" for LLM scores
**After:** All scores 0-1, displays correctly as percentages

```typescript
// components/chat/MessageJudgments.tsx:174
Score: {(judgment.score * 100).toFixed(1)}%

// BEFORE: LLM score 8.0 â†’ "800%" âŒ
// AFTER:  LLM score 0.8 â†’ "80%" âœ…
```

---

## ðŸ—„ï¸ Database Migration

### Required: Migrate Existing Data

**File:** `MIGRATE_LLM_SCORES.sql`

**Steps:**
1. **Backup existing LLM judgments**
2. **Normalize scores:** `UPDATE judgments SET score = score / 10 WHERE judge_type = 'llm' AND score > 1.0`
3. **Update notes:** Preserve original score in notes
4. **Verify:** Check all scores are 0-1

**Run Migration:**
```bash
psql $DATABASE_URL -f MIGRATE_LLM_SCORES.sql
```

**Migration includes:**
- âœ… Pre-migration verification queries
- âœ… Automatic backup table creation
- âœ… Score normalization UPDATE query
- âœ… Post-migration verification
- âœ… Rollback instructions
- âœ… Analytics verification queries

---

## âœ… Verification Checklist

### Code Changes Verified

- [x] `app/api/evaluation/judge/route.ts:258` - Score normalized before DB save
- [x] `app/api/evaluation/judge/route.ts:266` - Original score preserved in evidence_json
- [x] `app/api/evaluation/judge/route.ts:268` - Notes updated to show original score
- [x] `app/api/evaluation/judge/route.ts:148` - Single eval average_score normalized
- [x] `app/api/evaluation/judge/route.ts:214` - Batch eval average_score normalized
- [x] `app/api/batch-testing/run/route.ts:668` - Log message improved
- [x] TypeScript compilation passes (no errors in modified files)

### Testing Required

**1. New LLM Judgments Test:**
```bash
# Trigger LLM judge evaluation
curl -X POST http://localhost:3000/api/evaluation/judge \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "message_id": "test-msg-123",
    "message_content": "Test response",
    "criteria": ["helpfulness"],
    "judge_model": "gpt-4-turbo",
    "save_to_db": true
  }'

# Check database
psql $DATABASE_URL -c "
  SELECT score, passed, notes, evidence_json->'original_score' as original_score
  FROM judgments
  WHERE message_id = 'test-msg-123'
  AND judge_type = 'llm';
"

# Expected:
# score = 0.7-0.9 (normalized to 0-1)
# original_score = 7-9 (in evidence_json)
# notes = "AI evaluation: 8/10 (85% confidence)"
```

**2. UI Display Test:**
```bash
# Open chat, find message with LLM judgment
# Check MessageJudgments component
# Verify: "Score: 80.0%" not "Score: 800%"
```

**3. Analytics Test:**
```sql
-- Test average calculation across judge types
SELECT
  COUNT(*) as total,
  ROUND(AVG(score)::numeric, 3) as avg_score,
  MIN(score) as min_score,
  MAX(score) as max_score
FROM judgments
WHERE created_at > NOW() - INTERVAL '1 day';

-- Expected:
-- All scores between 0.0 and 1.0
-- Average makes sense (e.g., 0.75 not 4.5)
```

**4. Batch Testing Test:**
```bash
# Run batch test with LLM judge enabled
# Check logs for: "avg score: 8.0/10 (80%)"
# Check database for normalized scores
```

---

## ðŸ”„ Migration Plan

### Step 1: Deploy Code Changes
```bash
git add app/api/evaluation/judge/route.ts
git add app/api/batch-testing/run/route.ts
git commit -m "fix: normalize LLM judge scores to 0-1 scale"
git push
```

### Step 2: Run Database Migration
```bash
# Backup database first
pg_dump $DATABASE_URL > backup_before_score_migration.sql

# Run migration
psql $DATABASE_URL -f MIGRATE_LLM_SCORES.sql

# Verify
psql $DATABASE_URL -c "
  SELECT judge_type, COUNT(*), MAX(score)
  FROM judgments
  GROUP BY judge_type;
"
# All scores should be <= 1.0
```

### Step 3: Monitor Production
```bash
# Watch for errors in logs
tail -f logs/production.log | grep -i "evaluation\|judge"

# Check first few LLM judgments
psql $DATABASE_URL -c "
  SELECT * FROM judgments
  WHERE judge_type = 'llm'
  ORDER BY created_at DESC
  LIMIT 5;
"
```

---

## ðŸ“ Breaking Changes

### âš ï¸ API Response Changes

**Before:**
```json
{
  "summary": {
    "average_score": 7.5
  }
}
```

**After:**
```json
{
  "summary": {
    "average_score": 0.75
  }
}
```

**Impact:**
- Any external code consuming `/api/evaluation/judge` must expect 0-1 scale
- Log consumers expecting "7.5/10" now see "7.5/10 (75%)"

### âš ï¸ Database Schema Changes

**evidence_json now contains:**
```json
{
  "reasoning": "...",
  "confidence": 0.85,
  "positive_aspects": [...],
  "negative_aspects": [...],
  "improvement_suggestions": [...],
  "original_score": 8
}
```

**notes format changed:**
```
Before: "AI evaluation with 85% confidence"
After:  "AI evaluation: 8/10 (85% confidence)"
```

---

## ðŸŽ¯ Benefits

### 1. Consistent Data Model âœ…
- All judgment scores use 0-1 scale
- Database queries work correctly
- Analytics produce accurate results

### 2. Correct UI Display âœ…
- Scores display as percentages correctly
- No more "800%" display bugs

### 3. Original Score Preserved âœ…
- Original 1-10 score stored in evidence_json
- Notes field shows "8/10" for clarity
- No information lost

### 4. Future-Proof âœ…
- New LLM judgments automatically normalized
- Old judgments migrated once
- Consistent going forward

---

## ðŸ› Potential Issues

### Issue 1: Existing Code Expecting 1-10 Scale

**Risk:** External code or scripts that read LLM scores directly from database

**Mitigation:**
- Original score preserved in evidence_json.original_score
- Notes field contains "X/10" for easy parsing
- Migration script includes verification queries

**Example Fix:**
```typescript
// OLD CODE:
const score = judgment.score; // Expects 1-10

// NEW CODE:
const score = judgment.judge_type === 'llm'
  ? judgment.evidence_json.original_score
  : judgment.score * 10;
```

### Issue 2: Analytics Dashboard Caching

**Risk:** Cached analytics might show old mixed-scale averages

**Mitigation:**
- Clear analytics cache after migration
- Rebuild dashboards with new data

### Issue 3: Historical Comparisons

**Risk:** Comparing pre-migration and post-migration averages

**Mitigation:**
- Document migration date
- Add note to analytics: "Scores normalized on Dec 3, 2025"
- Use evidence_json.original_score for historical analysis if needed

---

## âœ… Summary

**Problem:** LLM scores (1-10) mixed with rule validator scores (0-1) broke analytics

**Solution:**
1. âœ… Normalize LLM scores to 0-1 when saving to database
2. âœ… Preserve original 1-10 score in evidence_json
3. âœ… Update API responses to return normalized averages
4. âœ… Migrate existing database records

**Files Changed:**
- `app/api/evaluation/judge/route.ts` (3 changes)
- `app/api/batch-testing/run/route.ts` (1 change)

**Files Created:**
- `MIGRATE_LLM_SCORES.sql` (migration script)
- `LLM_SCORE_NORMALIZATION_FIX.md` (this document)

**Verification:**
- âœ… TypeScript compiles without errors
- âœ… No breaking changes to core logic
- âœ… Original scores preserved
- âœ… Migration script includes rollback

**Ready for Production:** âœ… YES

---

**Fix Complete**
**Date:** December 3, 2025
