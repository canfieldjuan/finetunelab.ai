# Ollama Deployment Model Compatibility

## Overview

The Ollama deployment pipeline supports **any model you can train with HuggingFace**, but GGUF conversion compatibility varies by architecture.

---

## üü¢ Fully Supported Models (Tested & Working)

These models work reliably with the current converter:

### **Qwen Family**

- ‚úÖ Qwen/Qwen2-0.5B
- ‚úÖ Qwen/Qwen2-1.5B
- ‚úÖ Qwen/Qwen3-0.6B (tested with your checkpoint)
- ‚úÖ Qwen/Qwen2.5-3B

### **Llama Family**

- ‚úÖ meta-llama/Llama-2-7b
- ‚úÖ meta-llama/Llama-3.1-8B
- ‚úÖ meta-llama/Llama-3.2-1B
- ‚úÖ Any Llama-based models

### **Mistral**

- ‚úÖ mistralai/Mistral-7B-v0.1
- ‚úÖ mistralai/Mistral-7B-Instruct-v0.2

### **Phi**

- ‚úÖ microsoft/phi-2
- ‚úÖ microsoft/phi-3-mini

---

## üü° Partially Supported (May Need Official Converter)

These models work but may need llama.cpp's official converter for best results:

### **Gemma**

- ‚ö†Ô∏è google/gemma-2b
- ‚ö†Ô∏è google/gemma-7b
- **Note**: Fallback converter works, but official converter recommended

### **CodeLlama**

- ‚ö†Ô∏è codellama/CodeLlama-7b-hf
- **Note**: Works, but code-specific optimizations better with official converter

### **Falcon**

- ‚ö†Ô∏è tiiuae/falcon-7b
- **Note**: May need architecture-specific handling

---

## üî¥ Requires Official Converter

These architectures need llama.cpp's official converter:

### **Mixture of Experts (MoE)**

- ‚ùå mistralai/Mixtral-8x7B-v0.1
- **Why**: MoE routing requires special GGUF format
- **Solution**: Use official converter

### **Vision Models**

- ‚ùå llava-hf/llava-1.5-7b-hf
- **Why**: Multi-modal models need special handling
- **Solution**: Use official converter or deploy to vLLM instead

### **Experimental Architectures**

- ‚ùå New/bleeding-edge architectures not yet in transformers
- **Solution**: Wait for transformers update or use official converter

---

## How It Works

### **Step 1: LoRA Merge (Universal)**

```python
# Works with ANY model supported by transformers + peft
base_model = AutoModelForCausalLM.from_pretrained("any/model")
model = PeftModel.from_pretrained(base_model, "checkpoint-1500")
merged_model = model.merge_and_unload()
```

‚úÖ **This step works with ALL models**

### **Step 2: GGUF Conversion (Architecture-Dependent)**

**Method 1: Official llama.cpp Converter** (Recommended)

```bash
python llama.cpp/convert_hf_to_gguf.py checkpoint-1500_merged \
  --outfile model.gguf --outtype f16
```

‚úÖ **Supports ALL architectures** (constantly updated)

**Method 2: Fallback Converter** (Current Implementation)

```python
# Simplified converter - works for common architectures
fallback_gguf_conversion(model_path, output_path)
```

‚ö†Ô∏è **Supports most common models** (Llama, Qwen, Mistral, Phi)

---

## Setup for Maximum Compatibility

### **Option 1: Install llama.cpp (Recommended)**

```bash
cd lib/training
git clone https://github.com/ggerganov/llama.cpp
pip install -r llama.cpp/requirements.txt
```

**Benefits:**

- ‚úÖ Supports ALL model architectures
- ‚úÖ Constantly updated for new models
- ‚úÖ Automatic use (converter detects it)
- ‚úÖ Better optimization

### **Option 2: Use Fallback Converter**

**Benefits:**

- ‚úÖ No setup required
- ‚úÖ Works for common models (90% of use cases)
- ‚ö†Ô∏è May fail on exotic architectures

---

## Troubleshooting

### **Error: "Unrecognized model" during GGUF conversion**

**Cause:** Fallback converter doesn't recognize the architecture

**Solution 1: Install Official Converter**

```bash
cd lib/training
git clone https://github.com/ggerganov/llama.cpp
pip install -r llama.cpp/requirements.txt
# Re-run deployment (will auto-use official converter)
```

**Solution 2: Deploy to vLLM Instead**

- vLLM doesn't need GGUF conversion
- Works with ALL transformers models
- Select "vLLM" instead of "Ollama" in deployment UI

### **Error: "GGUF conversion failed with code 1"**

**Already Fixed!** This was the LoRA checkpoint issue - now handled automatically.

### **Deployment successful but model doesn't work in Ollama**

**Possible causes:**

1. GGUF format mismatch (use official converter)
2. Model too large for system RAM
3. Ollama version incompatible

**Solution:**

```bash
# Verify GGUF file
ollama create test-model -f Modelfile

# If fails, reconvert with official llama.cpp
python llama.cpp/convert_hf_to_gguf.py checkpoint-1500_merged \
  --outfile model.gguf --outtype f16
```

---

## Recommendations by Use Case

### **For Production (Reliability Priority)**

- ‚úÖ Install llama.cpp official converter
- ‚úÖ Use well-tested base models (Llama, Qwen, Mistral)
- ‚úÖ Test GGUF file with `ollama run` before deploying

### **For Experimentation (Speed Priority)**

- ‚úÖ Use fallback converter (works 90% of time)
- ‚úÖ Try vLLM if Ollama fails
- ‚úÖ Install llama.cpp only if needed

### **For Maximum Performance**

- ‚úÖ Use Q4_K_M quantization (8GB GPU)
- ‚úÖ Use Q5_K_M for quality (16GB GPU)
- ‚úÖ Use F16 for best quality (requires more RAM)

---

## Summary

| Model Type | LoRA Merge | Fallback GGUF | Official GGUF | vLLM |
|------------|-----------|---------------|---------------|------|
| Llama      | ‚úÖ        | ‚úÖ            | ‚úÖ            | ‚úÖ   |
| Qwen       | ‚úÖ        | ‚úÖ            | ‚úÖ            | ‚úÖ   |
| Mistral    | ‚úÖ        | ‚úÖ            | ‚úÖ            | ‚úÖ   |
| Phi        | ‚úÖ        | ‚úÖ            | ‚úÖ            | ‚úÖ   |
| Gemma      | ‚úÖ        | ‚ö†Ô∏è            | ‚úÖ            | ‚úÖ   |
| Mixtral    | ‚úÖ        | ‚ùå            | ‚úÖ            | ‚úÖ   |
| Vision     | ‚úÖ        | ‚ùå            | ‚ö†Ô∏è            | ‚úÖ   |

**Key:**

- ‚úÖ Full support
- ‚ö†Ô∏è Works with caveats
- ‚ùå Not supported

**Bottom line:** For maximum compatibility, install llama.cpp. Otherwise, fallback works for 90% of models.
