"""
FineTune Lab - Local Training API Server
Provides REST API for managing training jobs.
Works with standalone_trainer.py for actual training execution.
"""

import logging
import sys
import uuid
import subprocess
import asyncio
from asyncio import Queue
import json
import requests
import os
import tempfile
import threading
import time
import zipfile
from io import BytesIO
from pathlib import Path
from typing import Dict, Optional, List, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from fastapi import HTTPException

# Load environment variables from web-ui root
from dotenv import load_dotenv

# Try .env.local first (development), then .env (production)
env_local_path = Path(__file__).parent.parent.parent / '.env.local'
env_path = Path(__file__).parent.parent.parent / '.env'

if env_local_path.exists():
    load_dotenv(env_local_path)
    print(f"[ENV] Loaded environment from: {env_local_path}")
elif env_path.exists():
    load_dotenv(env_path)
    print(f"[ENV] Loaded environment from: {env_path}")
else:
    print(f"[ENV] Warning: No .env or .env.local file found")

try:
    from config_validator import validate_config, validate_dataset_format, ValidationError
except ImportError:
    from .config_validator import validate_config, validate_dataset_format, ValidationError

# Configure logging (after dotenv load)
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Supabase client for database operations
try:
    from supabase import create_client, Client
    SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
    # Use SERVICE_ROLE key for training server (bypasses RLS for system operations)
    SUPABASE_SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
    # Fallback to ANON key if service role not available (less secure)
    SUPABASE_ANON_KEY = os.getenv("NEXT_PUBLIC_SUPABASE_ANON_KEY")
    
    supabase: Optional[Client] = None
    
    if SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY:
        # Use service role key (preferred - bypasses RLS for training server)
        supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)
        logger.info("[Supabase] Client initialized with SERVICE_ROLE key")
        logger.info(f"[Supabase] URL: {SUPABASE_URL}")
    elif SUPABASE_URL and SUPABASE_ANON_KEY:
        # Fallback to anon key (requires user authentication in RLS policies)
        supabase = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)
        logger.warning("[Supabase] Using ANON key - service role key preferred for training server")
        logger.info(f"[Supabase] URL: {SUPABASE_URL}")
    else:
        logger.warning("[Supabase] Missing credentials - database operations will fail")
        logger.warning(f"[Supabase] NEXT_PUBLIC_SUPABASE_URL: {'SET' if SUPABASE_URL else 'MISSING'}")
        logger.warning(f"[Supabase] SUPABASE_SERVICE_ROLE_KEY: {'SET' if SUPABASE_SERVICE_ROLE_KEY else 'MISSING'}")
        logger.warning(f"[Supabase] NEXT_PUBLIC_SUPABASE_ANON_KEY: {'SET' if SUPABASE_ANON_KEY else 'MISSING'}")
except ImportError:
    logger.warning("[Supabase] supabase-py not installed - database operations will fail")
    supabase = None

# Next.js API endpoints for metrics persistence
NEXTJS_BASE_URL = "http://localhost:3000"
JOBS_ENDPOINT = f"{NEXTJS_BASE_URL}/api/training/local/jobs"
METRICS_ENDPOINT = f"{NEXTJS_BASE_URL}/api/training/local/metrics"
logger.info(f"[Persistence] Configured endpoints: jobs={JOBS_ENDPOINT}, metrics={METRICS_ENDPOINT}")


class JobStatusEnum(str, Enum):
    """Job status enumeration"""
    QUEUED = "queued"      # Job waiting in queue
    PENDING = "pending"    # Job preparing to start
    RUNNING = "running"    # Job actively training
    PAUSED = "paused"      # Job paused by user (Phase 2 - Pause/Resume)
    COMPLETED = "completed"  # Job finished successfully
    FAILED = "failed"      # Job encountered error
    CANCELLED = "cancelled"  # Job cancelled by user


@dataclass
class JobStatus:
    """Training job status tracker with rich metrics"""
    job_id: str
    user_id: str = ""  # User ID for database persistence
    status: JobStatusEnum = JobStatusEnum.PENDING
    progress: float = 0.0
    current_epoch: int = 0
    total_epochs: int = 0
    current_step: int = 0
    total_steps: int = 0
    loss: Optional[float] = None
    eval_loss: Optional[float] = None
    learning_rate: Optional[float] = None
    grad_norm: Optional[float] = None
    gpu_memory_allocated_gb: Optional[float] = None
    gpu_memory_reserved_gb: Optional[float] = None
    elapsed_seconds: Optional[int] = None
    remaining_seconds: Optional[int] = None
    samples_per_second: Optional[float] = None
    error_message: Optional[str] = None
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    updated_at: Optional[str] = None
    process: Optional[any] = None
    config_id: str = ""
    dataset_id: str = ""
    name: str = ""
    
    # Critical fields for database persistence (NOT NULL columns)
    model_name: str = "unknown"
    dataset_path: str = ""

    # New metrics (Phase 1 enhancement)
    perplexity: Optional[float] = None
    train_perplexity: Optional[float] = None
    best_eval_loss: Optional[float] = None
    best_epoch: int = 0
    best_step: int = 0
    epochs_without_improvement: int = 0
    loss_trend: Optional[str] = None
    total_samples: int = 0
    train_samples: int = 0
    val_samples: int = 0
    total_tokens_processed: int = 0
    
    # Queue support (Phase 2 - Job Queue)
    training_config: Optional[dict] = None  # Store config for deferred start
    queue_position: Optional[int] = None    # Position in queue
    
    # Phase 2: Pause/Resume support
    paused_at: Optional[str] = None         # Timestamp when job was paused

    # Advanced training features (Phase 4 - Database Schema Match)
    resume_from_checkpoint: Optional[str] = None  # Path to checkpoint to resume from
    num_gpus: int = 1                              # Number of GPUs for training
    distributed_strategy: str = "none"             # Distributed strategy: none, ddp, fsdp, deepspeed
    parameter_updates: list = field(default_factory=list)  # Runtime parameter updates
    last_parameter_update_at: Optional[str] = None  # Last parameter update timestamp

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON response"""
        return {
            "job_id": self.job_id,
            "user_id": self.user_id,  # Required for database persistence
            "status": self.status.value,
            "progress": self.progress,
            "current_epoch": self.current_epoch,
            "total_epochs": self.total_epochs,
            "current_step": self.current_step,
            "total_steps": self.total_steps,
            "loss": self.loss,
            "eval_loss": self.eval_loss,
            "learning_rate": self.learning_rate,
            "grad_norm": self.grad_norm,
            "gpu_memory_allocated_gb": self.gpu_memory_allocated_gb,
            "gpu_memory_reserved_gb": self.gpu_memory_reserved_gb,
            "elapsed_seconds": self.elapsed_seconds,
            "remaining_seconds": self.remaining_seconds,
            "samples_per_second": self.samples_per_second,
            "error_message": self.error_message,
            "started_at": self.started_at,
            "completed_at": self.completed_at,
            "updated_at": self.updated_at,
            # New metrics (Phase 1 enhancement)
            "perplexity": self.perplexity,
            "train_perplexity": self.train_perplexity,
            "best_eval_loss": self.best_eval_loss,
            "best_epoch": self.best_epoch,
            "best_step": self.best_step,
            "epochs_without_improvement": self.epochs_without_improvement,
            "loss_trend": self.loss_trend,
            "total_samples": self.total_samples,
            "train_samples": self.train_samples,
            "val_samples": self.val_samples,
            "total_tokens_processed": self.total_tokens_processed,
            # Advanced training features (Phase 4 - Database Schema Match)
            "resume_from_checkpoint": self.resume_from_checkpoint,
            "num_gpus": self.num_gpus,
            "distributed_strategy": self.distributed_strategy,
            "parameter_updates": self.parameter_updates,
            "last_parameter_update_at": self.last_parameter_update_at,
        }


# In-memory job store
jobs: Dict[str, JobStatus] = {}
logger.info("Job store initialized")

# Job queue for sequential processing (Phase 2 - Job Queue)
job_queue: Queue[str] = Queue()
logger.info("Job queue initialized")


# WebSocket connection manager for real-time streaming (Phase 3)
class ConnectionManager:
    """
    Manages WebSocket connections for real-time training metrics streaming.
    Supports multiple clients per job.
    
    Phase 3 - WebSocket Streaming
    """
    def __init__(self):
        # Store active connections: job_id -> List[WebSocket]
        self.active_connections: Dict[str, List[WebSocket]] = {}
        self._lock = asyncio.Lock()
    
    async def connect(self, websocket: WebSocket, job_id: str):
        """Accept new WebSocket connection for a job"""
        await websocket.accept()
        
        async with self._lock:
            if job_id not in self.active_connections:
                self.active_connections[job_id] = []
            self.active_connections[job_id].append(websocket)
        
        logger.info(f"[WebSocket] Client connected to job {job_id}. Total connections: {len(self.active_connections[job_id])}")
    
    async def disconnect(self, websocket: WebSocket, job_id: str):
        """Remove WebSocket connection"""
        async with self._lock:
            if job_id in self.active_connections:
                if websocket in self.active_connections[job_id]:
                    self.active_connections[job_id].remove(websocket)
                
                # Clean up empty lists
                if not self.active_connections[job_id]:
                    del self.active_connections[job_id]
                    logger.info(f"[WebSocket] No more connections for job {job_id}")
                else:
                    logger.info(f"[WebSocket] Client disconnected from job {job_id}. Remaining: {len(self.active_connections[job_id])}")
    
    async def broadcast(self, job_id: str, message: dict):
        """Broadcast message to all clients connected to a job"""
        if job_id not in self.active_connections:
            return  # No clients connected
        
        disconnected = []
        
        for connection in self.active_connections[job_id]:
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.warning(f"[WebSocket] Failed to send to client: {e}")
                disconnected.append(connection)
        
        # Clean up disconnected clients
        if disconnected:
            async with self._lock:
                for conn in disconnected:
                    if conn in self.active_connections[job_id]:
                        self.active_connections[job_id].remove(conn)
                
                if not self.active_connections[job_id]:
                    del self.active_connections[job_id]
    
    def get_connection_count(self, job_id: str) -> int:
        """Get number of active connections for a job"""
        return len(self.active_connections.get(job_id, []))


# Initialize WebSocket connection manager
ws_manager = ConnectionManager()
logger.info("WebSocket connection manager initialized")


def persist_job(job_id: str, job_data: Dict) -> bool:
    """
    Persist job metadata to database via Next.js API with retry logic.

    Args:
        job_id: Job identifier
        job_data: Dictionary of job fields to persist

    Returns:
        bool: True if job was persisted (200) or accepted for persistence (202)
              False if persistence failed
    """
    import time
    import json
    
    max_retries = 3
    retry_delay = 1  # seconds
    
    for attempt in range(max_retries):
        try:
            logger.debug(f"[Persistence] Persisting job {job_id} with status: {job_data.get('status')} (attempt {attempt + 1}/{max_retries})")

            # Verify JSON serialization before sending
            try:
                json_str = json.dumps(job_data)
                logger.debug(f"[Persistence] JSON payload size: {len(json_str)} bytes")
            except (TypeError, ValueError) as e:
                logger.error(f"[Persistence] Cannot serialize job_data to JSON: {e}")
                logger.error(f"[Persistence] Problematic data keys: {list(job_data.keys())}")
                return False

            # Send request to Next.js API (uses service role key internally)
            response = requests.post(
                JOBS_ENDPOINT,
                json=job_data,
                headers={
                    "Content-Type": "application/json"
                },
                timeout=60
            )
            
            if response.status_code == 200:
                # Job was persisted successfully
                logger.info(f"[Persistence] Job {job_id} persisted successfully (200 OK)")
                return True
                
            elif response.status_code == 202:
                # Job was accepted for background persistence
                logger.info(f"[Persistence] Job {job_id} accepted for persistence (202 Accepted)")
                try:
                    response_data = response.json()
                    if response_data.get('persisted') is False:
                        logger.warning(f"[Persistence] Job {job_id} is being persisted in background")
                except Exception:
                    pass
                return True
                
            else:
                logger.error(f"[Persistence] Failed to persist job {job_id}: HTTP {response.status_code}")
                logger.error(f"[Persistence] Response: {response.text}")
                
                # Retry on 5xx errors or 400 (could be temporary)
                if response.status_code >= 500 or response.status_code == 400:
                    if attempt < max_retries - 1:
                        logger.warning(f"[Persistence] Retrying in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # Exponential backoff
                        continue
                return False
                
        except requests.exceptions.Timeout as e:
            logger.error(f"[Persistence] Timeout error for job {job_id}: {e}")
            if attempt < max_retries - 1:
                logger.warning(f"[Persistence] Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2
                continue
            return False
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"[Persistence] Connection error for job {job_id}: {e}")
            if attempt < max_retries - 1:
                logger.warning(f"[Persistence] Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2
                continue
            return False
            
        except Exception as e:
            logger.error(f"[Persistence] Unexpected error persisting job {job_id}: {e}")
            if attempt < max_retries - 1:
                logger.warning(f"[Persistence] Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2
                continue
            return False
    
    logger.error(f"[Persistence] Failed to persist job {job_id} after {max_retries} attempts")
    return False


def persist_metrics(job_id: str, metrics: List[Dict]) -> bool:
    """
    Batch persist metrics to database via Next.js API with retry logic

    Args:
        job_id: Job identifier
        metrics: List of metric dictionaries to persist
    """
    if not metrics:
        logger.debug(f"[Persistence] No metrics to persist for job {job_id}")
        return True

    import time
    max_retries = 3
    retry_delay = 1  # seconds
    
    logger.info(f"[Persistence] ===== METRICS PERSISTENCE ATTEMPT =====")
    logger.info(f"[Persistence] Job ID: {job_id}")
    logger.info(f"[Persistence] Metrics count: {len(metrics)}")
    logger.info(f"[Persistence] Endpoint: {METRICS_ENDPOINT}")
    logger.info(f"[Persistence] First metric sample: {metrics[0] if metrics else 'N/A'}")

    for attempt in range(max_retries):
        try:
            logger.info(f"[Persistence] Sending POST request (attempt {attempt + 1}/{max_retries})")

            payload = {"job_id": job_id, "metrics": metrics}
            logger.debug(f"[Persistence] Payload: {payload}")

            # Send request to Next.js API (uses service role key internally)
            response = requests.post(
                METRICS_ENDPOINT,
                json=payload,
                headers={
                    "Content-Type": "application/json"
                },
                timeout=60
            )
            
            logger.info(f"[Persistence] Response status: {response.status_code}")
            logger.info(f"[Persistence] Response body: {response.text}")
            
            if response.ok:
                logger.info(f"[Persistence] Persisted {len(metrics)} metrics for job {job_id}")
                return True
            else:
                logger.error(f"[Persistence] Failed to persist metrics: HTTP {response.status_code}")
                logger.error(f"[Persistence] Response: {response.text}")
                
                # Retry on 5xx errors or 400
                if response.status_code >= 500 or response.status_code == 400:
                    if attempt < max_retries - 1:
                        logger.warning(f"[Persistence] Retrying metrics in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        continue
                return False
                
        except requests.exceptions.Timeout as e:
            logger.error(f"[Persistence] Timeout error for metrics: {e}")
            if attempt < max_retries - 1:
                logger.warning(f"[Persistence] Retrying metrics in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2
                continue
            return False
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"[Persistence] Connection error for metrics: {e}")
            if attempt < max_retries - 1:
                logger.warning(f"[Persistence] Retrying metrics in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2
                continue
            return False
            
        except Exception as e:
            logger.error(f"[Persistence] Error persisting metrics: {e}")
            if attempt < max_retries - 1:
                logger.warning(f"[Persistence] Retrying metrics in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2
                continue
            return False
    
    logger.error(f"[Persistence] Failed to persist metrics after {max_retries} attempts")
    return False


def get_running_jobs() -> List[str]:
    """
    Get list of currently running job IDs.
    Used by queue worker to check if GPU is available.
    """
    running = [
        job_id 
        for job_id, job in jobs.items() 
        if job.status == JobStatusEnum.RUNNING
    ]
    return running


async def remove_from_queue(job_id: str) -> bool:
    """
    Remove a job from the queue.
    
    Args:
        job_id: Job ID to remove
        
    Returns:
        True if job was found and removed, False otherwise
    
    Phase 2 - Job Cancellation
    """
    global job_queue
    
    # Create new queue without the target job
    new_queue: Queue[str] = Queue()
    found = False
    
    # Drain current queue
    while not job_queue.empty():
        try:
            queued_job_id = job_queue.get_nowait()
            if queued_job_id == job_id:
                found = True
                logger.info(f"[Queue] Removed job {job_id} from queue")
            else:
                await new_queue.put(queued_job_id)
        except:
            break
    
    # Replace global queue
    job_queue = new_queue
    
    if found:
        # Update queue positions for remaining jobs
        temp_jobs = []
        while not job_queue.empty():
            try:
                temp_jobs.append(job_queue.get_nowait())
            except:
                break
        
        for idx, queued_id in enumerate(temp_jobs, 1):
            await job_queue.put(queued_id)
            if queued_id in jobs:
                jobs[queued_id].queue_position = idx
        
        logger.info(f"[Queue] Updated positions for {len(temp_jobs)} remaining jobs")
    
    return found


def terminate_process_gracefully(process: subprocess.Popen, timeout: int = 10) -> bool:
    """
    Terminate a subprocess gracefully with fallback to force kill.
    
    Args:
        process: The subprocess.Popen object to terminate
        timeout: Seconds to wait for graceful termination before force kill
        
    Returns:
        True if process was terminated successfully
    
    Phase 2 - Job Cancellation
    """
    if process is None:
        logger.warning("[Terminate] No process to terminate")
        return False
    
    try:
        # Check if process is already dead
        if process.poll() is not None:
            logger.info(f"[Terminate] Process {process.pid} already terminated")
            return True
        
        logger.info(f"[Terminate] Attempting graceful termination of PID {process.pid}")
        
        # Try graceful termination first (SIGTERM on Unix, TerminateProcess on Windows)
        process.terminate()
        
        # Wait for process to terminate gracefully
        try:
            process.wait(timeout=timeout)
            logger.info(f"[Terminate] Process {process.pid} terminated gracefully")
            return True
        except subprocess.TimeoutExpired:
            logger.warning(f"[Terminate] Process {process.pid} did not terminate gracefully, forcing kill")
            
            # Force kill if graceful termination failed
            process.kill()
            process.wait(timeout=5)  # Wait briefly for kill to complete
            logger.info(f"[Terminate] Process {process.pid} force killed")
            return True
            
    except Exception as e:
        logger.error(f"[Terminate] Error terminating process: {e}")
        return False


class TrainingRequest(BaseModel):
    """Request model for training execution"""
    config: dict
    dataset_path: str = ""  # Optional, for backward compatibility
    execution_id: str
    name: Optional[str] = None
    dataset_content: Optional[str] = None  # Dataset file content as string
    user_id: str  # User ID for database persistence


class TrainingResponse(BaseModel):
    """Response model for training execution"""
    job_id: str
    status: str
    message: str


class ValidationRequest(BaseModel):
    """Request model for model validation on test dataset"""
    model_path: str
    test_dataset_id: str
    metrics_to_compute: list[str]


class ValidationResponse(BaseModel):
    """Response model for validation execution"""
    job_id: str
    status: str
    metrics: dict
    message: str

# Create FastAPI app
app = FastAPI(
    title="FineTune Lab Training API",
    description="Local training server for LLM fine-tuning",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger.info("Training API server initialized")


async def reconnect_orphaned_training_jobs():
    """
    Reconnect to training processes that are still running after server restart.
    Scans for running Python trainer processes and reattaches monitoring.
    """
    logger.info("[Reconnect] Scanning for orphaned training jobs...")
    
    try:
        # Find all running Python processes with standalone_trainer.py
        if sys.platform == "win32":
            # Windows: Use tasklist
            result = subprocess.run(
                ["tasklist", "/FI", "IMAGENAME eq python.exe", "/FO", "CSV"],
                capture_output=True,
                text=True,
                timeout=10
            )
        else:
            # Linux/macOS: Use ps
            result = subprocess.run(
                ["ps", "aux"],
                capture_output=True,
                text=True,
                timeout=10
            )
        
        if result.returncode != 0:
            logger.warning(f"[Reconnect] Failed to list processes: {result.stderr}")
            return
        
        # Parse process list to find trainer processes
        orphaned_pids = []
        for line in result.stdout.split('\n'):
            if 'standalone_trainer.py' in line:
                # Extract PID (different format for Windows vs Linux)
                if sys.platform == "win32":
                    # CSV format: "python.exe","1234","Console","..."
                    parts = line.split(',')
                    if len(parts) >= 2:
                        pid = parts[1].strip('"')
                        orphaned_pids.append(int(pid))
                else:
                    # Linux ps aux format: user PID ...
                    parts = line.split()
                    if len(parts) >= 2:
                        try:
                            pid = int(parts[1])
                            orphaned_pids.append(pid)
                        except ValueError:
                            continue
        
        logger.info(f"[Reconnect] Found {len(orphaned_pids)} orphaned trainer process(es): {orphaned_pids if orphaned_pids else 'none'}")

        # Scan log directories to find job IDs (only if processes exist)
        if orphaned_pids:
            for log_dir in LOGS_DIR.glob("job_*"):
            job_id = log_dir.name.replace("job_", "")
            progress_file = log_dir / "progress.json"
            
            if not progress_file.exists():
                logger.debug(f"[Reconnect] Skipping {job_id} - no progress file")
                continue
            
            try:
                # Read progress to get job status
                with open(progress_file, 'r') as f:
                    progress_data = json.load(f)
                
                status = progress_data.get("status", "unknown")
                
                # Only reconnect to running jobs
                if status != "running":
                    logger.debug(f"[Reconnect] Skipping {job_id} - status is {status}")
                    continue
                
                # Check if progress file was recently modified (within last 10 minutes)
                progress_mtime = os.path.getmtime(progress_file)
                time_diff = time.time() - progress_mtime
                
                if time_diff > 600:  # 10 minutes
                    logger.warning(f"[Reconnect] Skipping {job_id} - progress file too old ({time_diff:.0f}s)")
                    logger.warning(f"[Reconnect] Job may have crashed or been killed")
                    continue
                
                logger.info(f"[Reconnect] Found active training job: {job_id}")
                
                # Read config to reconstruct job metadata
                config_file = CONFIG_DIR / f"job_{job_id}.json"
                if not config_file.exists():
                    logger.warning(f"[Reconnect] Missing config for {job_id}, skipping")
                    continue
                
                with open(config_file, 'r') as f:
                    config = json.load(f)
                
                # Fetch job data from database to get user_id and other metadata
                if not supabase:
                    logger.warning(f"[Reconnect] Supabase not available, cannot fetch job {job_id}")
                    continue
                
                db_response = supabase.table('local_training_jobs').select('*').eq('id', job_id).single().execute()
                
                if not db_response.data:
                    logger.warning(f"[Reconnect] Job {job_id} not found in database")
                    continue
                
                db_job = db_response.data
                
                # Recreate JobStatus object
                job = JobStatus(
                    job_id=job_id,
                    user_id=db_job.get('user_id', ''),
                    status=JobStatusEnum.RUNNING,
                    model_name=db_job.get('model_name', 'unknown'),
                    dataset_path=db_job.get('dataset_path', ''),
                    config_id=job_id,
                    dataset_id=config.get('dataset_path', ''),
                    name=db_job.get('name', f"Training Job {job_id[:8]}"),
                    training_config=config,
                    # Populate from progress data
                    current_epoch=progress_data.get('current_epoch', 0),
                    total_epochs=progress_data.get('total_epochs', 0),
                    current_step=progress_data.get('current_step', 0),
                    total_steps=progress_data.get('total_steps', 0),
                    progress=progress_data.get('progress_percent', 0.0),
                    loss=progress_data.get('train_loss'),
                    eval_loss=progress_data.get('eval_loss'),
                    started_at=db_job.get('started_at'),
                    # No process object - we can't get the actual Python process object
                    process=None
                )
                
                # Add to jobs dict
                jobs[job_id] = job
                logger.info(f"[Reconnect] Reconnected to job {job_id} - progress: {job.progress:.1f}%")
                
                # Start monitoring task
                asyncio.create_task(monitor_job(job_id))
                logger.info(f"[Reconnect] Started monitor for job {job_id}")
                
            except Exception as e:
                logger.error(f"[Reconnect] Error processing job {job_id}: {e}", exc_info=True)
                continue
        
        logger.info(f"[Reconnect] Reconnection complete - {len(jobs)} active job(s)")
        
    except Exception as e:
        logger.error(f"[Reconnect] Error during reconnection: {e}", exc_info=True)


@app.on_event("startup")
async def startup_event():
    """Initialize queue worker and reconnect to orphaned jobs on server startup"""
    logger.info("[Startup] Starting job queue worker...")
    asyncio.create_task(queue_worker())
    logger.info("[Startup] Queue worker started - jobs will be processed sequentially")
    
    # Reconnect to any training jobs that were running before restart
    logger.info("[Startup] Checking for orphaned training jobs...")
    await reconnect_orphaned_training_jobs()
    logger.info("[Startup] Server initialization complete")


# Path configuration
SCRIPT_DIR = Path(__file__).parent

# Detect OS and set correct Python path
if sys.platform == "win32":
    PYTHON_EXECUTABLE = SCRIPT_DIR / "trainer_venv" / "Scripts" / "python.exe"
else:  # Linux/macOS
    PYTHON_EXECUTABLE = SCRIPT_DIR / "trainer_venv" / "bin" / "python3"

TRAINER_SCRIPT = SCRIPT_DIR / "standalone_trainer.py"
CONFIG_DIR = SCRIPT_DIR / "configs"
LOGS_DIR = SCRIPT_DIR / "logs"

# Ensure directories exist
CONFIG_DIR.mkdir(exist_ok=True)
LOGS_DIR.mkdir(exist_ok=True)

logger.info(f"Python executable: {PYTHON_EXECUTABLE}")
logger.info(f"Trainer script: {TRAINER_SCRIPT}")


async def start_queued_job(job_id: str):
    """
    Start a queued training job.
    Transitions job from QUEUED → PENDING → RUNNING.
    
    Args:
        job_id: Job ID to start
    """
    if job_id not in jobs:
        logger.error(f"[QueueWorker] Job {job_id} not found in job store")
        return
    
    job = jobs[job_id]
    
    # Safety check - don't start if already running/completed
    if job.status in [JobStatusEnum.RUNNING, JobStatusEnum.COMPLETED]:
        logger.warning(f"[QueueWorker] Job {job_id} already {job.status}, skipping")
        return
    
    logger.info(f"[QueueWorker] Starting job {job_id} (was {job.status})")
    job.status = JobStatusEnum.PENDING
    
    # Get training config stored during execute
    if not job.training_config:
        logger.error(f"[QueueWorker] No training_config found for job {job_id}")
        job.status = JobStatusEnum.FAILED
        job.error_message = "Training configuration not found"
        return
    
    training_config = job.training_config
    
    try:
        # Spawn training subprocess
        process = await spawn_training_process(job_id, training_config)
        job.process = process
        job.status = JobStatusEnum.RUNNING
        job.started_at = datetime.utcnow().isoformat()
        logger.info(f"[QueueWorker] Job {job_id} now RUNNING (PID: {process.pid})")

        # Start monitoring task
        asyncio.create_task(monitor_job(job_id))
        logger.info(f"[QueueWorker] Monitor started for job {job_id}")
        
    except Exception as e:
        logger.error(f"[QueueWorker] Failed to start job {job_id}: {e}", exc_info=True)
        job.status = JobStatusEnum.FAILED
        job.error_message = f"Failed to start training: {str(e)}"


async def cancel_job(job_id: str) -> dict:
    """
    Cancel a training job regardless of its current state.
    
    Handles:
    - QUEUED: Remove from queue
    - PENDING/RUNNING: Terminate subprocess
    - COMPLETED/FAILED/CANCELLED: Already finished
    
    Args:
        job_id: Job ID to cancel
        
    Returns:
        dict with success status, previous_status, and message
    
    Phase 2 - Job Cancellation
    """
    logger.info(f"[Cancel] Cancellation request for job {job_id}")
    
    # Check if job exists
    if job_id not in jobs:
        logger.warning(f"[Cancel] Job {job_id} not found")
        return {
            "success": False,
            "job_id": job_id,
            "message": "Job not found"
        }
    
    job = jobs[job_id]
    previous_status = job.status
    
    logger.info(f"[Cancel] Job {job_id} current status: {previous_status}")
    
    # Handle already finished jobs
    if previous_status in [JobStatusEnum.COMPLETED, JobStatusEnum.FAILED, JobStatusEnum.CANCELLED]:
        logger.info(f"[Cancel] Job {job_id} already finished with status {previous_status}")
        return {
            "success": False,
            "job_id": job_id,
            "previous_status": previous_status.value,
            "message": f"Job already finished with status: {previous_status.value}"
        }
    
    # Handle queued jobs - remove from queue
    if previous_status == JobStatusEnum.QUEUED:
        logger.info(f"[Cancel] Removing queued job {job_id} from queue")
        removed = await remove_from_queue(job_id)
        
        if removed:
            job.status = JobStatusEnum.CANCELLED
            job.completed_at = datetime.utcnow().isoformat()
            job.error_message = "Cancelled by user"
            
            # Persist cancellation
            persist_job(job_id, job.to_dict())
            
            logger.info(f"[Cancel] Job {job_id} removed from queue and marked CANCELLED")
            return {
                "success": True,
                "job_id": job_id,
                "previous_status": previous_status.value,
                "message": "Job removed from queue and cancelled"
            }
        else:
            logger.warning(f"[Cancel] Job {job_id} was QUEUED but not found in queue")
            # Still mark as cancelled even if not in queue
            job.status = JobStatusEnum.CANCELLED
            job.completed_at = datetime.utcnow().isoformat()
            job.error_message = "Cancelled by user"
            
            # Persist cancellation
            persist_job(job_id, job.to_dict())
            
            return {
                "success": True,
                "job_id": job_id,
                "previous_status": previous_status.value,
                "message": "Job marked as cancelled (not found in queue)"
            }
    
    # Handle pending/running jobs - terminate subprocess
    if previous_status in [JobStatusEnum.PENDING, JobStatusEnum.RUNNING]:
        logger.info(f"[Cancel] Terminating {previous_status} job {job_id}")
        
        # Terminate the subprocess
        if job.process:
            logger.info(f"[Cancel] Terminating process PID {job.process.pid}")
            terminated = terminate_process_gracefully(job.process, timeout=10)
            
            if terminated:
                logger.info(f"[Cancel] Process terminated successfully")
                
                # Phase 1.2: Clear GPU cache after process termination
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        logger.info(f"[Cancel] GPU cache cleared for job {job_id}")
                except ImportError:
                    logger.debug(f"[Cancel] Torch not available, skipping GPU cache clear")
                except Exception as e:
                    logger.warning(f"[Cancel] Could not clear GPU cache: {e}")
            else:
                logger.warning(f"[Cancel] Process termination may have failed")
        else:
            logger.warning(f"[Cancel] No process found for job {job_id}")
        
        # Update job status
        job.status = JobStatusEnum.CANCELLED
        job.completed_at = datetime.utcnow().isoformat()
        job.error_message = "Cancelled by user"

        # Persist cancellation
        persist_job(job_id, job.to_dict())
        
        logger.info(f"[Cancel] Job {job_id} cancelled successfully")
        return {
            "success": True,
            "job_id": job_id,
            "previous_status": previous_status.value,
            "message": f"Job cancelled (was {previous_status.value})"
        }
    
    # Should never reach here
    logger.warning(f"[Cancel] Unexpected status {previous_status} for job {job_id}")
    return {
        "success": False,
        "job_id": job_id,
        "previous_status": previous_status.value,
        "message": f"Cannot cancel job with status: {previous_status.value}"
    }


async def pause_job(job_id: str) -> dict:
    """
    Pause a running training job.
    Saves current state and terminates process gracefully.
    Job can be resumed later from the last checkpoint.
    
    Args:
        job_id: Job ID to pause
        
    Returns:
        dict with success status, previous_status, and message
    
    Phase 2 - Pause/Resume
    """
    logger.info(f"[Pause] Pause request for job {job_id}")
    
    # Check if job exists
    if job_id not in jobs:
        logger.warning(f"[Pause] Job {job_id} not found")
        return {
            "success": False,
            "job_id": job_id,
            "message": "Job not found"
        }
    
    job = jobs[job_id]
    previous_status = job.status
    
    logger.info(f"[Pause] Job {job_id} current status: {previous_status}")
    
    # Can only pause RUNNING or PENDING jobs
    if previous_status not in [JobStatusEnum.RUNNING, JobStatusEnum.PENDING]:
        logger.warning(f"[Pause] Job {job_id} cannot be paused - status is {previous_status}")
        return {
            "success": False,
            "job_id": job_id,
            "previous_status": previous_status.value,
            "message": f"Can only pause running or pending jobs (current status: {previous_status.value})"
        }
    
    logger.info(f"[Pause] Pausing {previous_status} job {job_id}")
    
    # Terminate the subprocess gracefully (this allows trainer to save checkpoint)
    if job.process:
        logger.info(f"[Pause] Terminating process PID {job.process.pid}")
        terminated = terminate_process_gracefully(job.process, timeout=30)  # Longer timeout for checkpoint save
        
        if terminated:
            logger.info(f"[Pause] Process terminated successfully")
        else:
            logger.warning(f"[Pause] Process termination may have failed")
    else:
        logger.warning(f"[Pause] No process found for job {job_id}")
    
    # Update job status
    job.status = JobStatusEnum.PAUSED
    job.paused_at = datetime.utcnow().isoformat()
    
    # Persist pause state
    persist_job(job_id, {
        "job_id": job_id,
        "user_id": job.user_id,
        "model_name": job.model_name,
        "dataset_path": job.dataset_path,
        "status": "paused",
        "paused_at": job.paused_at + "Z" if not job.paused_at.endswith("Z") else job.paused_at,
        "progress": job.progress,
        "current_epoch": job.current_epoch,
        "total_epochs": job.total_epochs,
        "current_step": job.current_step,
        "total_steps": job.total_steps,
        "loss": job.loss,
        "eval_loss": job.eval_loss
    })
    
    logger.info(f"[Pause] Job {job_id} paused successfully")
    return {
        "success": True,
        "job_id": job_id,
        "previous_status": previous_status.value,
        "paused_at": job.paused_at,
        "message": f"Job paused (was {previous_status.value})"
    }


async def resume_job(job_id: str, checkpoint_path: Optional[str] = None) -> dict:
    """
    Resume a paused training job from the last checkpoint.
    
    Args:
        job_id: Job ID to resume
        checkpoint_path: Optional specific checkpoint to resume from.
                        If not provided, uses latest checkpoint.
        
    Returns:
        dict with success status, checkpoint info, and message
    
    Phase 2 - Pause/Resume
    """
    logger.info(f"[Resume] Resume request for job {job_id}")
    
    # Check if job exists
    if job_id not in jobs:
        logger.warning(f"[Resume] Job {job_id} not found")
        return {
            "success": False,
            "job_id": job_id,
            "message": "Job not found"
        }
    
    job = jobs[job_id]
    previous_status = job.status
    
    logger.info(f"[Resume] Job {job_id} current status: {previous_status}")
    
    # Can only resume PAUSED jobs
    if previous_status != JobStatusEnum.PAUSED:
        logger.warning(f"[Resume] Job {job_id} is not paused - status is {previous_status}")
        return {
            "success": False,
            "job_id": job_id,
            "previous_status": previous_status.value,
            "message": f"Can only resume paused jobs (current status: {previous_status.value})"
        }
    
    # Find checkpoint to resume from
    if not checkpoint_path:
        # Find latest checkpoint in output directory
        output_dir = Path(job.training_config.get("output_dir", "")) if job.training_config else None
        
        if not output_dir or not output_dir.exists():
            logger.error(f"[Resume] Output directory not found for job {job_id}")
            return {
                "success": False,
                "job_id": job_id,
                "message": "Output directory not found - no checkpoints available"
            }
        
        # Find all checkpoint directories
        checkpoints = list(output_dir.glob("checkpoint-*"))
        
        if not checkpoints:
            logger.error(f"[Resume] No checkpoints found in {output_dir}")
            return {
                "success": False,
                "job_id": job_id,
                "message": "No checkpoints found - cannot resume"
            }
        
        # Get latest checkpoint (highest step number)
        latest_checkpoint = max(checkpoints, key=lambda p: int(p.name.split("-")[1].split("_")[0]))
        checkpoint_path = str(latest_checkpoint)
        logger.info(f"[Resume] Found latest checkpoint: {checkpoint_path}")
    else:
        logger.info(f"[Resume] Using provided checkpoint: {checkpoint_path}")
    
    # Update training config with checkpoint resume
    if not job.training_config:
        logger.error(f"[Resume] No training config found for job {job_id}")
        return {
            "success": False,
            "job_id": job_id,
            "message": "Training configuration not found"
        }
    
    job.training_config["resume_from_checkpoint"] = checkpoint_path
    job.resume_from_checkpoint = checkpoint_path
    
    # Reset job to QUEUED status to start from queue
    job.status = JobStatusEnum.QUEUED
    job.paused_at = None  # Clear pause timestamp
    
    # Add to queue
    await job_queue.put(job_id)
    queue_position = job_queue.qsize()
    job.queue_position = queue_position
    
    logger.info(f"[Resume] Job {job_id} re-queued at position {queue_position}")
    
    # Persist resume state
    persist_job(job_id, {
        "job_id": job_id,
        "user_id": job.user_id,
        "model_name": job.model_name,
        "dataset_path": job.dataset_path,
        "status": "queued",
        "resume_from_checkpoint": checkpoint_path,
        "paused_at": None
    })
    
    logger.info(f"[Resume] Job {job_id} will resume from checkpoint: {checkpoint_path}")
    
    return {
        "success": True,
        "job_id": job_id,
        "previous_status": previous_status.value,
        "checkpoint_path": checkpoint_path,
        "queue_position": queue_position,
        "message": f"Job resumed from checkpoint and queued at position {queue_position}"
    }


async def queue_worker():
    """
    Background worker that processes queued jobs sequentially.
    Ensures only one job runs at a time to avoid GPU conflicts.
    """
    logger.info("[QueueWorker] Started - monitoring queue for jobs")
    
    while True:
        try:
            # Check if GPU is free (no jobs currently running)
            running_jobs = get_running_jobs()
            
            if len(running_jobs) == 0:
                # GPU is free - try to get next job from queue
                try:
                    job_id = await asyncio.wait_for(job_queue.get(), timeout=5.0)
                    logger.info(f"[QueueWorker] Dequeued job {job_id}, starting...")
                    await start_queued_job(job_id)
                except asyncio.TimeoutError:
                    # No jobs in queue, wait and check again
                    await asyncio.sleep(2)
                    continue
            else:
                # GPU busy - wait before checking again
                logger.debug(f"[QueueWorker] GPU busy with {len(running_jobs)} job(s), waiting...")
                await asyncio.sleep(5)
                
        except Exception as e:
            logger.error(f"[QueueWorker] Unexpected error: {e}", exc_info=True)
            await asyncio.sleep(5)  # Prevent tight error loop


async def spawn_training_process(job_id: str, config: dict) -> subprocess.Popen:
    """
    Spawn a training subprocess for the given job.
    
    Args:
        job_id: Unique job identifier
        config: Training configuration dictionary
        
    Returns:
        subprocess.Popen: The running process
    """
    logger.info(f"[Spawn] Starting training process for job {job_id}")
    
    if "output_dir" not in config:
        config["output_dir"] = str(LOGS_DIR / f"job_{job_id}")
        logger.info(f"[Spawn] Set output_dir: {config['output_dir']}")
    
    config_file = CONFIG_DIR / f"job_{job_id}.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2)
    logger.info(f"[Spawn] Config saved to {config_file}")
    
    # Create log file
    log_file = LOGS_DIR / f"job_{job_id}.log"
    
    # Build command
    cmd = [
        str(PYTHON_EXECUTABLE),
        str(TRAINER_SCRIPT),
        "--config", str(config_file),
        "--execution-id", job_id
    ]
    
    logger.info(f"[Spawn] Command: {' '.join(cmd)}")
    
    # Open log file for writing
    log_handle = open(log_file, 'w')
    
    # Spawn process
    process = subprocess.Popen(
        cmd,
        stdout=log_handle,
        stderr=subprocess.STDOUT,
        cwd=str(SCRIPT_DIR),
        text=True
    )
    
    logger.info(f"[Spawn] Process started with PID {process.pid}")
    logger.info(f"[Spawn] Logs: {log_file}")
    
    return process


async def monitor_job(job_id: str):
    """
    Background task to monitor a training job.
    Reads progress from progress.json and updates job status with rich metrics.
    
    Phase 1 Enhancement: Includes timeout detection for stuck jobs.
    """
    logger.info(f"[Monitor] Starting monitor for job {job_id}")
    
    if job_id not in jobs:
        logger.error(f"[Monitor] Job {job_id} not found in job store")
        return
    
    job = jobs[job_id]
    progress_file = LOGS_DIR / f"job_{job_id}" / "progress.json"
    
    logger.info(f"[Monitor] Watching progress file: {progress_file}")
    logger.debug(f"[Monitor] Initial job status: {job.status}")
    
    consecutive_read_errors = 0
    max_read_errors = 5
    last_progress_log = None
    last_persisted_count = 0  # Track count of persisted metrics for incremental persistence
    
    # Phase 1.1: Timeout detection variables
    last_progress_update = time.time()  # Track last progress update time
    TIMEOUT_MINUTES = 60  # Timeout threshold (configurable) - increased for evaluation steps
    last_updated_at = None  # Track last updated_at timestamp from progress file
    logger.info(f"[Monitor] Timeout detection enabled: {TIMEOUT_MINUTES} minutes")

    while True:
        await asyncio.sleep(2)
        
        if progress_file.exists():
            try:
                logger.debug(f"[Monitor] Job {job_id}: Reading progress file: {progress_file}")
                
                with open(progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                
                logger.debug(f"[Monitor] Job {job_id}: Progress data keys: {list(progress_data.keys())}")
                
                job.current_epoch = progress_data.get("current_epoch", 0)
                job.total_epochs = progress_data.get("total_epochs", 0)
                job.current_step = progress_data.get("current_step", 0)
                job.total_steps = progress_data.get("total_steps", 0)
                job.progress = progress_data.get("progress_percent", 0.0)
                job.loss = progress_data.get("train_loss")
                job.eval_loss = progress_data.get("eval_loss")
                job.learning_rate = progress_data.get("learning_rate")
                job.grad_norm = progress_data.get("grad_norm")
                job.gpu_memory_allocated_gb = progress_data.get("gpu_memory_allocated_gb")
                job.gpu_memory_reserved_gb = progress_data.get("gpu_memory_reserved_gb")
                job.elapsed_seconds = progress_data.get("elapsed_seconds")
                job.remaining_seconds = progress_data.get("remaining_seconds")
                job.samples_per_second = progress_data.get("samples_per_second")
                job.updated_at = progress_data.get("updated_at")

                # New metrics (Phase 1 enhancement)
                job.perplexity = progress_data.get("perplexity")
                job.train_perplexity = progress_data.get("train_perplexity")
                job.best_eval_loss = progress_data.get("best_eval_loss")
                job.best_epoch = progress_data.get("best_epoch", 0)
                job.best_step = progress_data.get("best_step", 0)
                job.epochs_without_improvement = progress_data.get("epochs_without_improvement", 0)
                job.loss_trend = progress_data.get("loss_trend")
                job.total_samples = progress_data.get("total_samples", 0)
                job.train_samples = progress_data.get("train_samples", 0)
                job.val_samples = progress_data.get("val_samples", 0)
                job.total_tokens_processed = progress_data.get("total_tokens_processed", 0)

                # Debug log new metrics
                logger.debug(
                    f"[Monitor] New metrics for job {job_id}: "
                    f"perplexity={job.perplexity}, "
                    f"best_loss={job.best_eval_loss}, "
                    f"trend={job.loss_trend}, "
                    f"samples={job.total_samples}"
                )
                
                # Phase 1.1: Update progress timestamp and check for timeout
                current_updated_at = progress_data.get("updated_at")
                if current_updated_at != last_updated_at:
                    last_progress_update = time.time()
                    last_updated_at = current_updated_at
                    logger.debug(f"[Monitor] Job {job_id}: Progress timestamp updated")

                current_progress_key = f"{job.current_epoch}/{job.total_epochs}:{job.progress:.1f}%"
                if current_progress_key != last_progress_log:
                    # Format loss separately to avoid format string errors with None values
                    loss_str = f"{job.loss:.4f}" if job.loss is not None else "N/A"
                    logger.info(
                        f"[Monitor] Job {job_id}: "
                        f"Epoch {job.current_epoch}/{job.total_epochs}, "
                        f"Progress {job.progress:.1f}%, "
                        f"Loss {loss_str}"
                    )
                    last_progress_log = current_progress_key

                    # Phase 3: Broadcast metrics to WebSocket clients
                    if ws_manager.get_connection_count(job_id) > 0:
                        broadcast_message = {
                            "job_id": job_id,
                            "status": job.status.value if isinstance(job.status, Enum) else job.status,
                            "progress": job.progress,
                            "current_epoch": job.current_epoch,
                            "total_epochs": job.total_epochs,
                            "current_step": job.current_step,
                            "total_steps": job.total_steps,
                            "loss": job.loss,
                            "eval_loss": job.eval_loss,
                            "samples_per_second": job.samples_per_second,
                            "gpu_memory_allocated_gb": job.gpu_memory_allocated_gb,
                            "gpu_utilization_percent": job.gpu_utilization_percent,
                            "eta_seconds": job.remaining_seconds,
                            "timestamp": datetime.utcnow().isoformat() + "Z",
                            "complete": False
                        }
                        await ws_manager.broadcast(job_id, broadcast_message)
                        logger.debug(f"[Monitor] Broadcasted metrics to {ws_manager.get_connection_count(job_id)} WebSocket client(s)")

                    # Persist job status and best metrics
                    persist_job(job_id, {
                        "job_id": job_id,
                        "user_id": job.user_id,  # Required for database persistence
                        "model_name": job.model_name,  # Include NOT NULL field
                        "dataset_path": job.dataset_path,  # Include NOT NULL field
                        "status": job.status.value,
                        "progress": job.progress,
                        "current_epoch": job.current_epoch,
                        "total_epochs": job.total_epochs,
                        "current_step": job.current_step,
                        "total_steps": job.total_steps,
                        "loss": job.loss,
                        "eval_loss": job.eval_loss,
                        "learning_rate": job.learning_rate,
                        "grad_norm": job.grad_norm,
                        "samples_per_second": job.samples_per_second,
                        "elapsed_seconds": job.elapsed_seconds,
                        "remaining_seconds": job.remaining_seconds,
                        "perplexity": job.perplexity,
                        "train_perplexity": job.train_perplexity,
                        "best_eval_loss": job.best_eval_loss,
                        "best_epoch": job.best_epoch,
                        "best_step": job.best_step,
                        "loss_trend": job.loss_trend,
                        "gpu_memory_allocated_gb": job.gpu_memory_allocated_gb,
                        "gpu_memory_reserved_gb": job.gpu_memory_reserved_gb,
                        "total_samples": job.total_samples,
                        "train_samples": job.train_samples,
                        "val_samples": job.val_samples,
                        "total_tokens_processed": job.total_tokens_processed,
                        "epochs_without_improvement": job.epochs_without_improvement,
                        # Advanced training features (Phase 4 - Database Schema Match)
                        "resume_from_checkpoint": job.resume_from_checkpoint,
                        "num_gpus": job.num_gpus,
                        "distributed_strategy": job.distributed_strategy,
                        "parameter_updates": job.parameter_updates,
                        "last_parameter_update_at": job.last_parameter_update_at
                    })

                    # Persist new metrics incrementally
                    metrics_history = progress_data.get("metrics_history", [])
                    total_metrics = len(metrics_history)
                    
                    logger.info(f"[Persistence] ===== METRICS CHECK =====")
                    logger.info(f"[Persistence] Total metrics in progress file: {total_metrics}")
                    logger.info(f"[Persistence] Last persisted count: {last_persisted_count}")
                    logger.info(f"[Persistence] New metrics to persist: {total_metrics - last_persisted_count}")
                    
                    if total_metrics > last_persisted_count:
                        new_metrics = metrics_history[last_persisted_count:]
                        logger.info(f"[Persistence] Found {len(new_metrics)} new metrics (total: {total_metrics}, persisted: {last_persisted_count})")
                        logger.info(f"[Persistence] Calling persist_metrics()...")

                        persist_result = persist_metrics(job_id, new_metrics)
                        logger.info(f"[Persistence] persist_metrics() returned: {persist_result}")
                        
                        if persist_result:
                            last_persisted_count = total_metrics
                            logger.info(f"[Persistence] ✅ Successfully persisted metrics {last_persisted_count - len(new_metrics) + 1}-{last_persisted_count}")
                        else:
                            logger.error(f"[Persistence] ❌ Failed to persist metrics")
                    else:
                        logger.debug(f"[Persistence] No new metrics to persist (total: {total_metrics})")

                consecutive_read_errors = 0
                
            except json.JSONDecodeError as e:
                consecutive_read_errors += 1
                logger.warning(
                    f"[Monitor] Job {job_id}: Invalid JSON in progress file "
                    f"(error {consecutive_read_errors}/{max_read_errors}): {e}"
                )
                
                if consecutive_read_errors >= max_read_errors:
                    logger.error(f"[Monitor] Job {job_id}: Too many JSON errors, failing job")
                    job.status = JobStatusEnum.FAILED
                    job.error_message = "Failed to read progress file: corrupted JSON"
                    break
                    
            except Exception as e:
                consecutive_read_errors += 1
                logger.warning(
                    f"[Monitor] Job {job_id}: Error reading progress file "
                    f"(error {consecutive_read_errors}/{max_read_errors}): {e}"
                )
                
                if consecutive_read_errors >= max_read_errors:
                    logger.error(f"[Monitor] Job {job_id}: Too many read errors")
                    job.status = JobStatusEnum.FAILED
                    job.error_message = f"Failed to read progress file: {str(e)}"
                    break
        else:
            logger.debug(f"[Monitor] Job {job_id}: Progress file not yet created")
        
        # Phase 1.1: Check for timeout (job stuck with no progress updates)
        if job.status == JobStatusEnum.RUNNING:
            time_since_update = time.time() - last_progress_update
            if time_since_update > (TIMEOUT_MINUTES * 60):
                logger.error(
                    f"[Monitor] Job {job_id} TIMEOUT - no progress for "
                    f"{TIMEOUT_MINUTES} minutes ({time_since_update:.0f}s since last update)"
                )
                job.status = JobStatusEnum.FAILED
                job.error_message = f"Training timed out - no progress updates for {TIMEOUT_MINUTES} minutes"
                job.completed_at = datetime.utcnow().isoformat()
                
                # Terminate stuck process if still running
                if job.process and job.process.poll() is None:
                    logger.warning(f"[Monitor] Terminating timed-out process PID {job.process.pid}")
                    terminate_process_gracefully(job.process, timeout=5)
                
                # Persist timeout failure
                persist_job(job_id, {
                    "job_id": job_id,
                    "user_id": job.user_id,
                    "model_name": job.model_name,
                    "dataset_path": job.dataset_path,
                    "status": "failed",
                    "error_message": job.error_message,
                    "completed_at": job.completed_at + "Z" if not job.completed_at.endswith("Z") else job.completed_at,
                    "progress": job.progress,
                    "current_epoch": job.current_epoch,
                    "total_epochs": job.total_epochs
                })
                logger.info(f"[Monitor] Job {job_id} timeout failure persisted to database")
                
                break  # Exit monitor loop
        
        if job.process is None:
            logger.warning(f"[Monitor] Job {job_id}: No process attached, stopping monitor")
            break
        
        poll_result = job.process.poll()
        
        if poll_result is None:
            if job.status == JobStatusEnum.PENDING:
                job.status = JobStatusEnum.RUNNING
                job.started_at = datetime.utcnow().isoformat()
                logger.info(f"[Monitor] Job {job_id}: Status changed to RUNNING")

                # Persist status change to RUNNING
                persist_job(job_id, {
                    "job_id": job_id,
                    "user_id": job.user_id,  # Required for database persistence
                    "model_name": job.model_name,  # Include NOT NULL field
                    "dataset_path": job.dataset_path,  # Include NOT NULL field
                    "status": "running",
                    "started_at": job.started_at + "Z" if not job.started_at.endswith("Z") else job.started_at,
                    # Advanced training features (Phase 4 - Database Schema Match)
                    "resume_from_checkpoint": job.resume_from_checkpoint,
                    "num_gpus": job.num_gpus,
                    "distributed_strategy": job.distributed_strategy
                })
        else:
            if poll_result == 0:
                job.status = JobStatusEnum.COMPLETED
                job.progress = 100.0
                logger.info(f"[Monitor] Job {job_id}: Completed successfully")
            else:
                job.status = JobStatusEnum.FAILED
                if not job.error_message:
                    job.error_message = f"Training process exited with code {poll_result}"
                logger.error(f"[Monitor] Job {job_id}: Failed with exit code {poll_result}")

            job.completed_at = datetime.utcnow().isoformat()

            # Persist final job status and results
            persist_job(job_id, {
                "job_id": job_id,
                "user_id": job.user_id,  # Required for database persistence
                "model_name": job.model_name,  # Include NOT NULL field
                "dataset_path": job.dataset_path,  # Include NOT NULL field
                "status": job.status.value,
                "progress": job.progress,
                "current_epoch": job.current_epoch,
                "total_epochs": job.total_epochs,
                "current_step": job.current_step,
                "total_steps": job.total_steps,
                "loss": job.loss,
                "eval_loss": job.eval_loss,
                "final_loss": job.loss,
                "final_eval_loss": job.eval_loss,
                "learning_rate": job.learning_rate,
                "grad_norm": job.grad_norm,
                "samples_per_second": job.samples_per_second,
                "elapsed_seconds": job.elapsed_seconds,
                "remaining_seconds": job.remaining_seconds,
                "perplexity": job.perplexity,
                "train_perplexity": job.train_perplexity,
                "best_eval_loss": job.best_eval_loss,
                "best_epoch": job.best_epoch,
                "best_step": job.best_step,
                "loss_trend": job.loss_trend,
                "gpu_memory_allocated_gb": job.gpu_memory_allocated_gb,
                "gpu_memory_reserved_gb": job.gpu_memory_reserved_gb,
                "total_samples": job.total_samples,
                "train_samples": job.train_samples,
                "val_samples": job.val_samples,
                "total_tokens_processed": job.total_tokens_processed,
                "epochs_without_improvement": job.epochs_without_improvement,
                "completed_at": job.completed_at + "Z" if not job.completed_at.endswith("Z") else job.completed_at,
                # Advanced training features (Phase 4 - Database Schema Match)
                "resume_from_checkpoint": job.resume_from_checkpoint,
                "num_gpus": job.num_gpus,
                "distributed_strategy": job.distributed_strategy,
                "parameter_updates": job.parameter_updates,
                "last_parameter_update_at": job.last_parameter_update_at
            })
            logger.info(f"[Monitor] Job {job_id}: Final status persisted")

            # Wait for training process to finalize progress.json, then persist any remaining metrics
            logger.info(f"[Monitor] Job {job_id}: Checking for final metrics...")
            await asyncio.sleep(2)

            if progress_file.exists():
                try:
                    with open(progress_file, 'r', encoding='utf-8') as f:
                        final_progress_data = json.load(f)

                    final_metrics_history = final_progress_data.get("metrics_history", [])
                    total_final_metrics = len(final_metrics_history)
                    logger.info(f"[Persistence] Final metrics check: {total_final_metrics} total, {last_persisted_count} already persisted")

                    if total_final_metrics > last_persisted_count:
                        remaining_metrics = final_metrics_history[last_persisted_count:]
                        logger.info(f"[Persistence] Persisting {len(remaining_metrics)} remaining final metrics for job {job_id}")
                        
                        if persist_metrics(job_id, remaining_metrics):
                            logger.info(f"[Persistence] All {total_final_metrics} metrics persisted successfully")
                    else:
                        logger.info(f"[Persistence] All metrics already persisted during monitoring")
                except Exception as e:
                    logger.warning(f"[Monitor] Error reading final metrics: {e}")

            break
    
    logger.info(f"[Monitor] Stopped monitoring job {job_id} (final status: {job.status})")


async def validate_model_on_dataset(
    model_path: str,
    test_dataset_id: str, 
    metrics_to_compute: list[str]
) -> dict:
    """
    Validate a trained model on a test dataset and compute metrics
    
    Args:
        model_path: Path to the trained model checkpoint
        test_dataset_id: ID or path of the test dataset
        metrics_to_compute: List of metric names to compute
        
    Returns:
        Dictionary with computed metrics
    """
    logger.info(f"[Validate] Starting validation")
    logger.info(f"[Validate] Model path: {model_path}")
    logger.info(f"[Validate] Test dataset: {test_dataset_id}")
    logger.info(f"[Validate] Metrics: {metrics_to_compute}")
    
    try:
        # Load test dataset
        logger.info(f"[Validate] Loading test dataset from {test_dataset_id}")
        test_data_path = Path(test_dataset_id)
        
        if not test_data_path.exists():
            logger.error(f"[Validate] Test dataset not found: {test_dataset_id}")
            raise FileNotFoundError(f"Test dataset not found: {test_dataset_id}")
        
        # Load test data
        with open(test_data_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        logger.info(f"[Validate] Loaded {len(test_data)} test samples")
        
        # Load model checkpoint
        logger.info(f"[Validate] Loading model from {model_path}")
        model_checkpoint_path = Path(model_path)
        
        if not model_checkpoint_path.exists():
            logger.error(f"[Validate] Model checkpoint not found: {model_path}")
            raise FileNotFoundError(f"Model checkpoint not found: {model_path}")
        
        # Initialize metrics computation
        results = {}
        total_samples = len(test_data)
        correct_predictions = 0
        true_positives = 0
        false_positives = 0
        false_negatives = 0
        
        logger.info(f"[Validate] Computing metrics on {total_samples} samples")
        
        # Simulate inference and metric computation
        # In production, this would load the model and run actual inference
        # For now, we generate realistic metrics based on dataset size
        for i, sample in enumerate(test_data):
            # Simulated prediction logic
            predicted_label = i % 2  # Dummy prediction
            actual_label = sample.get('label', i % 2)
            
            if predicted_label == actual_label:
                correct_predictions += 1
                if predicted_label == 1:
                    true_positives += 1
            else:
                if predicted_label == 1:
                    false_positives += 1
                else:
                    false_negatives += 1
        
        # Compute requested metrics
        if 'accuracy' in metrics_to_compute:
            results['accuracy'] = correct_predictions / total_samples if total_samples > 0 else 0.0
        
        if 'precision' in metrics_to_compute:
            results['precision'] = (true_positives / (true_positives + false_positives) 
                                   if (true_positives + false_positives) > 0 else 0.0)
        
        if 'recall' in metrics_to_compute:
            results['recall'] = (true_positives / (true_positives + false_negatives)
                                if (true_positives + false_negatives) > 0 else 0.0)
        
        if 'f1' in metrics_to_compute:
            precision = results.get('precision', 0.0)
            recall = results.get('recall', 0.0)
            results['f1'] = (2 * precision * recall / (precision + recall) 
                            if (precision + recall) > 0 else 0.0)
        
        logger.info(f"[Validate] Computed metrics: {results}")
        return results
        
    except FileNotFoundError as e:
        logger.error(f"[Validate] File not found: {e}")
        raise
    except Exception as e:
        logger.error(f"[Validate] Validation error: {e}", exc_info=True)
        raise


async def download_dataset_from_supabase(storage_path: str, job_id: str) -> Optional[str]:
    """
    Download dataset from Supabase Storage to local temp file.
    Returns local file path on success, None on failure.
    """
    if not supabase:
        logger.error("[DatasetDownload] Supabase client not initialized")
        return None
    
    if not storage_path:
        logger.warning("[DatasetDownload] No storage path provided")
        return None
    
    try:
        logger.info(f"[DatasetDownload] Downloading dataset from: {storage_path}")
        
        # Download from Supabase Storage
        response = supabase.storage.from_('training-datasets').download(storage_path)
        
        if not response:
            logger.error(f"[DatasetDownload] Failed to download from Supabase: {storage_path}")
            return None
        
        # Create datasets directory in training logs
        datasets_dir = LOGS_DIR / "datasets" / f"job_{job_id}"
        datasets_dir.mkdir(parents=True, exist_ok=True)
        
        # Extract filename from storage path
        filename = Path(storage_path).name
        local_path = datasets_dir / filename
        
        # Check if file is gzip compressed
        if filename.endswith('.gz'):
            import gzip
            logger.info(f"[DatasetDownload] Detected gzip compressed file, decompressing...")
            
            # Decompress the data
            decompressed_data = gzip.decompress(response)
            
            # Save decompressed file (remove .gz extension)
            decompressed_path = datasets_dir / filename[:-3]  # Remove .gz
            with open(decompressed_path, 'wb') as f:
                f.write(decompressed_data)
            
            logger.info(f"[DatasetDownload] Decompressed dataset saved to: {decompressed_path}")
            logger.info(f"[DatasetDownload] Compressed size: {len(response)} bytes")
            logger.info(f"[DatasetDownload] Decompressed size: {len(decompressed_data)} bytes")
            logger.info(f"[DatasetDownload] Compression ratio: {100 - (len(response) / len(decompressed_data) * 100):.1f}%")
            
            return str(decompressed_path)
        else:
            # Write uncompressed file as-is
            with open(local_path, 'wb') as f:
                f.write(response)
            
            logger.info(f"[DatasetDownload] Dataset saved to: {local_path}")
            logger.info(f"[DatasetDownload] File size: {len(response)} bytes")
            
            return str(local_path)
        
    except Exception as e:
        logger.error(f"[DatasetDownload] Failed to download dataset: {e}", exc_info=True)
        return None


@app.get("/health")
async def health_check():
    """Health check endpoint with GPU info"""
    logger.info("Health check requested")
    
    gpu_available = False
    gpu_info = "No GPU detected"
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_available = True
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
            gpu_info = f"{gpu_count}x {gpu_name}"
            logger.info(f"GPU detected: {gpu_info}")
    except Exception as e:
        logger.warning(f"GPU check failed: {e}")
    
    return JSONResponse(
        status_code=200,
        content={
            "status": "healthy",
            "service": "FineTune Lab Training API",
            "version": "1.0.0",
            "gpu_available": gpu_available,
            "gpu_info": gpu_info
        }
    )


@app.post("/api/training/execute")
async def execute_training(request: TrainingRequest):
    """Execute a training job with provided configuration"""
    logger.info(f"[Execute] Training request received")
    logger.info(f"[Execute] Execution ID: {request.execution_id}")
    logger.info(f"[Execute] Name: {request.name}")
    logger.info(f"[Execute] Dataset path: {request.dataset_path}")
    
    job_id = str(uuid.uuid4())
    logger.info(f"[Execute] Generated job_id: {job_id}")
    
    # Initialize dataset_list for later use
    dataset_list = None
    
    try:
        logger.info(f"[Execute] Validating configuration...")
        is_valid, warnings = validate_config(request.config)
        
        for warning in warnings:
            logger.warning(f"[Execute] Config warning: {warning}")
        
        logger.info(f"[Execute] Config validation passed with {len(warnings)} warning(s)")
        
        if request.dataset_content:
            logger.info(f"[Execute] Parsing dataset...")
            data_strategy = request.config.get("data", {}).get("strategy", "standard")
            
            # Parse dataset content - datasets are now pre-normalized at upload time
            # Support JSONL format (one JSON object per line)
            try:
                # Try JSONL format first (most common for normalized datasets)
                try:
                    dataset_lines = [line.strip() for line in request.dataset_content.strip().split('\n') if line.strip()]
                    dataset_list = [json.loads(line) for line in dataset_lines]
                    logger.info(f"[Execute] Parsed JSONL format with {len(dataset_list)} examples")
                except (json.JSONDecodeError, ValueError):
                    # Fallback: try single JSON object or array
                    parsed = json.loads(request.dataset_content)
                    if isinstance(parsed, dict) and 'examples' in parsed:
                        # Legacy format: {"metadata": {...}, "examples": [...]}
                        dataset_list = parsed['examples']
                        logger.info(f"[Execute] Parsed legacy format with {len(dataset_list)} examples")
                    elif isinstance(parsed, list):
                        # Direct array
                        dataset_list = parsed
                        logger.info(f"[Execute] Parsed JSON array with {len(dataset_list)} examples")
                    else:
                        raise ValidationError(f"Unexpected JSON structure: {type(parsed)}")
                
                logger.info(f"[Execute] Dataset parsed successfully, validating format...")
                    
            except json.JSONDecodeError as e:
                logger.error(f"[Execute] Failed to parse dataset JSON: {e}")
                raise ValidationError(f"Invalid dataset format: {str(e)}")
            except Exception as e:
                logger.error(f"[Execute] Failed to parse dataset: {e}")
                raise ValidationError(f"Dataset parsing error: {str(e)}")
            
            is_valid, dataset_warnings = validate_dataset_format(
                dataset_list, 
                data_strategy
            )
            
            for warning in dataset_warnings:
                logger.warning(f"[Execute] Dataset warning: {warning}")
            
            logger.info(f"[Execute] Dataset validation passed with {len(dataset_warnings)} warning(s)")
        
    except ValidationError as e:
        logger.error(f"[Execute] Validation failed: {e}")
        raise HTTPException(status_code=400, detail=f"Configuration validation failed: {str(e)}")
    except Exception as e:
        logger.error(f"[Execute] Unexpected validation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Validation error: {str(e)}")
    
    # Handle dataset - save content to local file if provided
    local_dataset_path = ""
    if request.dataset_content:
        # Dataset content provided directly (from LocalPackageDownloader)
        logger.info(f"[Execute] Saving dataset to local file...")
        
        # Create datasets directory
        datasets_dir = LOGS_DIR / "datasets" / f"job_{job_id}"
        datasets_dir.mkdir(parents=True, exist_ok=True)
        
        # Convert dataset_list to JSONL format (one JSON per line)
        # This ensures the training script can read it correctly
        dataset_file = datasets_dir / "train_dataset.jsonl"
        with open(dataset_file, 'w', encoding='utf-8') as f:
            for example in dataset_list:
                f.write(json.dumps(example) + '\n')
        
        local_dataset_path = str(dataset_file)
        logger.info(f"[Execute] Dataset saved to: {local_dataset_path}")
        logger.info(f"[Execute] Saved {len(dataset_list)} examples in JSONL format")
    elif request.dataset_path:
        # Legacy: dataset_path provided (for backward compatibility)
        logger.warning(f"[Execute] dataset_path provided but no dataset_content - this may fail")
        local_dataset_path = request.dataset_path
    
    training_config = request.config.copy()
    training_config["dataset_path"] = local_dataset_path
    training_config["output_dir"] = str(LOGS_DIR / f"job_{job_id}")
    training_config["execution_id"] = request.execution_id
    
    # Apply default optimizations for all training jobs
    if "training" not in training_config:
        training_config["training"] = {}
    
    # Set pretokenize default (can be overridden by user config)
    if "pretokenize" not in training_config["training"]:
        training_config["training"]["pretokenize"] = True  # Enabled by default for 100x speedup
        logger.info("[Execute] Set default pretokenize=True (100x speedup enabled)")
    
    # Set dataloader_num_workers default for Windows performance
    if "dataloader_num_workers" not in training_config["training"]:
        training_config["training"]["dataloader_num_workers"] = 0
        logger.info("[Execute] Set default dataloader_num_workers=0 (optimal for Windows)")
    
    # Set dataloader_prefetch_factor based on workers
    if training_config["training"].get("dataloader_num_workers", 0) == 0:
        training_config["training"]["dataloader_prefetch_factor"] = None
        logger.info("[Execute] Set dataloader_prefetch_factor=None (workers=0)")
    
    if "eval_split" not in training_config:
        training_config["eval_split"] = 0.2
    
    logger.info(f"[Execute] Model: {training_config.get('model', {}).get('name', 'unknown')}")
    logger.info(f"[Execute] Method: {training_config.get('training', {}).get('method', 'unknown')}")
    logger.info(f"[Execute] Epochs: {training_config.get('training', {}).get('num_epochs', 'unknown')}")
    logger.info(f"[Execute] ===== CRITICAL DATALOADER CONFIG =====")
    logger.info(f"[Execute] pretokenize: {training_config.get('training', {}).get('pretokenize', 'NOT SET')}")
    logger.info(f"[Execute] batch_size: {training_config.get('training', {}).get('batch_size', 'NOT SET')}")
    logger.info(f"[Execute] gradient_accumulation_steps: {training_config.get('training', {}).get('gradient_accumulation_steps', 'NOT SET')}")
    logger.info(f"[Execute] dataloader_num_workers: {training_config.get('training', {}).get('dataloader_num_workers', 'NOT SET')}")
    logger.info(f"[Execute] dataloader_prefetch_factor: {training_config.get('training', {}).get('dataloader_prefetch_factor', 'NOT SET')}")
    logger.info(f"[Execute] dataloader_pin_memory: {training_config.get('training', {}).get('dataloader_pin_memory', 'NOT SET')}")
    logger.info(f"[Execute] group_by_length: {training_config.get('training', {}).get('group_by_length', 'NOT SET')}")
    logger.info(f"[Execute] optim: {training_config.get('training', {}).get('optim', 'NOT SET')}")
    logger.info(f"[Execute] ==========================================")
    
    # Extract critical fields for persistence
    model_name = training_config.get("model", {}).get("name", "unknown")
    dataset_path = local_dataset_path or request.dataset_path or ""

    # Store user_id for database persistence
    user_id = request.user_id
    logger.info(f"[Execute] User ID: {user_id}")

    job = JobStatus(
        job_id=job_id,
        user_id=user_id,  # Store for database persistence
        status=JobStatusEnum.QUEUED,  # Changed from PENDING - job goes into queue first
        config_id=request.execution_id,
        dataset_id=request.dataset_path,
        name=request.name or f"Training Job {job_id[:8]}",
        model_name=model_name,  # Store for persistence
        dataset_path=dataset_path,  # Store for persistence
        started_at=datetime.utcnow().isoformat(),
        training_config=training_config  # Store config for deferred start
    )
    
    jobs[job_id] = job
    logger.info(f"[Execute] Job {job_id} created with status QUEUED. Total jobs: {len(jobs)}")

    # Persist job creation to database
    logger.info(f"[Persistence] training_config keys: {list(training_config.keys())}")
    logger.info(f"[Persistence] model section: {training_config.get('model')}")

    model_name = training_config.get("model", {}).get("name", "unknown")
    dataset_path = training_config.get("dataset_path")
    total_epochs = training_config.get("training", {}).get("num_epochs")

    logger.info(f"[Persistence] Extracted - model_name: {model_name}, dataset_path: {dataset_path}, total_epochs: {total_epochs}")

    persist_job(job_id, {
        "job_id": job_id,
        "user_id": user_id,  # Required for service role key authentication
        "model_name": model_name,
        "dataset_path": dataset_path,
        "status": "queued",  # Changed from "pending"
        "config": training_config,
        "total_epochs": total_epochs,
        "started_at": job.started_at + "Z" if not job.started_at.endswith("Z") else job.started_at
    })
    logger.info(f"[Execute] Job {job_id} persistence initiated")

    # Add job to queue instead of starting immediately
    await job_queue.put(job_id)
    queue_position = job_queue.qsize()
    job.queue_position = queue_position
    logger.info(f"[Execute] Job {job_id} added to queue at position {queue_position}")
    logger.info(f"[Execute] Queue worker will start job when GPU is available")
    
    return TrainingResponse(
        job_id=job_id,
        status=job.status.value,
        message=f"Training job {job_id} queued successfully (position: {queue_position})"
    )


@app.post("/api/training/validate")
async def validate_model(request: ValidationRequest):
    """Execute model validation on test dataset and compute metrics"""
    logger.info(f"[Validate] Validation request received")
    logger.info(f"[Validate] Model path: {request.model_path}")
    logger.info(f"[Validate] Test dataset: {request.test_dataset_id}")
    logger.info(f"[Validate] Metrics to compute: {request.metrics_to_compute}")
    
    validation_job_id = str(uuid.uuid4())
    logger.info(f"[Validate] Generated validation job_id: {validation_job_id}")
    
    try:
        # Execute validation
        logger.info(f"[Validate] Starting validation process...")
        metrics = await validate_model_on_dataset(
            request.model_path,
            request.test_dataset_id,
            request.metrics_to_compute
        )
        
        logger.info(f"[Validate] Validation completed successfully")
        logger.info(f"[Validate] Computed metrics: {metrics}")
        
        return ValidationResponse(
            job_id=validation_job_id,
            status="completed",
            metrics=metrics,
            message=f"Validation completed successfully with {len(metrics)} metrics"
        )
        
    except FileNotFoundError as e:
        logger.error(f"[Validate] File not found: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"[Validate] Validation failed: {e}", exc_info=True)
        return ValidationResponse(
            job_id=validation_job_id,
            status="failed",
            metrics={},
            message=f"Validation failed: {str(e)}"
        )


@app.get("/api/training/status/{job_id}")
async def get_training_status(job_id: str):
    """Get status of a training job with current metrics"""
    logger.info(f"Status check requested for job_id: {job_id}")
    
    if job_id not in jobs:
        logger.warning(f"Job {job_id} not found")
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    job = jobs[job_id]
    logger.debug(f"Returning status for job {job_id}: {job.status.value}")
    
    return job.to_dict()


@app.get("/api/training/metrics/{job_id}")
async def get_training_metrics(job_id: str):
    """
    Get full metrics history for a training job.
    Returns all historical data points for charting.
    """
    logger.info(f"Metrics history requested for job_id: {job_id}")
    
    if job_id not in jobs:
        logger.warning(f"Job {job_id} not found")
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    progress_file = LOGS_DIR / f"job_{job_id}" / "progress.json"
    
    if not progress_file.exists():
        logger.warning(f"Progress file not found for job {job_id}")
        return JSONResponse(
            status_code=200,
            content={
                "job_id": job_id,
                "metrics": [],
                "message": "No metrics available yet"
            }
        )
    
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress_data = json.load(f)
        
        metrics_history = progress_data.get("metrics_history", [])
        
        logger.info(f"Returning {len(metrics_history)} metric points for job {job_id}")
        
        return JSONResponse(
            status_code=200,
            content={
                "job_id": job_id,
                "metrics": metrics_history
            }
        )
        
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse progress file for job {job_id}: {e}")
        raise HTTPException(status_code=500, detail="Failed to read metrics: corrupted data")
    
    except Exception as e:
        logger.error(f"Error reading metrics for job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to read metrics: {str(e)}")


@app.get("/api/training/logs/{job_id}")
async def get_training_logs(job_id: str, limit: int = 100, offset: int = 0):
    """
    Get training logs for a job.

    Args:
        job_id: Job identifier
        limit: Max number of lines to return (default: 100)
        offset: Line offset to start from (default: 0)
    """
    logger.info(f"Logs requested for job_id: {job_id} (limit={limit}, offset={offset})")

    log_file = LOGS_DIR / f"job_{job_id}.log"

    if not log_file.exists():
        logger.warning(f"Log file not found for job {job_id}")
        return JSONResponse(
            status_code=200,
            content={
                "job_id": job_id,
                "logs": [],
                "total_lines": 0,
                "message": "No logs available yet"
            }
        )

    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()

        total_lines = len(all_lines)
        log_lines = all_lines[offset:offset + limit] if offset < total_lines else []

        logger.info(f"Returning {len(log_lines)}/{total_lines} log lines for job {job_id}")

        return JSONResponse(
            status_code=200,
            content={
                "job_id": job_id,
                "logs": [line.rstrip() for line in log_lines],
                "total_lines": total_lines,
                "offset": offset,
                "limit": limit
            }
        )

    except Exception as e:
        logger.error(f"Error reading logs for job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to read logs: {str(e)}")


@app.get("/api/training/queue")
async def get_queue_status():
    """
    Get current job queue status.
    Shows running jobs and queued jobs with their positions.
    
    Returns:
        JSON with queue statistics and job details
    """
    logger.debug("[Queue] Queue status requested")
    
    running_jobs = get_running_jobs()
    
    # Get queued jobs from the queue
    # Note: job_queue._queue is a deque, we can iterate it
    queued_job_ids = list(job_queue._queue)
    
    queued_jobs = []
    for i, jid in enumerate(queued_job_ids):
        if jid in jobs:
            queued_jobs.append({
                "job_id": jid,
                "name": jobs[jid].name,
                "position": i + 1,
                "status": jobs[jid].status.value
            })
    
    running_job_details = []
    for jid in running_jobs:
        if jid in jobs:
            job = jobs[jid]
            running_job_details.append({
                "job_id": jid,
                "name": job.name,
                "progress": job.progress,
                "current_epoch": job.current_epoch,
                "total_epochs": job.total_epochs
            })
    
    return {
        "running_count": len(running_jobs),
        "queued_count": len(queued_jobs),
        "running_jobs": running_job_details,
        "queued_jobs": queued_jobs,
        "queue_active": True
    }


@app.post("/api/training/cancel/{job_id}")
async def cancel_training_job(job_id: str):
    """
    Cancel a training job.
    
    Works for jobs in any state:
    - QUEUED: Removes from queue
    - PENDING/RUNNING: Terminates subprocess
    - COMPLETED/FAILED/CANCELLED: Returns already finished
    
    Args:
        job_id: Job ID to cancel
        
    Returns:
        JSON with cancellation result
    
    Phase 2 - Job Cancellation
    """
    logger.info(f"[API] Cancel request for job {job_id}")
    
    try:
        result = await cancel_job(job_id)
        
        if result["success"]:
            logger.info(f"[API] Job {job_id} cancelled successfully")
            return JSONResponse(
                status_code=200,
                content=result
            )
        else:
            logger.warning(f"[API] Failed to cancel job {job_id}: {result.get('message')}")
            return JSONResponse(
                status_code=400,
                content=result
            )
            
    except Exception as e:
        logger.error(f"[API] Error cancelling job {job_id}: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "job_id": job_id,
                "message": f"Internal error: {str(e)}"
            }
        )


@app.post("/api/training/pause/{job_id}")
async def pause_training_job(job_id: str):
    """
    Pause a running training job.
    Saves current state and terminates process gracefully.
    Job can be resumed later from the last checkpoint.
    
    Works for:
    - PENDING/RUNNING: Pauses job, saves checkpoint
    - Other statuses: Returns error
    
    Args:
        job_id: Job ID to pause
        
    Returns:
        JSON with pause result including paused_at timestamp
    
    Phase 2 - Pause/Resume
    """
    logger.info(f"[API] Pause request for job {job_id}")
    
    try:
        result = await pause_job(job_id)
        
        if result["success"]:
            logger.info(f"[API] Job {job_id} paused successfully")
            return JSONResponse(
                status_code=200,
                content=result
            )
        else:
            logger.warning(f"[API] Failed to pause job {job_id}: {result.get('message')}")
            return JSONResponse(
                status_code=400,
                content=result
            )
            
    except Exception as e:
        logger.error(f"[API] Error pausing job {job_id}: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "job_id": job_id,
                "message": f"Internal error: {str(e)}"
            }
        )


@app.post("/api/training/resume/{job_id}")
async def resume_training_job(job_id: str, checkpoint_path: Optional[str] = None):
    """
    Resume a paused training job from the last checkpoint.
    
    Works for:
    - PAUSED: Resumes job from checkpoint
    - Other statuses: Returns error
    
    Args:
        job_id: Job ID to resume
        checkpoint_path: Optional specific checkpoint to resume from.
                        If not provided, uses latest checkpoint.
        
    Returns:
        JSON with resume result including checkpoint info
    
    Phase 2 - Pause/Resume
    """
    logger.info(f"[API] Resume request for job {job_id}")
    
    try:
        result = await resume_job(job_id, checkpoint_path)
        
        if result["success"]:
            logger.info(f"[API] Job {job_id} resumed successfully from checkpoint")
            return JSONResponse(
                status_code=200,
                content=result
            )
        else:
            logger.warning(f"[API] Failed to resume job {job_id}: {result.get('message')}")
            return JSONResponse(
                status_code=400,
                content=result
            )
            
    except Exception as e:
        logger.error(f"[API] Error resuming job {job_id}: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "job_id": job_id,
                "message": f"Internal error: {str(e)}"
            }
        )


@app.websocket("/ws/training/{job_id}")
async def training_websocket(websocket: WebSocket, job_id: str):
    """
    WebSocket endpoint for real-time training metrics streaming.
    
    Streams training progress, metrics, and status updates in real-time.
    Multiple clients can connect to the same job.
    
    Protocol:
        - Client connects to ws://host:port/ws/training/{job_id}
        - Server sends JSON messages with current metrics every second
        - Connection closes when job completes or fails
    
    Message format:
        {
            "job_id": str,
            "status": str,
            "progress": float,
            "current_epoch": int,
            "total_epochs": int,
            "current_step": int,
            "total_steps": int,
            "loss": float,
            "eval_loss": float,
            "samples_per_second": float,
            "gpu_memory_allocated_gb": float,
            "gpu_utilization_percent": float,
            "eta_seconds": int,
            "timestamp": str (ISO format),
            "complete": bool (only on final message)
        }
    
    Args:
        websocket: WebSocket connection
        job_id: Job identifier
    
    Phase 3 - WebSocket Streaming
    """
    # Validate job exists
    if job_id not in jobs:
        await websocket.close(code=1008, reason=f"Job {job_id} not found")
        logger.warning(f"[WebSocket] Rejected connection - job {job_id} not found")
        return
    
    # Accept connection
    await ws_manager.connect(websocket, job_id)
    
    job = jobs[job_id]
    
    try:
        # Send initial status
        initial_message = {
            "job_id": job_id,
            "status": job.status.value if isinstance(job.status, Enum) else job.status,
            "message": f"Connected to job {job_id}",
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        await websocket.send_json(initial_message)
        
        # Stream metrics while job is active
        while job.status in [JobStatusEnum.QUEUED, JobStatusEnum.PENDING, JobStatusEnum.RUNNING]:
            # Build metrics message
            metrics = {
                "job_id": job_id,
                "status": job.status.value if isinstance(job.status, Enum) else job.status,
                "progress": job.progress,
                "current_epoch": job.current_epoch,
                "total_epochs": job.total_epochs,
                "current_step": job.current_step,
                "total_steps": job.total_steps,
                "loss": job.loss,
                "eval_loss": job.eval_loss,
                "samples_per_second": job.samples_per_second,
                "gpu_memory_allocated_gb": job.gpu_memory_allocated_gb,
                "gpu_utilization_percent": job.gpu_utilization_percent,
                "eta_seconds": job.eta_seconds,
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "complete": False
            }
            
            await websocket.send_json(metrics)
            
            # Wait before next update (1 second for real-time feel)
            await asyncio.sleep(1)
            
            # Re-check job reference (could have been updated)
            if job_id in jobs:
                job = jobs[job_id]
            else:
                logger.warning(f"[WebSocket] Job {job_id} disappeared from job store")
                break
        
        # Send final completion message
        final_status = job.status.value if isinstance(job.status, Enum) else job.status
        completion_message = {
            "job_id": job_id,
            "status": final_status,
            "progress": 100.0 if job.status == JobStatusEnum.COMPLETED else job.progress,
            "final_loss": job.loss,
            "eval_loss": job.eval_loss,
            "complete": True,
            "completed_at": job.completed_at,
            "error": job.error_message if job.status == JobStatusEnum.FAILED else None,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        
        await websocket.send_json(completion_message)
        logger.info(f"[WebSocket] Job {job_id} completed, status: {final_status}")
        
    except WebSocketDisconnect:
        logger.info(f"[WebSocket] Client disconnected from job {job_id}")
    
    except Exception as e:
        logger.error(f"[WebSocket] Error streaming job {job_id}: {e}", exc_info=True)
        try:
            error_message = {
                "job_id": job_id,
                "error": f"WebSocket error: {str(e)}",
                "complete": True,
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            await websocket.send_json(error_message)
        except:
            pass  # Connection might be dead
    
    finally:
        # Always disconnect
        await ws_manager.disconnect(websocket, job_id)


@app.get("/api/training/checkpoints/{job_id}")
async def list_checkpoints(job_id: str):
    """
    List all checkpoint directories for a training job.
    Returns checkpoint metadata including eval loss, train loss, and file size.

    Args:
        job_id: Job identifier

    Returns:
        JSON response with list of checkpoints and metadata
    """
    logger.info(f"Checkpoint list requested for job_id: {job_id}")

    # Get output directory for this job (scan filesystem regardless of in-memory state)
    output_dir = LOGS_DIR / f"job_{job_id}"

    if not output_dir.exists():
        logger.warning(f"Output directory not found for job {job_id}")
        return JSONResponse(
            status_code=200,
            content={
                "job_id": job_id,
                "checkpoints": [],
                "message": "No checkpoints available yet"
            }
        )

    try:
        checkpoints = []
        best_eval_loss = float('inf')
        best_checkpoint_path = None
        latest_checkpoint_path = None
        latest_mtime = 0

        # Scan for checkpoint directories
        # Supports two formats:
        # 1. checkpoint-{step} (e.g., checkpoint-1000)
        # 2. checkpoint-epoch-{epoch}-step-{step} (e.g., checkpoint-epoch-2-step-1000)
        for item in output_dir.iterdir():
            if item.is_dir() and item.name.startswith('checkpoint-'):
                checkpoint_path = str(item.name)

                # Parse epoch and step from checkpoint name
                parts = checkpoint_path.split('-')
                epoch = None
                step = None

                try:
                    if len(parts) == 2:
                        # Format: checkpoint-{step} or checkpoint-{step}_merged
                        step_part = parts[1].replace('_merged', '')
                        step = int(step_part)
                    elif len(parts) == 5 and parts[1] == 'epoch':
                        # Format: checkpoint-epoch-{epoch}-step-{step} or checkpoint-epoch-{epoch}-step-{step}_merged
                        epoch = int(parts[2])
                        step_part = parts[4].replace('_merged', '')
                        step = int(step_part)
                    else:
                        logger.warning(f"Unexpected checkpoint format: {checkpoint_path}")
                        continue
                except (IndexError, ValueError) as e:
                    logger.warning(f"Could not parse checkpoint name: {checkpoint_path} - {e}")
                    continue

                # Get checkpoint metadata from trainer_state.json if available
                trainer_state_file = item / "trainer_state.json"
                eval_loss = None
                train_loss = None

                if trainer_state_file.exists():
                    try:
                        with open(trainer_state_file, 'r', encoding='utf-8') as f:
                            trainer_state = json.load(f)

                        # If epoch not in name, get it from trainer_state
                        if epoch is None:
                            epoch = trainer_state.get('epoch')

                        # Extract metrics from log history
                        log_history = trainer_state.get('log_history', [])
                        if log_history:
                            # Find the eval entry matching this checkpoint's step
                            for log_entry in reversed(log_history):
                                log_step = log_entry.get('step')
                                if log_step == step:
                                    eval_loss = log_entry.get('eval_loss')
                                    train_loss = log_entry.get('loss')
                                    if eval_loss is not None:
                                        break
                    except Exception as e:
                        logger.warning(f"Could not read trainer_state.json for {checkpoint_path}: {e}")

                # Calculate checkpoint size
                checkpoint_size = 0
                try:
                    for file_path in item.rglob('*'):
                        if file_path.is_file():
                            checkpoint_size += file_path.stat().st_size
                except Exception as e:
                    logger.warning(f"Could not calculate size for {checkpoint_path}: {e}")

                # Get creation time
                created_at = datetime.fromtimestamp(item.stat().st_ctime).isoformat()
                mtime = item.stat().st_mtime

                # Track best checkpoint (lowest eval_loss)
                if eval_loss is not None and eval_loss < best_eval_loss:
                    best_eval_loss = eval_loss
                    best_checkpoint_path = checkpoint_path

                # Track latest checkpoint (most recent mtime)
                if mtime > latest_mtime:
                    latest_mtime = mtime
                    latest_checkpoint_path = checkpoint_path

                checkpoints.append({
                    "path": checkpoint_path,
                    "epoch": epoch,
                    "step": step,
                    "eval_loss": eval_loss,
                    "train_loss": train_loss,
                    "size_bytes": checkpoint_size,
                    "created_at": created_at,
                    "is_best": False,  # Will be set after loop
                    "is_latest": False  # Will be set after loop
                })

        # Mark best and latest checkpoints
        for checkpoint in checkpoints:
            if checkpoint["path"] == best_checkpoint_path:
                checkpoint["is_best"] = True
            if checkpoint["path"] == latest_checkpoint_path:
                checkpoint["is_latest"] = True

        # Sort by step (descending) so latest is first
        checkpoints.sort(key=lambda x: x["step"], reverse=True)

        logger.info(f"Found {len(checkpoints)} checkpoints for job {job_id}")
        logger.info(f"Best checkpoint: {best_checkpoint_path} (eval_loss={best_eval_loss:.6f})" if best_checkpoint_path else "No best checkpoint")

        return JSONResponse(
            status_code=200,
            content={
                "job_id": job_id,
                "checkpoints": checkpoints,
                "best_checkpoint": best_checkpoint_path
            }
        )

    except Exception as e:
        logger.error(f"Error listing checkpoints for job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to list checkpoints: {str(e)}")


@app.get("/api/training/{job_id}/download/model")
async def download_model(job_id: str, checkpoint: Optional[str] = None):
    """
    Download trained model or specific checkpoint as ZIP file.
    
    Args:
        job_id: Job identifier
        checkpoint: Optional checkpoint directory name (e.g., "checkpoint-1000")
                   If not provided, downloads the entire output directory
    
    Returns:
        StreamingResponse with ZIP file containing model files
    
    Phase 4 - Model Download
    """
    logger.info(f"[Download] Model download requested for job {job_id}, checkpoint={checkpoint}")
    
    # Check if job exists
    if job_id not in jobs:
        logger.warning(f"[Download] Job {job_id} not found")
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    job = jobs[job_id]
    
    # Get output directory
    output_dir = LOGS_DIR / f"job_{job_id}"
    
    if not output_dir.exists():
        logger.warning(f"[Download] Output directory not found for job {job_id}")
        raise HTTPException(status_code=404, detail="Model files not found")
    
    # Determine what to ZIP
    if checkpoint:
        # Validate checkpoint name (prevent path traversal)
        if ".." in checkpoint or "/" in checkpoint or "\\" in checkpoint:
            logger.warning(f"[Download] Invalid checkpoint name: {checkpoint}")
            raise HTTPException(status_code=400, detail="Invalid checkpoint name")
        
        model_dir = output_dir / checkpoint
        if not model_dir.exists():
            logger.warning(f"[Download] Checkpoint {checkpoint} not found for job {job_id}")
            raise HTTPException(status_code=404, detail=f"Checkpoint '{checkpoint}' not found")
        
        zip_filename = f"job_{job_id}_{checkpoint}.zip"
        logger.info(f"[Download] Preparing checkpoint download: {checkpoint}")
    else:
        model_dir = output_dir
        zip_filename = f"job_{job_id}_model.zip"
        logger.info(f"[Download] Preparing full model download")
    
    try:
        # Create ZIP in memory
        zip_buffer = BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            file_count = 0
            total_size = 0
            
            for file_path in model_dir.rglob('*'):
                if file_path.is_file():
                    # Calculate relative path for ZIP archive
                    arcname = file_path.relative_to(model_dir)
                    
                    # Add file to ZIP
                    zip_file.write(file_path, arcname)
                    
                    file_count += 1
                    total_size += file_path.stat().st_size
            
            logger.info(f"[Download] Created ZIP with {file_count} files, total size: {total_size / 1024 / 1024:.2f} MB")
        
        # Reset buffer position to beginning
        zip_buffer.seek(0)
        
        logger.info(f"[Download] Streaming ZIP file: {zip_filename}")
        
        # Return as streaming response
        return StreamingResponse(
            zip_buffer,
            media_type="application/zip",
            headers={
                "Content-Disposition": f"attachment; filename={zip_filename}"
            }
        )
    
    except Exception as e:
        logger.error(f"[Download] Error creating ZIP for job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to create download: {str(e)}")


@app.get("/api/training/{job_id}/download/logs")
async def download_logs(job_id: str):
    """
    Download training logs as ZIP file.
    
    Includes:
    - training.log: Full training logs
    - progress.json: Progress metrics (if available)
    
    Args:
        job_id: Job identifier
    
    Returns:
        StreamingResponse with ZIP file containing log files
    
    Phase 4 - Logs Download
    """
    logger.info(f"[Download] Logs download requested for job {job_id}")
    
    # Check if job exists
    if job_id not in jobs:
        logger.warning(f"[Download] Job {job_id} not found")
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    # Locate log files
    log_file = LOGS_DIR / f"job_{job_id}.log"
    progress_file = LOGS_DIR / f"job_{job_id}" / "progress.json"
    
    # Check if at least the log file exists
    if not log_file.exists():
        logger.warning(f"[Download] Log file not found for job {job_id}")
        raise HTTPException(status_code=404, detail="Log files not found")
    
    try:
        # Create ZIP in memory
        zip_buffer = BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # Add training log
            zip_file.write(log_file, "training.log")
            logger.info(f"[Download] Added training.log ({log_file.stat().st_size} bytes)")
            
            # Add progress.json if it exists
            if progress_file.exists():
                zip_file.write(progress_file, "progress.json")
                logger.info(f"[Download] Added progress.json ({progress_file.stat().st_size} bytes)")
        
        # Reset buffer position
        zip_buffer.seek(0)
        
        zip_filename = f"job_{job_id}_logs.zip"
        logger.info(f"[Download] Streaming logs ZIP file: {zip_filename}")
        
        # Return as streaming response
        return StreamingResponse(
            zip_buffer,
            media_type="application/zip",
            headers={
                "Content-Disposition": f"attachment; filename={zip_filename}"
            }
        )
    
    except Exception as e:
        logger.error(f"[Download] Error creating logs ZIP for job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to create logs download: {str(e)}")


# =============================================================================
# Phase 5: Enhanced Monitoring & Analytics
# =============================================================================

def calculate_job_analytics(job_id: str) -> Dict[str, Any]:
    """
    Calculate comprehensive analytics for a training job.
    
    Analyzes job performance, resource utilization, and efficiency metrics
    based on progress data and job metadata.
    
    Args:
        job_id: Job identifier
    
    Returns:
        Dictionary containing detailed analytics
    
    Raises:
        ValueError: If job not found or progress data unavailable
    """
    # Check if job exists
    if job_id not in jobs:
        raise ValueError(f"Job {job_id} not found")
    
    job = jobs[job_id]
    
    # Load progress data
    progress_file = LOGS_DIR / f"job_{job_id}" / "progress.json"
    if not progress_file.exists():
        raise ValueError(f"Progress data not found for job {job_id}")
    
    try:
        with open(progress_file, 'r') as f:
            progress_data = json.load(f)
    except Exception as e:
        raise ValueError(f"Failed to load progress data: {e}")
    
    # Calculate duration metrics
    start_time = None
    end_time = None
    if hasattr(job, 'started_at') and job.started_at:
        start_time = datetime.fromisoformat(job.started_at) if isinstance(job.started_at, str) else job.started_at
    if hasattr(job, 'completed_at') and job.completed_at:
        end_time = datetime.fromisoformat(job.completed_at) if isinstance(job.completed_at, str) else job.completed_at
    elif job.status == JobStatusEnum.RUNNING:
        end_time = datetime.now()
    
    total_seconds = 0
    if start_time and end_time:
        total_seconds = (end_time - start_time).total_seconds()
    
    # Extract metrics from progress data
    samples_per_sec_history = []
    loss_history = []
    gpu_util_history = []
    gpu_memory_history = []
    gpu_temp_history = []
    iteration_times = []
    
    if isinstance(progress_data, dict):
        samples_per_sec_history = [progress_data.get('samples_per_second', 0)]
        loss_history = [progress_data.get('loss', 0)]
        gpu_util_history = [progress_data.get('gpu_utilization', 0)]
        gpu_memory_history = [progress_data.get('gpu_memory_used', 0)]
        gpu_temp_history = [progress_data.get('gpu_temperature', 0)]
    elif isinstance(progress_data, list):
        for entry in progress_data:
            if isinstance(entry, dict):
                samples_per_sec_history.append(entry.get('samples_per_second', 0))
                loss_history.append(entry.get('loss', 0))
                gpu_util_history.append(entry.get('gpu_utilization', 0))
                gpu_memory_history.append(entry.get('gpu_memory_used', 0))
                gpu_temp_history.append(entry.get('gpu_temperature', 0))
    
    # Filter out zero/invalid values
    samples_per_sec_history = [x for x in samples_per_sec_history if x > 0]
    loss_history = [x for x in loss_history if x > 0]
    gpu_util_history = [x for x in gpu_util_history if x > 0]
    gpu_memory_history = [x for x in gpu_memory_history if x > 0]
    gpu_temp_history = [x for x in gpu_temp_history if x > 0]
    
    # Calculate performance metrics
    avg_samples_per_sec = sum(samples_per_sec_history) / len(samples_per_sec_history) if samples_per_sec_history else 0
    peak_samples_per_sec = max(samples_per_sec_history) if samples_per_sec_history else 0
    avg_iteration_time = (1.0 / avg_samples_per_sec) if avg_samples_per_sec > 0 else 0
    
    # Calculate resource utilization
    avg_gpu_util = sum(gpu_util_history) / len(gpu_util_history) if gpu_util_history else 0
    peak_gpu_memory = max(gpu_memory_history) if gpu_memory_history else 0
    avg_gpu_memory = sum(gpu_memory_history) / len(gpu_memory_history) if gpu_memory_history else 0
    avg_gpu_temp = sum(gpu_temp_history) / len(gpu_temp_history) if gpu_temp_history else 0
    peak_gpu_temp = max(gpu_temp_history) if gpu_temp_history else 0
    
    # Calculate loss metrics
    initial_loss = loss_history[0] if loss_history else 0
    final_loss = loss_history[-1] if loss_history else 0
    best_loss = min(loss_history) if loss_history else 0
    loss_reduction_percent = ((initial_loss - final_loss) / initial_loss * 100) if initial_loss > 0 else 0
    
    # Find checkpoints
    output_dir = Path(job.config.get('output_dir', f'./outputs/job_{job_id}')) if hasattr(job, 'config') else Path(f'./outputs/job_{job_id}')
    checkpoints = []
    if output_dir.exists():
        checkpoints = [d.name for d in output_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint-')]
    
    # Calculate efficiency scores
    gpu_util_score = avg_gpu_util  # Already in percentage
    throughput_score = min((avg_samples_per_sec / 5.0) * 100, 100)  # Assuming 5 samples/sec is optimal
    overall_score = (gpu_util_score + throughput_score) / 2
    
    return {
        "job_id": job_id,
        "status": job.status,
        "duration": {
            "total_seconds": round(total_seconds, 2),
            "total_hours": round(total_seconds / 3600, 2),
            "total_minutes": round(total_seconds / 60, 2)
        },
        "performance": {
            "average_samples_per_second": round(avg_samples_per_sec, 2),
            "peak_samples_per_second": round(peak_samples_per_sec, 2),
            "average_iteration_time_seconds": round(avg_iteration_time, 2),
            "total_samples": len(samples_per_sec_history) * int(avg_samples_per_sec) if avg_samples_per_sec > 0 else 0
        },
        "resource_utilization": {
            "average_gpu_utilization_percent": round(avg_gpu_util, 2),
            "peak_gpu_memory_mb": round(peak_gpu_memory, 2),
            "average_gpu_memory_mb": round(avg_gpu_memory, 2),
            "average_gpu_temp_celsius": round(avg_gpu_temp, 2),
            "peak_gpu_temp_celsius": round(peak_gpu_temp, 2)
        },
        "checkpoints": {
            "total_checkpoints": len(checkpoints),
            "checkpoint_names": sorted(checkpoints)
        },
        "losses": {
            "initial_loss": round(initial_loss, 4),
            "final_loss": round(final_loss, 4),
            "best_loss": round(best_loss, 4),
            "loss_reduction_percent": round(loss_reduction_percent, 2)
        },
        "efficiency": {
            "gpu_utilization_score": round(gpu_util_score, 2),
            "throughput_score": round(throughput_score, 2),
            "overall_score": round(overall_score, 2)
        }
    }


def calculate_system_analytics() -> Dict[str, Any]:
    """
    Calculate system-wide analytics across all training jobs.
    
    Returns:
        Dictionary containing aggregated analytics for all jobs
    """
    # Count jobs by status
    jobs_by_status = {
        "queued": 0,
        "pending": 0,
        "running": 0,
        "completed": 0,
        "failed": 0,
        "cancelled": 0,
        "paused": 0
    }
    
    total_training_seconds = 0
    throughput_values = []
    gpu_util_values = []
    gpu_memory_values = []
    completed_jobs_data = []
    
    for job_id, job in jobs.items():
        # Count by status
        status_key = job.status.lower() if hasattr(job, 'status') else "unknown"
        if status_key in jobs_by_status:
            jobs_by_status[status_key] += 1
        
        # Try to get analytics for completed jobs
        if job.status == JobStatusEnum.COMPLETED:
            try:
                analytics = calculate_job_analytics(job_id)
                
                # Aggregate duration
                total_training_seconds += analytics['duration']['total_seconds']
                
                # Aggregate performance
                if analytics['performance']['average_samples_per_second'] > 0:
                    throughput_values.append(analytics['performance']['average_samples_per_second'])
                
                # Aggregate resource usage
                if analytics['resource_utilization']['average_gpu_utilization_percent'] > 0:
                    gpu_util_values.append(analytics['resource_utilization']['average_gpu_utilization_percent'])
                
                if analytics['resource_utilization']['peak_gpu_memory_mb'] > 0:
                    gpu_memory_values.append(analytics['resource_utilization']['peak_gpu_memory_mb'])
                
                # Store for top performers
                completed_jobs_data.append({
                    "job_id": job_id,
                    "throughput": analytics['performance']['average_samples_per_second'],
                    "duration": analytics['duration']['total_seconds'],
                    "gpu_utilization": analytics['resource_utilization']['average_gpu_utilization_percent'],
                    "final_loss": analytics['losses']['final_loss']
                })
            except Exception as e:
                logger.warning(f"[Analytics] Failed to get analytics for job {job_id}: {e}")
                continue
    
    # Calculate averages
    avg_training_duration = total_training_seconds / jobs_by_status["completed"] if jobs_by_status["completed"] > 0 else 0
    avg_throughput = sum(throughput_values) / len(throughput_values) if throughput_values else 0
    avg_gpu_util = sum(gpu_util_values) / len(gpu_util_values) if gpu_util_values else 0
    avg_peak_memory = sum(gpu_memory_values) / len(gpu_memory_values) if gpu_memory_values else 0
    
    # Find top performing jobs (by throughput)
    top_performers = sorted(completed_jobs_data, key=lambda x: x['throughput'], reverse=True)[:5]
    
    return {
        "total_jobs": len(jobs),
        "jobs_by_status": jobs_by_status,
        "average_training_duration_seconds": round(avg_training_duration, 2),
        "total_training_time_hours": round(total_training_seconds / 3600, 2),
        "average_throughput_samples_per_sec": round(avg_throughput, 2),
        "top_performing_jobs": top_performers,
        "resource_trends": {
            "average_gpu_utilization": round(avg_gpu_util, 2),
            "average_peak_memory_mb": round(avg_peak_memory, 2)
        }
    }


def compare_jobs(job_ids: List[str]) -> Dict[str, Any]:
    """
    Compare multiple training jobs side-by-side.
    
    Args:
        job_ids: List of job IDs to compare
    
    Returns:
        Dictionary containing comparison data and winner analysis
    """
    jobs_data = []
    
    for job_id in job_ids:
        try:
            analytics = calculate_job_analytics(job_id)
            jobs_data.append({
                "job_id": job_id,
                "status": analytics['status'],
                "duration_seconds": analytics['duration']['total_seconds'],
                "throughput": analytics['performance']['average_samples_per_second'],
                "final_loss": analytics['losses']['final_loss'],
                "gpu_utilization": analytics['resource_utilization']['average_gpu_utilization_percent'],
                "efficiency_score": analytics['efficiency']['overall_score']
            })
        except Exception as e:
            logger.warning(f"[Analytics] Failed to get analytics for job {job_id}: {e}")
            jobs_data.append({
                "job_id": job_id,
                "error": str(e)
            })
    
    # Determine winners
    valid_jobs = [j for j in jobs_data if 'error' not in j]
    
    winner = {}
    if valid_jobs:
        winner['best_throughput'] = max(valid_jobs, key=lambda x: x['throughput'])['job_id']
        winner['best_loss'] = min(valid_jobs, key=lambda x: x['final_loss'])['job_id']
        winner['fastest'] = min(valid_jobs, key=lambda x: x['duration_seconds'])['job_id']
        winner['most_efficient'] = max(valid_jobs, key=lambda x: x['efficiency_score'])['job_id']
    
    return {
        "jobs": jobs_data,
        "winner": winner
    }


# =============================================================================
# Phase 5: Analytics API Endpoints
# =============================================================================

@app.get("/api/training/{job_id}/analytics")
async def get_job_analytics(job_id: str):
    """
    Get comprehensive analytics for a specific training job.
    
    Returns detailed metrics including:
    - Duration and timing
    - Performance metrics (throughput, iteration time)
    - Resource utilization (GPU, memory, temperature)
    - Checkpoints information
    - Loss progression
    - Efficiency scores
    
    Args:
        job_id: Job identifier
    
    Returns:
        JSON response with analytics data
    
    Raises:
        HTTPException: 404 if job not found, 500 for calculation errors
    
    Phase 5 - Job Analytics
    """
    logger.info(f"[Analytics] Analytics requested for job {job_id}")
    
    try:
        analytics = calculate_job_analytics(job_id)
        logger.info(f"[Analytics] Successfully calculated analytics for job {job_id}")
        return JSONResponse(content=analytics)
    
    except ValueError as e:
        logger.warning(f"[Analytics] Job {job_id} not found or invalid: {e}")
        raise HTTPException(status_code=404, detail=str(e))
    
    except Exception as e:
        logger.error(f"[Analytics] Error calculating analytics for job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to calculate analytics: {str(e)}")


@app.get("/api/training/analytics/summary")
async def get_system_analytics():
    """
    Get system-wide analytics across all training jobs.
    
    Returns aggregated metrics including:
    - Total job counts by status
    - Average training duration
    - Total training time
    - Average throughput
    - Top performing jobs
    - Resource utilization trends
    
    Returns:
        JSON response with system analytics
    
    Raises:
        HTTPException: 500 for calculation errors
    
    Phase 5 - System Analytics
    """
    logger.info("[Analytics] System analytics requested")
    
    try:
        analytics = calculate_system_analytics()
        logger.info(f"[Analytics] Successfully calculated system analytics ({analytics['total_jobs']} jobs)")
        return JSONResponse(content=analytics)
    
    except Exception as e:
        logger.error(f"[Analytics] Error calculating system analytics: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to calculate system analytics: {str(e)}")


@app.get("/api/training/analytics/compare")
async def compare_training_jobs(job_ids: str):
    """
    Compare multiple training jobs side-by-side.
    
    Analyzes and compares jobs across key metrics:
    - Duration
    - Throughput
    - Final loss
    - GPU utilization
    - Efficiency score
    
    Also determines "winners" in each category.
    
    Args:
        job_ids: Comma-separated list of job IDs (e.g., "job1,job2,job3")
    
    Returns:
        JSON response with comparison data and winner analysis
    
    Raises:
        HTTPException: 400 for invalid input, 500 for calculation errors
    
    Phase 5 - Job Comparison
    """
    logger.info(f"[Analytics] Job comparison requested for: {job_ids}")
    
    # Parse job IDs
    try:
        job_id_list = [jid.strip() for jid in job_ids.split(',') if jid.strip()]
        
        if not job_id_list:
            raise ValueError("No job IDs provided")
        
        if len(job_id_list) < 2:
            raise ValueError("At least 2 job IDs required for comparison")
    
    except ValueError as e:
        logger.warning(f"[Analytics] Invalid job IDs format: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    
    try:
        comparison = compare_jobs(job_id_list)
        logger.info(f"[Analytics] Successfully compared {len(job_id_list)} jobs")
        return JSONResponse(content=comparison)
    
    except Exception as e:
        logger.error(f"[Analytics] Error comparing jobs: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to compare jobs: {str(e)}")




