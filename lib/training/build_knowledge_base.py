#!/usr/bin/env python3
"""
Build comprehensive knowledge base from FineTune Lab codebase
Extract ALL facts for DeepSeek to use when answering questions
"""

import re
from pathlib import Path
from typing import List, Dict

def extract_from_docs() -> str:
    """Extract content from all doc pages"""
    docs_dir = Path("/home/juan-canfield/Desktop/web-ui/app/docs")

    knowledge = []
    knowledge.append("="*80)
    knowledge.append("FINETUNE LAB DOCUMENTATION")
    knowledge.append("="*80)
    knowledge.append("")

    # Read all doc pages
    doc_files = [
        ("quick-start/page.tsx", "Quick Start Guide"),
        ("features/page.tsx", "Features"),
        ("guides/page.tsx", "Guides"),
    ]

    for file_path, title in doc_files:
        full_path = docs_dir / file_path
        if full_path.exists():
            content = full_path.read_text()

            # Extract text content from TSX (remove JSX/code)
            # Find strings in the content
            strings = re.findall(r'["\']([^"\']{20,})["\']', content)

            knowledge.append(f"\n## {title}\n")
            for s in strings[:100]:  # Limit per file
                if any(keyword in s.lower() for keyword in ['api', 'training', 'model', 'deploy', 'dataset', 'port', 'gpu', 'endpoint', 'default']):
                    knowledge.append(f"- {s}")

    return "\n".join(knowledge)

def extract_api_endpoints() -> str:
    """Extract all API endpoints"""
    api_dir = Path("/home/juan-canfield/Desktop/web-ui/app/api/training")

    endpoints = []
    endpoints.append("\n" + "="*80)
    endpoints.append("API ENDPOINTS")
    endpoints.append("="*80)
    endpoints.append("")

    for route_file in sorted(api_dir.rglob("route.ts")):
        rel_path = route_file.parent.relative_to(api_dir.parent)
        endpoint = f"/api/{rel_path}".replace("[", "{").replace("]", "}")

        # Read file to get HTTP methods
        content = route_file.read_text()
        methods = re.findall(r'export async function (GET|POST|PUT|DELETE|PATCH)', content)

        if methods:
            endpoints.append(f"{', '.join(methods):15} {endpoint}")

    return "\n".join(endpoints)

def extract_verified_facts() -> str:
    """Extract verified facts from codebase"""
    facts = []
    facts.append("\n" + "="*80)
    facts.append("VERIFIED IMPLEMENTATION FACTS")
    facts.append("="*80)
    facts.append("")

    facts.append("## Default Configuration Values")
    facts.append("- GPU Memory Utilization (vLLM): 0.8 (80%)")
    facts.append("  Source: components/training/DeployModelButton.tsx line 138")
    facts.append("- Eval Split: 0.2 (20% for evaluation)")
    facts.append("  Source: lib/training/training_server.py line 158")
    facts.append("- vLLM Port Range: 8002-8020")
    facts.append("  Source: components/training/DeployModelButton.tsx lines 335, 365")
    facts.append("")

    facts.append("## Deployment Options")
    facts.append("1. RunPod Serverless - Auto-scaling cloud inference")
    facts.append("   - GPU Options: A4000, A5000, A6000, H100")
    facts.append("   - Pricing: A4000 ($0.0004/req), A5000 ($0.0006/req), A6000 ($0.0008/req), H100 ($0.0035/req)")
    facts.append("   - Features: Auto-scaling, budget controls, cost tracking")
    facts.append("2. vLLM (Local) - Fast local GPU inference")
    facts.append("   - Requires: Local GPU with CUDA")
    facts.append("   - Port: Auto-assigned 8002-8020")
    facts.append("3. Ollama (Local) - Lightweight local inference")
    facts.append("   - Supports: LoRA adapters, quantized models")
    facts.append("")

    facts.append("## Supported Models")
    facts.append("- Model Families: Llama, Mistral, Qwen, Phi, Gemma")
    facts.append("- Training Methods: LoRA, QLoRA, Full fine-tuning")
    facts.append("- Precision: FP16, BF16, Mixed precision")
    facts.append("")

    facts.append("## Dataset Requirements")
    facts.append("- Format: JSONL (JSON Lines)")
    facts.append("- Structure: {\"messages\": [{\"role\": \"user/assistant/system\", \"content\": \"...\"}]}")
    facts.append("- Minimum Examples: 50")
    facts.append("- Optimal Range: 500-5000 examples")
    facts.append("- Max Tokens per Example: 4000")
    facts.append("- Encoding: UTF-8")
    facts.append("- Validation: Automatic JSONL validation, role alternation checks, tool call format verification")
    facts.append("")

    facts.append("## Hyperparameter Recommendations")
    facts.append("Learning Rate:")
    facts.append("  - Recommended: 1e-5 to 1e-4 (0.00001 to 0.0001)")
    facts.append("  - Large datasets/smaller models: 1e-4 to 1e-3")
    facts.append("Batch Size:")
    facts.append("  - Small GPU (<8GB VRAM): 1-2")
    facts.append("  - Medium GPU (8-16GB VRAM): 4-8")
    facts.append("  - Large GPU (>16GB VRAM): 16-32")
    facts.append("Epochs:")
    facts.append("  - Large dataset (>1000 examples): 1-3")
    facts.append("  - Medium dataset (100-1000 examples): 3-5")
    facts.append("  - Small dataset (<100 examples): 5-10")
    facts.append("")

    facts.append("## Tech Stack")
    facts.append("- Frontend: Next.js")
    facts.append("- Database: Supabase (PostgreSQL) with row-level security")
    facts.append("- Training: Hugging Face Transformers, PyTorch")
    facts.append("- Inference: vLLM, Ollama")
    facts.append("- Cloud GPU: RunPod Serverless")
    facts.append("- GPU Acceleration: CUDA")
    facts.append("- Containerization: Docker")
    facts.append("- Job Queue: Redis")
    facts.append("")

    facts.append("## Features")
    facts.append("- Real-time training metrics (loss, learning rate, GPU utilization)")
    facts.append("- Live loss visualization")
    facts.append("- Automatic checkpoint saving")
    facts.append("- Pause/resume functionality")
    facts.append("- Budget controls for cloud deployment")
    facts.append("- Auto-scaling for RunPod Serverless")
    facts.append("- Dataset versioning")
    facts.append("- A/B testing support")
    facts.append("- Training run comparisons")
    facts.append("- Cost analysis per job")
    facts.append("")

    facts.append("## Budget & Cost Management")
    facts.append("- Minimum Budget: $1.00")
    facts.append("- Recommended for Production: $10-50")
    facts.append("- Budget Alerts: Automatic at 50%, 80%, and 100%")
    facts.append("- Auto-Stop: Optional automatic stop when budget reached")
    facts.append("- Cost Tracking: Real-time spend monitoring per request")
    facts.append("")

    return "\n".join(facts)

def extract_training_workflow() -> str:
    """Extract training workflow steps"""
    workflow = []
    workflow.append("\n" + "="*80)
    workflow.append("TRAINING WORKFLOW")
    workflow.append("="*80)
    workflow.append("")

    workflow.append("## Step 1: Prepare Dataset")
    workflow.append("- Create JSONL file with training examples")
    workflow.append("- Format: {\"messages\": [{\"role\": \"...\", \"content\": \"...\"}]}")
    workflow.append("- Upload via: POST /api/training/datasets")
    workflow.append("")

    workflow.append("## Step 2: Create Training Config")
    workflow.append("- Endpoint: POST /api/training")
    workflow.append("- Required: name, base_model, dataset_id")
    workflow.append("- Optional: learning_rate, batch_size, epochs, warmup_steps, etc.")
    workflow.append("")

    workflow.append("## Step 3: Start Training")
    workflow.append("- Endpoint: POST /api/training/execute")
    workflow.append("- Provide: config ID")
    workflow.append("- Returns: job_id for monitoring")
    workflow.append("")

    workflow.append("## Step 4: Monitor Progress")
    workflow.append("- Metrics: GET /api/training/metrics/{id}")
    workflow.append("- Logs: GET /api/training/logs/{id}")
    workflow.append("- Status: GET /api/training/status/{job_id}")
    workflow.append("")

    workflow.append("## Step 5: Deploy Model")
    workflow.append("- Endpoint: POST /api/inference/deploy")
    workflow.append("- Options: RunPod Serverless, vLLM (local), Ollama (local)")
    workflow.append("- For RunPod: Requires API key in Settings ‚Üí Secrets")
    workflow.append("- For vLLM: Auto-assigns port 8002-8020")
    workflow.append("")

    return "\n".join(workflow)

def main():
    print("="*80)
    print("BUILDING FINETUNE LAB KNOWLEDGE BASE")
    print("="*80)
    print()

    # Build comprehensive knowledge base
    knowledge_base = []

    print("üìö Extracting documentation...")
    knowledge_base.append(extract_from_docs())

    print("üîó Extracting API endpoints...")
    knowledge_base.append(extract_api_endpoints())

    print("‚úÖ Extracting verified facts...")
    knowledge_base.append(extract_verified_facts())

    print("üìã Extracting workflow...")
    knowledge_base.append(extract_training_workflow())

    # Save knowledge base
    output_path = Path("/home/juan-canfield/Desktop/web-ui/output/evaluation/finetune_lab_knowledge_base.txt")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    full_kb = "\n".join(knowledge_base)
    output_path.write_text(full_kb)

    print(f"\n‚úÖ Knowledge base created!")
    print(f"üìÅ Location: {output_path}")
    print(f"üìä Size: {len(full_kb):,} characters ({len(full_kb.split()):,} words)")
    print(f"üìÑ Lines: {len(full_kb.splitlines()):,}")
    print()
    print("üí° Next step: Generate questions for DeepSeek to answer using this knowledge base")

if __name__ == "__main__":
    main()
